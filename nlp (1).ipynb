{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5kKms5Xk6s3"
      },
      "source": [
        "# üì¶ PH·∫¶N 1: T·∫¢I V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
        "\n",
        "---\n",
        "\n",
        "**M·ª•c ti√™u:**\n",
        "1. T·∫£i dataset Multi30K (English ‚Üí French)\n",
        "2. Tokenization v·ªõi SpaCy\n",
        "3. X√¢y d·ª±ng Vocabulary v·ªõi special tokens: `<unk>`, `<pad>`, `<sos>`, `<eos>`\n",
        "4. T·∫°o DataLoader v·ªõi sorting + padding (s·∫µn s√†ng cho LSTM)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwW3in4hk-o",
        "outputId": "0281a429-1d7a-4f73-82b1-3d7db4c68d8e"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN & C·∫§U H√åNH\n",
        "# ==============================================================================\n",
        "\n",
        "# C√†i ƒë·∫∑t th∆∞ vi·ªán (ch·∫°y tr√™n Google Colab)\n",
        "!pip install torch==2.2.2 torchtext==0.17.2 -q\n",
        "!pip install spacy nltk -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "!python -m spacy download fr_core_news_sm -q\n",
        "\n",
        "# Import th∆∞ vi·ªán\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # C·∫ßn cho Attention\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "\n",
        "# =============================================================================\n",
        "# SEED cho Reproducibility\n",
        "# =============================================================================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"‚úÖ Torch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ Torchtext version: {torchtext.__version__}\")\n",
        "print(f\"‚úÖ Device: {device}\")\n",
        "print(f\"‚úÖ Seed: {SEED}\")\n",
        "\n",
        "# =============================================================================\n",
        "# T·∫¢I DATASET MULTI30K (EN-FR)\n",
        "# =============================================================================\n",
        "!mkdir -p data\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz -O data/train.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz -O data/train.fr.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz -O data/val.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz -O data/val.fr.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz -O data/test.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz -O data/test.fr.gz\n",
        "\n",
        "# Gi·∫£i n√©n\n",
        "!gunzip -kf data/*.gz\n",
        "!ls -la data/\n",
        "\n",
        "print(\"\\n‚úÖ ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu v√† th∆∞ vi·ªán!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgtl2UdijV7e",
        "outputId": "c435739f-5680-40d8-9a1f-6af9380fecc9"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.2: ƒê·ªåC V√Ä KI·ªÇM TRA D·ªÆ LI·ªÜU\n",
        "# ==============================================================================\n",
        "\n",
        "def read_data(en_file, fr_file):\n",
        "    \"\"\"\n",
        "    ƒê·ªçc d·ªØ li·ªáu song ng·ªØ t·ª´ file.\n",
        "    \n",
        "    Returns:\n",
        "        en_sentences: List c√¢u ti·∫øng Anh\n",
        "        fr_sentences: List c√¢u ti·∫øng Ph√°p (cƒÉn ch·ªânh 1-1)\n",
        "    \"\"\"\n",
        "    with open(en_file, 'r', encoding='utf-8') as f:\n",
        "        en_sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "    with open(fr_file, 'r', encoding='utf-8') as f:\n",
        "        fr_sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
        "    \n",
        "    # ƒê·∫£m b·∫£o s·ªë c√¢u kh·ªõp nhau\n",
        "    assert len(en_sentences) == len(fr_sentences), \"S·ªë c√¢u EN v√† FR kh√¥ng kh·ªõp!\"\n",
        "    return en_sentences, fr_sentences\n",
        "\n",
        "# ƒê·ªçc d·ªØ li·ªáu train, val, test t·ª´ folder 'data/'\n",
        "train_en, train_fr = read_data('data/train.en', 'data/train.fr')\n",
        "val_en, val_fr = read_data('data/val.en', 'data/val.fr')\n",
        "test_en, test_fr = read_data('data/test.en', 'data/test.fr')\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"üìä TH·ªêNG K√ä D·ªÆ LI·ªÜU MULTI30K\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"   Train:      {len(train_en):,} c·∫∑p c√¢u\")\n",
        "print(f\"   Validation: {len(val_en):,} c·∫∑p c√¢u\")\n",
        "print(f\"   Test:       {len(test_en):,} c·∫∑p c√¢u\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Hi·ªÉn th·ªã v√≠ d·ª•\n",
        "print(\"\\nüìù V√ç D·ª§ 5 C·∫∂P C√ÇU ƒê·∫¶U TI√äN:\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"   EN: {train_en[i]}\")\n",
        "    print(f\"   FR: {train_fr[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2YzQaXJjaB1",
        "outputId": "0b519ee4-8391-4832-f805-7b1cb91a2398"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.3: TOKENIZATION & VOCABULARY\n",
        "# ==============================================================================\n",
        "\n",
        "# --- C·∫§U H√åNH TOKEN ƒê·∫∂C BI·ªÜT ---\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "# =============================================================================\n",
        "# H√ÄM: get_tokenizers()\n",
        "# Nhi·ªám v·ª•: T·∫£i tokenizer c·ªßa SpaCy cho ti·∫øng Anh v√† Ph√°p\n",
        "# =============================================================================\n",
        "def get_tokenizers():\n",
        "    try:\n",
        "        en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "        fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "        print(\"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng Tokenizer (SpaCy)\")\n",
        "        return en_tokenizer, fr_tokenizer\n",
        "    except OSError:\n",
        "        print(\"‚ùå L·ªói: Ch∆∞a t√¨m th·∫•y model SpaCy. H√£y ch·∫°y l·∫°i Cell 1.1\")\n",
        "        return None, None\n",
        "\n",
        "# =============================================================================\n",
        "# H√ÄM: build_vocab()\n",
        "# Nhi·ªám v·ª•: X√¢y d·ª±ng vocabulary t·ª´ file d·ªØ li·ªáu\n",
        "# =============================================================================\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    \"\"\"\n",
        "    X√¢y d·ª±ng vocabulary t·ª´ file text.\n",
        "    \n",
        "    Args:\n",
        "        filepath: ƒê∆∞·ªùng d·∫´n file text\n",
        "        tokenizer: Tokenizer function\n",
        "    \n",
        "    Returns:\n",
        "        vocab: Vocabulary object v·ªõi special tokens\n",
        "    \"\"\"\n",
        "    def yield_tokens(path):\n",
        "        with io.open(path, encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    yield tokenizer(line.strip())\n",
        "    \n",
        "    print(f\"   ƒêang x√¢y d·ª±ng vocab t·ª´ {filepath}...\")\n",
        "    \n",
        "    vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(filepath),\n",
        "        min_freq=2,           # B·ªè qua t·ª´ xu·∫•t hi·ªán < 2 l·∫ßn\n",
        "        max_tokens=10000,     # Gi·ªõi h·∫°n vocabulary size\n",
        "        specials=SPECIAL_SYMBOLS\n",
        "    )\n",
        "    \n",
        "    # Set default index cho t·ª´ kh√¥ng c√≥ trong vocab (OOV)\n",
        "    vocab.set_default_index(UNK_IDX)\n",
        "    return vocab\n",
        "\n",
        "# =============================================================================\n",
        "# TH·ª∞C THI\n",
        "# =============================================================================\n",
        "tokenizer_en, tokenizer_fr = get_tokenizers()\n",
        "\n",
        "if tokenizer_en and tokenizer_fr:\n",
        "    print(\"\\nüìö X√¢y d·ª±ng Vocabulary:\")\n",
        "    vocab_en = build_vocab('data/train.en', tokenizer_en)\n",
        "    vocab_fr = build_vocab('data/train.fr', tokenizer_fr)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìä B√ÅO C√ÅO VOCABULARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"   Vocab EN: {len(vocab_en):,} tokens (max: 10,004)\")\n",
        "    print(f\"   Vocab FR: {len(vocab_fr):,} tokens (max: 10,004)\")\n",
        "    print(f\"   Special tokens: <unk>={UNK_IDX}, <pad>={PAD_IDX}, <sos>={SOS_IDX}, <eos>={EOS_IDX}\")\n",
        "    \n",
        "    # Ki·ªÉm tra x·ª≠ l√Ω t·ª´ l·∫° (OOV)\n",
        "    test_oov = vocab_en['t·ª´_kh√¥ng_t·ªìn_t·∫°i_xyz']\n",
        "    if test_oov == UNK_IDX:\n",
        "        print(\"   OOV handling: ‚úÖ Th√†nh c√¥ng (tr·∫£ v·ªÅ index 0)\")\n",
        "    else:\n",
        "        print(f\"   OOV handling: ‚ùå Th·∫•t b·∫°i (tr·∫£ v·ªÅ {test_oov})\")\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIH4Wa1SjfHn",
        "outputId": "c2b1246d-cd01-4754-8f9e-de1009b5593c"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.4: TEXT TRANSFORM (Numericalization)\n",
        "# ==============================================================================\n",
        "\n",
        "def text_transform(text, tokenizer, vocab):\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi c√¢u text th√†nh tensor s·ªë.\n",
        "    \n",
        "    Pipeline: text ‚Üí tokens ‚Üí token_ids ‚Üí tensor v·ªõi <sos> v√† <eos>\n",
        "    \n",
        "    Args:\n",
        "        text: C√¢u input (string)\n",
        "        tokenizer: Tokenizer function\n",
        "        vocab: Vocabulary object\n",
        "    \n",
        "    Returns:\n",
        "        tensor: [SOS, token_ids..., EOS]\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(text)\n",
        "    token_ids = [vocab[token] for token in tokens]\n",
        "    return torch.tensor([SOS_IDX] + token_ids + [EOS_IDX], dtype=torch.long)\n",
        "\n",
        "# =============================================================================\n",
        "# KI·ªÇM TRA TEXT PIPELINE\n",
        "# =============================================================================\n",
        "print(\"=\" * 50)\n",
        "print(\"üîç KI·ªÇM TRA TEXT PIPELINE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "sample_sentence = \"Two young, White males are outside.\"\n",
        "print(f\"1. C√¢u g·ªëc:     '{sample_sentence}'\")\n",
        "\n",
        "# Tokenize\n",
        "sample_tokens = tokenizer_en(sample_sentence)\n",
        "print(f\"2. Tokens:      {sample_tokens}\")\n",
        "\n",
        "# Transform\n",
        "sample_tensor = text_transform(sample_sentence, tokenizer_en, vocab_en)\n",
        "print(f\"3. Tensor:      {sample_tensor}\")\n",
        "print(f\"4. Shape:       {sample_tensor.shape}\")\n",
        "print(f\"5. Dtype:       {sample_tensor.dtype}\")\n",
        "\n",
        "# Ki·ªÉm tra logic <sos> v√† <eos>\n",
        "if sample_tensor[0] == SOS_IDX and sample_tensor[-1] == EOS_IDX:\n",
        "    print(\"\\n‚úÖ Logic <sos>/<eos>: ƒê√öNG (ƒë·∫ßu=2, cu·ªëi=3)\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Logic <sos>/<eos>: SAI (ƒë·∫ßu={sample_tensor[0]}, cu·ªëi={sample_tensor[-1]})\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzRNeMyVjrEc",
        "outputId": "eb84c721-f65d-4d4c-f936-57cd614f5b51"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.5: DATASET, COLLATE_FN & DATALOADER\n",
        "# ==============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: TranslationDataset\n",
        "# =============================================================================\n",
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset cho d·ªØ li·ªáu song ng·ªØ.\n",
        "    L∆∞u tr·ªØ c·∫∑p c√¢u (source, target) d·∫°ng text.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_list, trg_list):\n",
        "        self.src_list = src_list\n",
        "        self.trg_list = trg_list\n",
        "        assert len(src_list) == len(trg_list), \"S·ªë c√¢u source v√† target kh√¥ng kh·ªõp!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_list[idx], self.trg_list[idx]\n",
        "\n",
        "# =============================================================================\n",
        "# H√ÄM: collate_fn\n",
        "# X·ª≠ l√Ω batch: padding + sorting theo ƒë·ªô d√†i (cho pack_padded_sequence)\n",
        "# =============================================================================\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω batch d·ªØ li·ªáu:\n",
        "    1. Chuy·ªÉn text ‚Üí tensor\n",
        "    2. Padding ƒë·ªÉ ƒë·ªìng b·ªô ƒë·ªô d√†i\n",
        "    3. Sort theo ƒë·ªô d√†i gi·∫£m d·∫ßn (y√™u c·∫ßu cho pack_padded_sequence)\n",
        "    \n",
        "    Returns:\n",
        "        src_padded: [src_len, batch_size]\n",
        "        trg_padded: [trg_len, batch_size]\n",
        "        sorted_lens: [batch_size] - ƒë·ªô d√†i th·ª±c c·ªßa t·ª´ng c√¢u source\n",
        "    \"\"\"\n",
        "    src_batch, trg_batch = [], []\n",
        "\n",
        "    # Chuy·ªÉn ƒë·ªïi Text ‚Üí Tensor\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(text_transform(src_sample, tokenizer_en, vocab_en))\n",
        "        trg_batch.append(text_transform(trg_sample, tokenizer_fr, vocab_fr))\n",
        "\n",
        "    # Padding (ƒë·ªìng b·ªô ƒë·ªô d√†i trong batch)\n",
        "    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    trg_padded = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
        "\n",
        "    # T√≠nh ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa t·ª´ng c√¢u ngu·ªìn (dtype=long cho pack_padded_sequence)\n",
        "    src_lens = torch.tensor([len(x) for x in src_batch], dtype=torch.long)\n",
        "\n",
        "    # Sort gi·∫£m d·∫ßn theo ƒë·ªô d√†i (y√™u c·∫ßu c·ªßa pack_padded_sequence v·ªõi enforce_sorted=True)\n",
        "    sorted_lens, sorted_indices = torch.sort(src_lens, descending=True)\n",
        "\n",
        "    # S·∫Øp x·∫øp l·∫°i tensors theo th·ª© t·ª± ƒë√£ sort\n",
        "    src_padded = src_padded[:, sorted_indices]\n",
        "    trg_padded = trg_padded[:, sorted_indices]\n",
        "\n",
        "    return src_padded, trg_padded, sorted_lens\n",
        "\n",
        "# =============================================================================\n",
        "# T·∫†O DATALOADER\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# T·∫°o Dataset\n",
        "train_dataset = TranslationDataset(train_en, train_fr)\n",
        "valid_dataset = TranslationDataset(val_en, val_fr)\n",
        "test_dataset = TranslationDataset(test_en, test_fr)\n",
        "\n",
        "# T·∫°o DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,           # Shuffle cho training\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# KI·ªÇM TRA DATALOADER\n",
        "# =============================================================================\n",
        "print(\"=\" * 50)\n",
        "print(\"üîç KI·ªÇM TRA DATALOADER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    src, trg, src_len = next(iter(train_loader))\n",
        "    \n",
        "    print(f\"1. Source shape:   {src.shape} (seq_len, batch_size)\")\n",
        "    print(f\"2. Target shape:   {trg.shape} (seq_len, batch_size)\")\n",
        "    print(f\"3. Lengths shape:  {src_len.shape}\")\n",
        "    print(f\"   - C√¢u d√†i nh·∫•t:  {src_len[0]} tokens\")\n",
        "    print(f\"   - C√¢u ng·∫Øn nh·∫•t: {src_len[-1]} tokens\")\n",
        "    \n",
        "    # Ki·ªÉm tra sorting\n",
        "    if src_len[0] >= src_len[-1]:\n",
        "        print(\"\\n‚úÖ Batch ƒë√£ SORT theo ƒë·ªô d√†i (gi·∫£m d·∫ßn)\")\n",
        "        print(\"   ‚Üí S·∫µn s√†ng cho pack_padded_sequence!\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå L·ªói: Batch ch∆∞a ƒë∆∞·ª£c sort ƒë√∫ng!\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    print(f\"üìä S·ªê BATCH:\")\n",
        "    print(f\"   Train:      {len(train_loader)} batches\")\n",
        "    print(f\"   Validation: {len(valid_loader)} batches\")\n",
        "    print(f\"   Test:       {len(test_loader)} batches\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\n‚úÖ PH·∫¶N 1 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 2 (Model)!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå L·ªñI: {e}\")\n",
        "    print(\"G·ª£i √Ω: Ki·ªÉm tra l·∫°i collate_fn c√≥ tr·∫£ v·ªÅ ƒë√∫ng 3 gi√° tr·ªã kh√¥ng?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcQacjzMley_"
      },
      "source": [
        "# üèóÔ∏è PH·∫¶N 2: X√ÇY D·ª∞NG M√î H√åNH BASELINE SEQ2SEQ\n",
        "\n",
        "---\n",
        "\n",
        "## Ki·∫øn tr√∫c Encoder-Decoder LSTM (Kh√¥ng Attention)\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                     BASELINE SEQ2SEQ ARCHITECTURE                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                  ‚îÇ\n",
        "‚îÇ  INPUT (EN)       ENCODER           DECODER        OUTPUT (FR)  ‚îÇ\n",
        "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí   ‚îÇ\n",
        "‚îÇ                                                                  ‚îÇ\n",
        "‚îÇ  \"A man walks\" ‚Üí [LSTM x2] ‚Üí (h,c) ‚Üí [LSTM x2] ‚Üí \"Un homme...\" ‚îÇ\n",
        "‚îÇ                              ‚Üë                                   ‚îÇ\n",
        "‚îÇ                       Context Vector                             ‚îÇ\n",
        "‚îÇ                  (Fixed representation)                          ‚îÇ\n",
        "‚îÇ                                                                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**C√¥ng th·ª©c:**\n",
        "- Encoder: `(h_t, c_t) = LSTM(embed(x_t), (h_{t-1}, c_{t-1}))`\n",
        "- Decoder: `(h_t, c_t) = LSTM(embed(y_{t-1}), (h'_{t-1}, c'_{t-1}))`\n",
        "- Output:  `p(y_t) = softmax(Linear(h_t))`\n",
        "\n",
        "**Hyperparameters:**\n",
        "\n",
        "| Parameter | Value | M√¥ t·∫£ |\n",
        "|-----------|-------|-------|\n",
        "| `EMB_DIM` | 256 | Embedding dimension |\n",
        "| `HID_DIM` | 512 | Hidden state dimension |\n",
        "| `N_LAYERS` | 2 | S·ªë l·ªõp LSTM |\n",
        "| `DROPOUT` | 0.5 | Dropout rate |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnwRGUmNnWMp",
        "outputId": "0b87e957-5f5d-43cf-e876-b7061b337b1b"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2.1: ƒê·ªäNH NGHƒ®A C√ÅC CLASS MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: Encoder\n",
        "# =============================================================================\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder LSTM cho Seq2Seq.\n",
        "    \n",
        "    Nhi·ªám v·ª•: ƒê·ªçc c√¢u ngu·ªìn v√† t·∫°o context vector (hidden, cell states).\n",
        "    S·ª≠ d·ª•ng pack_padded_sequence ƒë·ªÉ x·ª≠ l√Ω padding hi·ªáu qu·∫£.\n",
        "    \n",
        "    Args:\n",
        "        input_dim: K√≠ch th∆∞·ªõc vocabulary ngu·ªìn\n",
        "        emb_dim: Embedding dimension\n",
        "        hid_dim: Hidden state dimension\n",
        "        n_layers: S·ªë l·ªõp LSTM\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Encoder.\n",
        "        \n",
        "        Args:\n",
        "            src: [src_len, batch_size] - Tensor c√¢u ngu·ªìn\n",
        "            src_len: [batch_size] - ƒê·ªô d√†i th·ª±c c·ªßa m·ªói c√¢u\n",
        "        \n",
        "        Returns:\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Hidden states\n",
        "            cell: [n_layers, batch_size, hid_dim] - Cell states\n",
        "        \n",
        "        Note: Baseline KH√îNG tr·∫£ v·ªÅ encoder_outputs (s·∫Ω th√™m khi c√≥ Attention)\n",
        "        \"\"\"\n",
        "        # src: [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # Pack ƒë·ªÉ LSTM kh√¥ng x·ª≠ l√Ω padding tokens\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)\n",
        "        \n",
        "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        # hidden: [n_layers, batch_size, hid_dim]\n",
        "        # cell: [n_layers, batch_size, hid_dim]\n",
        "        \n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: Decoder\n",
        "# =============================================================================\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder LSTM cho Seq2Seq (Baseline - kh√¥ng Attention).\n",
        "    \n",
        "    Nhi·ªám v·ª•: Sinh c√¢u ƒë√≠ch t·ª´ng token m·ªôt, d·ª±a tr√™n context t·ª´ Encoder.\n",
        "    \n",
        "    Args:\n",
        "        output_dim: K√≠ch th∆∞·ªõc vocabulary ƒë√≠ch\n",
        "        emb_dim: Embedding dimension\n",
        "        hid_dim: Hidden state dimension\n",
        "        n_layers: S·ªë l·ªõp LSTM\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Decoder (1 timestep).\n",
        "        \n",
        "        Args:\n",
        "            input: [batch_size] - Token hi·ªán t·∫°i\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Hidden states\n",
        "            cell: [n_layers, batch_size, hid_dim] - Cell states\n",
        "        \n",
        "        Returns:\n",
        "            prediction: [batch_size, output_dim] - Logits cho vocabulary\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Updated hidden\n",
        "            cell: [n_layers, batch_size, hid_dim] - Updated cell\n",
        "        \"\"\"\n",
        "        # input: [batch_size] ‚Üí [1, batch_size]\n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [1, batch_size, emb_dim]\n",
        "        \n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # output: [1, batch_size, hid_dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction: [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: Seq2Seq\n",
        "# =============================================================================\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    M√¥ h√¨nh Seq2Seq Baseline (Encoder-Decoder kh√¥ng Attention).\n",
        "    \n",
        "    ƒêi·ªÅu ph·ªëi Encoder v√† Decoder, x·ª≠ l√Ω teacher forcing.\n",
        "    \n",
        "    Args:\n",
        "        encoder: Encoder instance\n",
        "        decoder: Decoder instance\n",
        "        device: 'cuda' ho·∫∑c 'cpu'\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o encoder v√† decoder t∆∞∆°ng th√≠ch\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must match!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Number of layers of encoder and decoder must match!\"\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Seq2Seq.\n",
        "        \n",
        "        Args:\n",
        "            src: [src_len, batch_size] - C√¢u ngu·ªìn\n",
        "            src_len: [batch_size] - ƒê·ªô d√†i c√¢u ngu·ªìn\n",
        "            trg: [trg_len, batch_size] - C√¢u ƒë√≠ch\n",
        "            teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng ground truth (0.0 - 1.0)\n",
        "        \n",
        "        Returns:\n",
        "            outputs: [trg_len, batch_size, output_dim] - Logits cho m·ªói timestep\n",
        "        \"\"\"\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # Tensor l∆∞u output c·ªßa decoder t·∫°i m·ªói timestep\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # Encode c√¢u ngu·ªìn\n",
        "        hidden, cell = self.encoder(src, src_len)\n",
        "        \n",
        "        # Token ƒë·∫ßu ti√™n l√† <sos>\n",
        "        input = trg[0, :]\n",
        "        \n",
        "        # Decode t·ª´ng timestep\n",
        "        for t in range(1, trg_len):\n",
        "            # Forward decoder\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            # L∆∞u output\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # Teacher Forcing: d√πng ground truth ho·∫∑c prediction\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# H√ÄM: init_weights\n",
        "# =============================================================================\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Kh·ªüi t·∫°o weights cho model.\n",
        "    - Weights: Uniform distribution [-0.08, 0.08]\n",
        "    - Biases: 0\n",
        "    \"\"\"\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: Encoder, Decoder, Seq2Seq, init_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze_ZolHEnagI",
        "outputId": "73826325-e2df-43a3-d056-e6655e563720"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2.2: KH·ªûI T·∫†O BASELINE MODEL (FIXED CONTEXT VECTOR)\n",
        "# ==============================================================================\n",
        "# ‚ö†Ô∏è ƒê√ÇY L√Ä M√î H√åNH BASELINE - S·ª¨ D·ª§NG CONTEXT VECTOR C·ªê ƒê·ªäNH\n",
        "# (hidden state cu·ªëi c√πng c·ªßa Encoder, KH√îNG c√≥ Attention)\n",
        "\n",
        "# =============================================================================\n",
        "# HYPERPARAMETERS\n",
        "# =============================================================================\n",
        "INPUT_DIM = len(vocab_en)    # Vocab size ti·∫øng Anh\n",
        "OUTPUT_DIM = len(vocab_fr)   # Vocab size ti·∫øng Ph√°p\n",
        "ENC_EMB_DIM = 256            # Encoder embedding dim\n",
        "DEC_EMB_DIM = 256            # Decoder embedding dim\n",
        "HID_DIM = 512                # Hidden state dim\n",
        "N_LAYERS = 2                 # S·ªë l·ªõp LSTM\n",
        "ENC_DROPOUT = 0.5            # Encoder dropout\n",
        "DEC_DROPOUT = 0.5            # Decoder dropout\n",
        "\n",
        "# =============================================================================\n",
        "# KH·ªûI T·∫†O BASELINE MODEL\n",
        "# =============================================================================\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "# ‚ö†Ô∏è QUAN TR·ªåNG: ƒê·∫∑t t√™n bi·∫øn l√† baseline_model ƒë·ªÉ ph√¢n bi·ªát v·ªõi attention_model\n",
        "baseline_model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# √Åp d·ª•ng weight initialization\n",
        "baseline_model.apply(init_weights)\n",
        "\n",
        "# ƒê·∫øm parameters\n",
        "baseline_params = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üèóÔ∏è M√î H√åNH BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìå ƒê·∫∑c ƒëi·ªÉm: S·ª≠ d·ª•ng CONTEXT VECTOR C·ªê ƒê·ªäNH\")\n",
        "print(\"   (Ch·ªâ d√πng hidden state cu·ªëi c√πng c·ªßa Encoder)\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Device:           {device}\")\n",
        "print(f\"Input dim (EN):   {INPUT_DIM:,}\")\n",
        "print(f\"Output dim (FR):  {OUTPUT_DIM:,}\")\n",
        "print(f\"Embedding dim:    {ENC_EMB_DIM}\")\n",
        "print(f\"Hidden dim:       {HID_DIM}\")\n",
        "print(f\"Num layers:       {N_LAYERS}\")\n",
        "print(f\"Dropout:          {ENC_DROPOUT}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Total parameters: {baseline_params:,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# =============================================================================\n",
        "# KI·ªÇM TRA K·∫æT N·ªêI DATA ‚Üí MODEL\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üîç KI·ªÇM TRA FORWARD PASS (BASELINE)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # L·∫•y 1 batch t·ª´ train_loader\n",
        "    src, trg, src_len = next(iter(train_loader))\n",
        "    src = src.to(device)\n",
        "    trg = trg.to(device)\n",
        "    \n",
        "    print(f\"Input (src):      {src.shape}  [seq_len, batch_size]\")\n",
        "    print(f\"Input (src_len):  {src_len.shape}  [batch_size]\")\n",
        "    print(f\"Target (trg):     {trg.shape}  [seq_len, batch_size]\")\n",
        "    \n",
        "    # Forward pass v·ªõi baseline_model\n",
        "    output = baseline_model(src, src_len, trg)\n",
        "    \n",
        "    print(f\"Output:           {output.shape}  [seq_len, batch_size, vocab_size]\")\n",
        "    \n",
        "    # Validate output shape\n",
        "    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)\n",
        "    if output.shape == expected_shape:\n",
        "        print(\"\\n‚úÖ BASELINE FORWARD PASS TH√ÄNH C√îNG!\")\n",
        "        print(\"   ‚Üí Baseline Model s·∫µn s√†ng cho Training.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Shape kh√¥ng kh·ªõp! Expected: {expected_shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå L·ªñI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úÖ PH·∫¶N 2 HO√ÄN T·∫§T - Baseline Model ƒë√£ s·∫µn s√†ng!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ PH·∫¶N 3: SEQ2SEQ + LUONG ATTENTION (M√î H√åNH CH√çNH)\n",
        "\n",
        "---\n",
        "\n",
        "## Ki·∫øn tr√∫c Encoder-Decoder LSTM v·ªõi Attention\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    SEQ2SEQ + LUONG ATTENTION ARCHITECTURE               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                         ‚îÇ\n",
        "‚îÇ  ENCODER                              DECODER                           ‚îÇ\n",
        "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ\n",
        "‚îÇ                                                                         ‚îÇ\n",
        "‚îÇ  \"A man walks\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                 ‚îÇ\n",
        "‚îÇ       ‚Üì               ‚îÇ                                                 ‚îÇ\n",
        "‚îÇ   [LSTM x2]           ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n",
        "‚îÇ       ‚Üì               ‚îÇ              ‚îÇ   ATTENTION     ‚îÇ               ‚îÇ\n",
        "‚îÇ  encoder_outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (Luong General)‚îÇ               ‚îÇ\n",
        "‚îÇ  (h‚ÇÅ, h‚ÇÇ, ..., h‚Çô)    ‚îÇ              ‚îÇ                 ‚îÇ               ‚îÇ\n",
        "‚îÇ       ‚Üì               ‚îÇ              ‚îÇ  score = h‚Çú·µÄW‚Çêh‚Çõ‚îÇ               ‚îÇ\n",
        "‚îÇ  (hidden, cell) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Œ± = softmax    ‚îÇ               ‚îÇ\n",
        "‚îÇ                       ‚îÇ              ‚îÇ  c = Œ£ Œ±·µ¢h·µ¢     ‚îÇ               ‚îÇ\n",
        "‚îÇ                       ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n",
        "‚îÇ                       ‚îÇ                       ‚Üì                         ‚îÇ\n",
        "‚îÇ                       ‚îÇ              context_vector                     ‚îÇ\n",
        "‚îÇ                       ‚îÇ                       ‚Üì                         ‚îÇ\n",
        "‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [embed; context]                 ‚îÇ\n",
        "‚îÇ                                              ‚Üì                          ‚îÇ\n",
        "‚îÇ                                         [LSTM x2]                       ‚îÇ\n",
        "‚îÇ                                              ‚Üì                          ‚îÇ\n",
        "‚îÇ                                    [hidden; context]                    ‚îÇ\n",
        "‚îÇ                                              ‚Üì                          ‚îÇ\n",
        "‚îÇ                                         Linear ‚Üí vocab                  ‚îÇ\n",
        "‚îÇ                                              ‚Üì                          ‚îÇ\n",
        "‚îÇ                                      \"Un homme marche\"                  ‚îÇ\n",
        "‚îÇ                                                                         ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## Luong Attention (General)\n",
        "\n",
        "**C√¥ng th·ª©c:**\n",
        "\n",
        "$$score(h_t, h_s) = h_t^T \\cdot W_a \\cdot h_s$$\n",
        "\n",
        "$$\\alpha = softmax(score)$$\n",
        "\n",
        "$$context = \\sum_i \\alpha_i \\cdot h_i$$\n",
        "\n",
        "**Tham kh·∫£o:** Luong et al. (2015) - \"Effective Approaches to Attention-based NMT\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# CELL 3.1: ATTENTION MECHANISM + ENCODER/DECODER V·ªöI ATTENTION\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# CLASS: Attention (Luong General)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Luong General Attention.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Reference: \"Effective Approaches to Attention-based NMT\" (Luong et al., 2015)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hid_dim):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3.1: ATTENTION MECHANISM + ENCODER/DECODER V·ªöI ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: Attention (Luong General)\n",
        "# =============================================================================\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Luong General Attention.\n",
        "    \n",
        "    C√¥ng th·ª©c: score(h_t, h_s) = h_t^T * W_a * h_s\n",
        "    \n",
        "    Args:\n",
        "        hid_dim: Hidden dimension c·ªßa encoder/decoder\n",
        "    \n",
        "    Reference: \"Effective Approaches to Attention-based NMT\" (Luong et al., 2015)\n",
        "    \"\"\"\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        # W_a: Linear layer ƒë·ªÉ t√≠nh score\n",
        "        self.W_a = nn.Linear(hid_dim, hid_dim, bias=False)\n",
        "    \n",
        "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
        "        \"\"\"\n",
        "        T√≠nh attention weights v√† context vector.\n",
        "        \n",
        "        Args:\n",
        "            decoder_hidden: [batch_size, hid_dim] - Hidden state hi·ªán t·∫°i c·ªßa decoder\n",
        "            encoder_outputs: [src_len, batch_size, hid_dim] - T·∫•t c·∫£ hidden states c·ªßa encoder\n",
        "            mask: [batch_size, src_len] - Mask cho padding (1=valid, 0=pad)\n",
        "        \n",
        "        Returns:\n",
        "            context: [batch_size, hid_dim] - Context vector\n",
        "            attention_weights: [batch_size, src_len] - Attention weights\n",
        "        \"\"\"\n",
        "        # decoder_hidden: [batch_size, hid_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim]\n",
        "        \n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        # Reshape decoder_hidden: [batch_size, hid_dim] ‚Üí [batch_size, 1, hid_dim]\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
        "        \n",
        "        # Permute encoder_outputs: [src_len, batch_size, hid_dim] ‚Üí [batch_size, src_len, hid_dim]\n",
        "        encoder_outputs_perm = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        # T√≠nh W_a * h_s: [batch_size, src_len, hid_dim]\n",
        "        energy = self.W_a(encoder_outputs_perm)\n",
        "        \n",
        "        # T√≠nh h_t^T * (W_a * h_s): [batch_size, 1, hid_dim] x [batch_size, hid_dim, src_len]\n",
        "        # ‚Üí [batch_size, 1, src_len]\n",
        "        attention_scores = torch.bmm(decoder_hidden, energy.permute(0, 2, 1))\n",
        "        \n",
        "        # Squeeze: [batch_size, 1, src_len] ‚Üí [batch_size, src_len]\n",
        "        attention_scores = attention_scores.squeeze(1)\n",
        "        \n",
        "        # √Åp d·ª•ng mask (n·∫øu c√≥) - ƒë·∫∑t padding positions = -inf\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        # Softmax ƒë·ªÉ c√≥ attention weights: [batch_size, src_len]\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        \n",
        "        # T√≠nh context vector: weighted sum c·ªßa encoder outputs\n",
        "        # [batch_size, 1, src_len] x [batch_size, src_len, hid_dim] ‚Üí [batch_size, 1, hid_dim]\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs_perm)\n",
        "        \n",
        "        # Squeeze: [batch_size, 1, hid_dim] ‚Üí [batch_size, hid_dim]\n",
        "        context = context.squeeze(1)\n",
        "        \n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: EncoderAttention (S·ª≠a t·ª´ Encoder ƒë·ªÉ tr·∫£ v·ªÅ encoder_outputs)\n",
        "# =============================================================================\n",
        "class EncoderAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder LSTM cho Seq2Seq + Attention.\n",
        "    \n",
        "    Kh√°c v·ªõi Encoder baseline: Tr·∫£ v·ªÅ th√™m encoder_outputs cho Attention.\n",
        "    \n",
        "    Args:\n",
        "        input_dim: K√≠ch th∆∞·ªõc vocabulary ngu·ªìn\n",
        "        emb_dim: Embedding dimension\n",
        "        hid_dim: Hidden state dimension\n",
        "        n_layers: S·ªë l·ªõp LSTM\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Encoder (cho Attention).\n",
        "        \n",
        "        Args:\n",
        "            src: [src_len, batch_size] - Tensor c√¢u ngu·ªìn\n",
        "            src_len: [batch_size] - ƒê·ªô d√†i th·ª±c c·ªßa m·ªói c√¢u\n",
        "        \n",
        "        Returns:\n",
        "            encoder_outputs: [src_len, batch_size, hid_dim] - T·∫•t c·∫£ hidden states\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Hidden state cu·ªëi\n",
        "            cell: [n_layers, batch_size, hid_dim] - Cell state cu·ªëi\n",
        "        \"\"\"\n",
        "        # src: [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: [src_len, batch_size, emb_dim]\n",
        "        \n",
        "        # Pack ƒë·ªÉ LSTM kh√¥ng x·ª≠ l√Ω padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)\n",
        "        \n",
        "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        \n",
        "        # Unpack ƒë·ªÉ l·∫•y encoder_outputs cho Attention\n",
        "        encoder_outputs, _ = pad_packed_sequence(packed_outputs)\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim]\n",
        "        # hidden: [n_layers, batch_size, hid_dim]\n",
        "        # cell: [n_layers, batch_size, hid_dim]\n",
        "        \n",
        "        return encoder_outputs, hidden, cell\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: DecoderAttention (Decoder v·ªõi Luong Attention)\n",
        "# =============================================================================\n",
        "class DecoderAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder LSTM v·ªõi Luong Attention.\n",
        "    \n",
        "    Ki·∫øn tr√∫c:\n",
        "    1. Embed input token\n",
        "    2. T√≠nh attention weights v√† context vector\n",
        "    3. Concat [embedding; context] ‚Üí LSTM input\n",
        "    4. LSTM forward\n",
        "    5. Concat [hidden; context] ‚Üí Linear ‚Üí vocab\n",
        "    \n",
        "    Args:\n",
        "        output_dim: K√≠ch th∆∞·ªõc vocabulary ƒë√≠ch\n",
        "        emb_dim: Embedding dimension\n",
        "        hid_dim: Hidden state dimension\n",
        "        n_layers: S·ªë l·ªõp LSTM\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.attention = Attention(hid_dim)\n",
        "        \n",
        "        # LSTM nh·∫≠n: embedding + context = emb_dim + hid_dim\n",
        "        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        \n",
        "        # Linear nh·∫≠n: hidden + context = hid_dim * 2\n",
        "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs, mask):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Decoder v·ªõi Attention (1 timestep).\n",
        "        \n",
        "        Args:\n",
        "            input: [batch_size] - Token hi·ªán t·∫°i\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Hidden states\n",
        "            cell: [n_layers, batch_size, hid_dim] - Cell states\n",
        "            encoder_outputs: [src_len, batch_size, hid_dim] - Encoder outputs\n",
        "            mask: [batch_size, src_len] - Mask cho padding\n",
        "        \n",
        "        Returns:\n",
        "            prediction: [batch_size, output_dim] - Logits cho vocabulary\n",
        "            hidden: [n_layers, batch_size, hid_dim] - Updated hidden\n",
        "            cell: [n_layers, batch_size, hid_dim] - Updated cell\n",
        "            attention_weights: [batch_size, src_len] - Attention weights\n",
        "        \"\"\"\n",
        "        # input: [batch_size] ‚Üí [1, batch_size]\n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # Embed\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded: [1, batch_size, emb_dim]\n",
        "        \n",
        "        # T√≠nh Attention: d√πng hidden state c·ªßa layer cu·ªëi c√πng\n",
        "        # hidden[-1]: [batch_size, hid_dim]\n",
        "        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
        "        # context: [batch_size, hid_dim]\n",
        "        # attention_weights: [batch_size, src_len]\n",
        "        \n",
        "        # Concat embedding v√† context cho LSTM input\n",
        "        # embedded: [1, batch_size, emb_dim]\n",
        "        # context: [batch_size, hid_dim] ‚Üí [1, batch_size, hid_dim]\n",
        "        lstm_input = torch.cat([embedded, context.unsqueeze(0)], dim=2)\n",
        "        # lstm_input: [1, batch_size, emb_dim + hid_dim]\n",
        "        \n",
        "        # LSTM forward\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: [1, batch_size, hid_dim]\n",
        "        \n",
        "        # Concat hidden v√† context cho prediction\n",
        "        # output.squeeze(0): [batch_size, hid_dim]\n",
        "        # context: [batch_size, hid_dim]\n",
        "        combined = torch.cat([output.squeeze(0), context], dim=1)\n",
        "        # combined: [batch_size, hid_dim * 2]\n",
        "        \n",
        "        # Prediction\n",
        "        prediction = self.fc_out(combined)\n",
        "        # prediction: [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden, cell, attention_weights\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS: Seq2SeqAttention\n",
        "# =============================================================================\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    M√¥ h√¨nh Seq2Seq v·ªõi Luong Attention (M√¥ h√¨nh ch√≠nh).\n",
        "    \n",
        "    ƒêi·ªÅu ph·ªëi EncoderAttention v√† DecoderAttention.\n",
        "    \n",
        "    Args:\n",
        "        encoder: EncoderAttention instance\n",
        "        decoder: DecoderAttention instance\n",
        "        device: 'cuda' ho·∫∑c 'cpu'\n",
        "        pad_idx: Index c·ªßa padding token\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device, pad_idx):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.pad_idx = pad_idx\n",
        "        \n",
        "        # ƒê·∫£m b·∫£o encoder v√† decoder t∆∞∆°ng th√≠ch\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must match!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Number of layers of encoder and decoder must match!\"\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        \"\"\"\n",
        "        T·∫°o mask cho padding positions.\n",
        "        \n",
        "        Args:\n",
        "            src: [src_len, batch_size]\n",
        "        \n",
        "        Returns:\n",
        "            mask: [batch_size, src_len] - 1 cho valid, 0 cho padding\n",
        "        \"\"\"\n",
        "        mask = (src != self.pad_idx).permute(1, 0)\n",
        "        # mask: [batch_size, src_len]\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass c·ªßa Seq2Seq v·ªõi Attention.\n",
        "        \n",
        "        Args:\n",
        "            src: [src_len, batch_size] - C√¢u ngu·ªìn\n",
        "            src_len: [batch_size] - ƒê·ªô d√†i c√¢u ngu·ªìn\n",
        "            trg: [trg_len, batch_size] - C√¢u ƒë√≠ch\n",
        "            teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng ground truth\n",
        "        \n",
        "        Returns:\n",
        "            outputs: [trg_len, batch_size, output_dim] - Logits cho m·ªói timestep\n",
        "        \"\"\"\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # Tensor l∆∞u outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # Encode - gi·ªù tr·∫£ v·ªÅ th√™m encoder_outputs\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_len)\n",
        "        # encoder_outputs: [src_len, batch_size, hid_dim]\n",
        "        # hidden: [n_layers, batch_size, hid_dim]\n",
        "        # cell: [n_layers, batch_size, hid_dim]\n",
        "        \n",
        "        # T·∫°o mask cho padding\n",
        "        mask = self.create_mask(src)\n",
        "        # mask: [batch_size, src_len]\n",
        "        \n",
        "        # Token ƒë·∫ßu ti√™n l√† <sos>\n",
        "        input = trg[0, :]\n",
        "        \n",
        "        # Decode t·ª´ng timestep\n",
        "        for t in range(1, trg_len):\n",
        "            # Forward decoder v·ªõi attention\n",
        "            output, hidden, cell, attention_weights = self.decoder(\n",
        "                input, hidden, cell, encoder_outputs, mask\n",
        "            )\n",
        "            # output: [batch_size, output_dim]\n",
        "            # attention_weights: [batch_size, src_len]\n",
        "            \n",
        "            # L∆∞u output\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # Teacher Forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: Attention, EncoderAttention, DecoderAttention, Seq2SeqAttention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3.2: KH·ªûI T·∫†O ATTENTION MODEL V√Ä KI·ªÇM TRA\n",
        "# ==============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# KH·ªûI T·∫†O MODEL V·ªöI ATTENTION\n",
        "# =============================================================================\n",
        "# Hyperparameters gi·ªØ nguy√™n t·ª´ baseline\n",
        "enc_attn = EncoderAttention(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec_attn = DecoderAttention(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "# ‚ö†Ô∏è QUAN TR·ªåNG: ƒê·∫∑t t√™n bi·∫øn l√† attention_model ƒë·ªÉ ph√¢n bi·ªát v·ªõi baseline_model\n",
        "attention_model = Seq2SeqAttention(enc_attn, dec_attn, device, PAD_IDX).to(device)\n",
        "\n",
        "# √Åp d·ª•ng weight initialization\n",
        "attention_model.apply(init_weights)\n",
        "\n",
        "# ƒê·∫øm parameters\n",
        "attention_params = sum(p.numel() for p in attention_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üéØ M√î H√åNH SEQ2SEQ + LUONG ATTENTION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìå ƒê·∫∑c ƒëi·ªÉm: S·ª≠ d·ª•ng DYNAMIC CONTEXT VECTOR\")\n",
        "print(\"   (Attention weights thay ƒë·ªïi theo t·ª´ng timestep)\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Device:           {device}\")\n",
        "print(f\"Input dim (EN):   {INPUT_DIM:,}\")\n",
        "print(f\"Output dim (FR):  {OUTPUT_DIM:,}\")\n",
        "print(f\"Embedding dim:    {ENC_EMB_DIM}\")\n",
        "print(f\"Hidden dim:       {HID_DIM}\")\n",
        "print(f\"Num layers:       {N_LAYERS}\")\n",
        "print(f\"Dropout:          {ENC_DROPOUT}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Attention params: {attention_params:,}\")\n",
        "print(f\"Baseline params:  {baseline_params:,}\")\n",
        "print(f\"Th√™m (Attention): {attention_params - baseline_params:,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# =============================================================================\n",
        "# KI·ªÇM TRA FORWARD PASS\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üîç KI·ªÇM TRA FORWARD PASS (ATTENTION MODEL)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # L·∫•y 1 batch t·ª´ train_loader\n",
        "    src, trg, src_len = next(iter(train_loader))\n",
        "    src = src.to(device)\n",
        "    trg = trg.to(device)\n",
        "    \n",
        "    print(f\"Input (src):      {src.shape}  [seq_len, batch_size]\")\n",
        "    print(f\"Input (src_len):  {src_len.shape}  [batch_size]\")\n",
        "    print(f\"Target (trg):     {trg.shape}  [seq_len, batch_size]\")\n",
        "    \n",
        "    # Test Encoder\n",
        "    encoder_outputs, hidden, cell = attention_model.encoder(src, src_len)\n",
        "    print(f\"\\nEncoder outputs:  {encoder_outputs.shape}  [src_len, batch_size, hid_dim]\")\n",
        "    print(f\"Hidden:           {hidden.shape}  [n_layers, batch_size, hid_dim]\")\n",
        "    print(f\"Cell:             {cell.shape}  [n_layers, batch_size, hid_dim]\")\n",
        "    \n",
        "    # Test full forward pass\n",
        "    output = attention_model(src, src_len, trg)\n",
        "    print(f\"\\nOutput:           {output.shape}  [trg_len, batch_size, vocab_size]\")\n",
        "    \n",
        "    # Validate output shape\n",
        "    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)\n",
        "    if output.shape == expected_shape:\n",
        "        print(\"\\n‚úÖ ATTENTION FORWARD PASS TH√ÄNH C√îNG!\")\n",
        "        print(\"   ‚Üí Attention Model s·∫µn s√†ng cho Training.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Shape kh√¥ng kh·ªõp! Expected: {expected_shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå L·ªñI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ‚ö†Ô∏è KH√îNG g√°n l·∫°i bi·∫øn - s·ª≠ d·ª•ng baseline_model v√† attention_model ri√™ng bi·ªát\n",
        "# ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c·∫£ hai m√¥ h√¨nh t·ªìn t·∫°i ƒë·ªôc l·∫≠p ƒë·ªÉ so s√°nh\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìå HAI M√î H√åNH ƒê√É KH·ªûI T·∫†O:\")\n",
        "print(\"   1. baseline_model  - Seq2Seq v·ªõi Fixed Context Vector\")\n",
        "print(\"   2. attention_model - Seq2Seq v·ªõi Luong Attention\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úÖ PH·∫¶N 3 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 4 (Training)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ PH·∫¶N 4: TRAINING PROCESS\n",
        "\n",
        "---\n",
        "\n",
        "## Quy tr√¨nh hu·∫•n luy·ªán\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                         TRAINING LOOP                               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                     ‚îÇ\n",
        "‚îÇ  for epoch in range(N_EPOCHS):                                      ‚îÇ\n",
        "‚îÇ      ‚îú‚îÄ‚îÄ train_loss = train(model, train_loader, ...)              ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ Forward pass (v·ªõi Teacher Forcing 0.5)            ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ T√≠nh loss (b·ªè <sos>, ignore <pad>)                ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ Backward + Gradient Clipping                       ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Update weights                                     ‚îÇ\n",
        "‚îÇ      ‚îÇ                                                              ‚îÇ\n",
        "‚îÇ      ‚îú‚îÄ‚îÄ valid_loss = evaluate(model, valid_loader, ...)           ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Forward pass (kh√¥ng Teacher Forcing)              ‚îÇ\n",
        "‚îÇ      ‚îÇ                                                              ‚îÇ\n",
        "‚îÇ      ‚îú‚îÄ‚îÄ if valid_loss < best_loss:                                ‚îÇ\n",
        "‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Save checkpoint (best_model.pth)                  ‚îÇ\n",
        "‚îÇ      ‚îÇ                                                              ‚îÇ\n",
        "‚îÇ      ‚îî‚îÄ‚îÄ if no_improvement >= PATIENCE:                            ‚îÇ\n",
        "‚îÇ              ‚îî‚îÄ‚îÄ EARLY STOPPING                                     ‚îÇ\n",
        "‚îÇ                                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Hyperparameters (theo ƒê·ªí √ÅN):**\n",
        "\n",
        "| Parameter | Value | M√¥ t·∫£ |\n",
        "|-----------|-------|-------|\n",
        "| `N_EPOCHS` | 20 | S·ªë epoch t·ªëi ƒëa |\n",
        "| `LEARNING_RATE` | 0.001 | Learning rate (Adam) |\n",
        "| `CLIP` | 1.0 | Gradient clipping |\n",
        "| `TEACHER_FORCING` | 0.5 | T·ª∑ l·ªá Teacher Forcing |\n",
        "| `PATIENCE` | 3 | Early Stopping patience |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.1: C·∫§U H√åNH TRAINING V√Ä HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "# =============================================================================\n",
        "# HYPERPARAMETERS (THEO ƒê·ªí √ÅN)\n",
        "# =============================================================================\n",
        "N_EPOCHS = 20                    # S·ªë epoch t·ªëi ƒëa\n",
        "CLIP = 1.0                       # Gradient clipping\n",
        "LEARNING_RATE = 0.001            # Learning rate\n",
        "PATIENCE = 3                     # Early Stopping patience\n",
        "TEACHER_FORCING_RATIO = 0.5      # T·ª∑ l·ªá Teacher Forcing\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER FUNCTION\n",
        "# =============================================================================\n",
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"T√≠nh th·ªùi gian ch·∫°y 1 epoch (ph√∫t, gi√¢y).\"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "print(\"‚úÖ ƒê√£ c·∫•u h√¨nh hyperparameters:\")\n",
        "print(f\"   N_EPOCHS = {N_EPOCHS}\")\n",
        "print(f\"   LEARNING_RATE = {LEARNING_RATE}\")\n",
        "print(f\"   CLIP = {CLIP}\")\n",
        "print(f\"   TEACHER_FORCING_RATIO = {TEACHER_FORCING_RATIO}\")\n",
        "print(f\"   PATIENCE = {PATIENCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.2: H√ÄM TRAIN V√Ä EVALUATE\n",
        "# ==============================================================================\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip, device, teacher_forcing_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Hu·∫•n luy·ªán model trong 1 epoch.\n",
        "    \n",
        "    Args:\n",
        "        model: M√¥ h√¨nh Seq2Seq (v·ªõi Attention)\n",
        "        iterator: DataLoader train\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: CrossEntropyLoss (v·ªõi ignore_index=PAD_IDX)\n",
        "        clip: Gradient clipping value\n",
        "        device: 'cuda' ho·∫∑c 'cpu'\n",
        "        teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng Teacher Forcing\n",
        "        \n",
        "    Returns:\n",
        "        epoch_loss: Loss trung b√¨nh c·ªßa epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    progress_bar = tqdm(iterator, desc=\"Training\", leave=False)\n",
        "    \n",
        "    for batch in progress_bar:\n",
        "        # ===== 1. UNPACK BATCH =====\n",
        "        src, trg, src_len = batch\n",
        "        \n",
        "        # Chuy·ªÉn src, trg l√™n device\n",
        "        src = src.to(device)         # [src_len, batch_size]\n",
        "        trg = trg.to(device)         # [trg_len, batch_size]\n",
        "        # ‚ö†Ô∏è src_len PH·∫¢I n·∫±m tr√™n CPU cho pack_padded_sequence!\n",
        "        \n",
        "        # ===== 2. FORWARD PASS =====\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward v·ªõi teacher_forcing_ratio\n",
        "        output = model(src, src_len, trg, teacher_forcing_ratio)\n",
        "        # output: [trg_len, batch_size, output_dim]\n",
        "        \n",
        "        # ===== 3. T√çNH LOSS =====\n",
        "        # üìå LOGIC SLICING:\n",
        "        # - output[0] l√† zeros tensor (do loop b·∫Øt ƒë·∫ßu t·ª´ t=1)\n",
        "        # - trg[0] l√† <sos> token\n",
        "        # - Ph·∫£i b·ªè c·∫£ hai tr∆∞·ªõc khi t√≠nh loss\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:]   # [trg_len-1, batch_size, output_dim]\n",
        "        trg = trg[1:]         # [trg_len-1, batch_size]\n",
        "        \n",
        "        # Reshape v·ªÅ 2D cho CrossEntropyLoss\n",
        "        output = output.reshape(-1, output_dim)  # [(trg_len-1)*batch_size, output_dim]\n",
        "        trg = trg.reshape(-1)                    # [(trg_len-1)*batch_size]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        # ===== 4. BACKWARD PASS =====\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping ƒë·ªÉ tr√°nh exploding gradient\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    \"\"\"\n",
        "    ƒê√°nh gi√° model tr√™n t·∫≠p validation/test.\n",
        "    \n",
        "    Args:\n",
        "        model: M√¥ h√¨nh Seq2Seq\n",
        "        iterator: DataLoader val/test\n",
        "        criterion: CrossEntropyLoss\n",
        "        device: 'cuda' ho·∫∑c 'cpu'\n",
        "        \n",
        "    Returns:\n",
        "        epoch_loss: Loss trung b√¨nh\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
        "            src, trg, src_len = batch\n",
        "            \n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            # src_len gi·ªØ nguy√™n tr√™n CPU\n",
        "            \n",
        "            # Forward v·ªõi teacher_forcing_ratio = 0 (kh√¥ng d√πng ground truth)\n",
        "            output = model(src, src_len, trg, teacher_forcing_ratio=0)\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:]\n",
        "            trg = trg[1:]\n",
        "            \n",
        "            output = output.reshape(-1, output_dim)\n",
        "            trg = trg.reshape(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: train(), evaluate()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 HU·∫§N LUY·ªÜN BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)\n",
        "\n",
        "---\n",
        "\n",
        "**M·ª•c ti√™u:** Hu·∫•n luy·ªán m√¥ h√¨nh Baseline ƒë·ªÉ ch·ª©ng minh ho·∫°t ƒë·ªông c·ªßa context vector c·ªë ƒë·ªãnh.\n",
        "\n",
        "> ‚ö†Ô∏è **QUAN TR·ªåNG:** ƒê√¢y l√† b∆∞·ªõc B·∫ÆT BU·ªòC ƒë·ªÉ ƒë√°p ·ª©ng ti√™u ch√≠ 1 (3.0 ƒëi·ªÉm).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.1a: HU·∫§N LUY·ªÜN BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)\n",
        "# ==============================================================================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import math\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üèóÔ∏è HU·∫§N LUY·ªÜN BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìå M√¥ h√¨nh n√†y s·ª≠ d·ª•ng context vector C·ªê ƒê·ªäNH\")\n",
        "print(\"   (Ch·ªâ hidden state cu·ªëi c√πng c·ªßa Encoder)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# C·∫§U H√åNH TRAINING CHO BASELINE\n",
        "# =============================================================================\n",
        "BASELINE_EPOCHS = 3              # ƒê·ªß ƒë·ªÉ ch·ª©ng minh m√¥ h√¨nh ho·∫°t ƒë·ªông\n",
        "LEARNING_RATE = 0.001\n",
        "CLIP = 1.0\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "\n",
        "# Optimizer & Criterion cho Baseline\n",
        "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# History tracking\n",
        "baseline_history = {\n",
        "    'train_loss': [],\n",
        "    'valid_loss': [],\n",
        "    'train_ppl': [],\n",
        "    'valid_ppl': []\n",
        "}\n",
        "\n",
        "print(f\"Epochs:           {BASELINE_EPOCHS}\")\n",
        "print(f\"Learning Rate:    {LEARNING_RATE}\")\n",
        "print(f\"Gradient Clip:    {CLIP}\")\n",
        "print(f\"Teacher Forcing:  {TEACHER_FORCING_RATIO}\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING LOOP CHO BASELINE\n",
        "# =============================================================================\n",
        "best_baseline_loss = float('inf')\n",
        "\n",
        "for epoch in range(BASELINE_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # ===== TRAIN =====\n",
        "    baseline_model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=f\"Baseline Epoch {epoch+1}/{BASELINE_EPOCHS}\", leave=False):\n",
        "        src, trg, src_len = batch\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        baseline_optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = baseline_model(src, src_len, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        # Reshape for loss\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = baseline_criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(baseline_model.parameters(), CLIP)\n",
        "        baseline_optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # ===== EVALUATE =====\n",
        "    baseline_model.eval()\n",
        "    valid_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
        "            src, trg, src_len = batch\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            \n",
        "            output = baseline_model(src, src_len, trg, 0)  # No teacher forcing\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            \n",
        "            loss = baseline_criterion(output, trg)\n",
        "            valid_loss += loss.item()\n",
        "    \n",
        "    valid_loss /= len(valid_loader)\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    train_ppl = math.exp(train_loss)\n",
        "    valid_ppl = math.exp(valid_loss)\n",
        "    \n",
        "    # Save history\n",
        "    baseline_history['train_loss'].append(train_loss)\n",
        "    baseline_history['valid_loss'].append(valid_loss)\n",
        "    baseline_history['train_ppl'].append(train_ppl)\n",
        "    baseline_history['valid_ppl'].append(valid_ppl)\n",
        "    \n",
        "    # Save best model\n",
        "    if valid_loss < best_baseline_loss:\n",
        "        best_baseline_loss = valid_loss\n",
        "        torch.save(baseline_model.state_dict(), 'baseline_model.pth')\n",
        "        save_status = \"‚úÖ Model saved!\"\n",
        "    else:\n",
        "        save_status = \"\"\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins = int((end_time - start_time) / 60)\n",
        "    epoch_secs = int((end_time - start_time) % 60)\n",
        "    \n",
        "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}\")\n",
        "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f} {save_status}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ BASELINE TRAINING HO√ÄN T·∫§T!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Best Validation Loss: {best_baseline_loss:.3f}\")\n",
        "print(f\"Best Validation PPL:  {math.exp(best_baseline_loss):.3f}\")\n",
        "print(f\"Model ƒë√£ l∆∞u t·∫°i:     'baseline_model.pth'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 HU·∫§N LUY·ªÜN ATTENTION SEQ2SEQ (M√î H√åNH CH√çNH)\n",
        "\n",
        "---\n",
        "\n",
        "**M·ª•c ti√™u:** Hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq + Luong Attention v·ªõi Early Stopping.\n",
        "\n",
        "**So s√°nh v·ªõi Baseline:**\n",
        "- Baseline: Context vector C·ªê ƒê·ªäNH (ch·ªâ hidden state cu·ªëi)\n",
        "- Attention: Context vector ƒê·ªòNG (weighted sum c·ªßa t·∫•t c·∫£ encoder outputs)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.3: KH·ªûI T·∫†O OPTIMIZER & CRITERION CHO ATTENTION MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "# Optimizer: Adam cho attention_model\n",
        "attention_optimizer = torch.optim.Adam(attention_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss function: CrossEntropyLoss v·ªõi ignore_index ƒë·ªÉ b·ªè qua PAD token\n",
        "attention_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# C·∫•u h√¨nh training\n",
        "N_EPOCHS = 20                    # S·ªë epoch t·ªëi ƒëa\n",
        "PATIENCE = 3                     # Early Stopping patience\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üéØ C·∫§U H√åNH HU·∫§N LUY·ªÜN ATTENTION MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device:              {device}\")\n",
        "print(f\"Model:               {attention_model.__class__.__name__}\")\n",
        "print(f\"Total Parameters:    {attention_params:,}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Optimizer:           Adam (lr={LEARNING_RATE})\")\n",
        "print(f\"Loss:                CrossEntropyLoss (ignore_index={PAD_IDX})\")\n",
        "print(f\"Epochs:              {N_EPOCHS} (max)\")\n",
        "print(f\"Gradient Clip:       {CLIP}\")\n",
        "print(f\"Teacher Forcing:     {TEACHER_FORCING_RATIO}\")\n",
        "print(f\"Early Stopping:      patience={PATIENCE}\")\n",
        "print(f\"Batch Size:          {BATCH_SIZE}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.4: V√íNG L·∫∂P HU·∫§N LUY·ªÜN ATTENTION MODEL (V·ªöI EARLY STOPPING)\n",
        "# ==============================================================================\n",
        "\n",
        "# Bi·∫øn theo d√µi\n",
        "best_valid_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "attention_history = {\n",
        "    'train_loss': [],\n",
        "    'valid_loss': [],\n",
        "    'train_ppl': [],\n",
        "    'valid_ppl': []\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ATTENTION MODEL\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # ===== TRAIN =====\n",
        "    attention_model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc=f\"Attention Epoch {epoch+1}/{N_EPOCHS}\", leave=False):\n",
        "        src, trg, src_len = batch\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        attention_optimizer.zero_grad()\n",
        "        \n",
        "        output = attention_model(src, src_len, trg, TEACHER_FORCING_RATIO)\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "        \n",
        "        loss = attention_criterion(output, trg)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(attention_model.parameters(), CLIP)\n",
        "        attention_optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # ===== EVALUATE =====\n",
        "    attention_model.eval()\n",
        "    valid_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
        "            src, trg, src_len = batch\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            \n",
        "            output = attention_model(src, src_len, trg, 0)\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            \n",
        "            loss = attention_criterion(output, trg)\n",
        "            valid_loss += loss.item()\n",
        "    \n",
        "    valid_loss /= len(valid_loader)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins = int((end_time - start_time) / 60)\n",
        "    epoch_secs = int((end_time - start_time) % 60)\n",
        "    \n",
        "    # ===== T√çNH PERPLEXITY =====\n",
        "    train_ppl = math.exp(train_loss)\n",
        "    valid_ppl = math.exp(valid_loss)\n",
        "    \n",
        "    # L∆∞u history\n",
        "    attention_history['train_loss'].append(train_loss)\n",
        "    attention_history['valid_loss'].append(valid_loss)\n",
        "    attention_history['train_ppl'].append(train_ppl)\n",
        "    attention_history['valid_ppl'].append(valid_ppl)\n",
        "    \n",
        "    # ===== CHECKPOINTING & EARLY STOPPING =====\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        epochs_without_improvement = 0\n",
        "        \n",
        "        # L∆∞u best model\n",
        "        torch.save(attention_model.state_dict(), 'attention_best_model.pth')\n",
        "        save_status = \"‚úÖ Model saved!\"\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        save_status = f\"‚ö†Ô∏è No improvement ({epochs_without_improvement}/{PATIENCE})\"\n",
        "    \n",
        "    # ===== IN K·∫æT QU·∫¢ =====\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n",
        "    print(f'\\t{save_status}')\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # ===== KI·ªÇM TRA EARLY STOPPING =====\n",
        "    if epochs_without_improvement >= PATIENCE:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"‚õî EARLY STOPPING: Val loss kh√¥ng gi·∫£m sau {PATIENCE} epochs\")\n",
        "        print(\"=\" * 60)\n",
        "        break\n",
        "\n",
        "# =============================================================================\n",
        "# T·ªîNG K·∫æT\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ ATTENTION TRAINING HO√ÄN T·∫§T!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Epochs ƒë√£ ch·∫°y:       {epoch + 1}\")\n",
        "print(f\"Best Validation Loss: {best_valid_loss:.3f}\")\n",
        "print(f\"Best Validation PPL:  {math.exp(best_valid_loss):.3f}\")\n",
        "print(f\"Model ƒë√£ l∆∞u t·∫°i:     'attention_best_model.pth'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.5: SO S√ÅNH K·∫æT QU·∫¢ BASELINE VS ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä SO S√ÅNH K·∫æT QU·∫¢ TRAINING: BASELINE VS ATTENTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load best models\n",
        "baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))\n",
        "attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))\n",
        "\n",
        "# Evaluate on test set\n",
        "def evaluate_model(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src, trg, src_len = batch\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            output = model(src, src_len, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Evaluate both models\n",
        "baseline_test_loss = evaluate_model(baseline_model, test_loader, baseline_criterion, device)\n",
        "attention_test_loss = evaluate_model(attention_model, test_loader, attention_criterion, device)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"üìå BASELINE SEQ2SEQ (Fixed Context Vector):\")\n",
        "print(f\"   Test Loss: {baseline_test_loss:.3f} | Test PPL: {math.exp(baseline_test_loss):7.3f}\")\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"üéØ ATTENTION SEQ2SEQ (Dynamic Context Vector):\")\n",
        "print(f\"   Test Loss: {attention_test_loss:.3f} | Test PPL: {math.exp(attention_test_loss):7.3f}\")\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "\n",
        "# Improvement\n",
        "improvement = baseline_test_loss - attention_test_loss\n",
        "ppl_improvement = math.exp(baseline_test_loss) - math.exp(attention_test_loss)\n",
        "print(f\"üìà C·∫£i thi·ªán khi d√πng Attention:\")\n",
        "print(f\"   Loss gi·∫£m:       {improvement:.3f}\")\n",
        "print(f\"   Perplexity gi·∫£m: {ppl_improvement:.3f}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úÖ PH·∫¶N 4 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 5 (Inference & Evaluation)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä PH·∫¶N 5: INFERENCE & BLEU EVALUATION\n",
        "\n",
        "---\n",
        "\n",
        "## Quy tr√¨nh Inference\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                        GREEDY DECODING                              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                     ‚îÇ\n",
        "‚îÇ  Input (EN): \"A man walks\"                                          ‚îÇ\n",
        "‚îÇ       ‚Üì                                                             ‚îÇ\n",
        "‚îÇ  Tokenize + Numericalize                                            ‚îÇ\n",
        "‚îÇ       ‚Üì                                                             ‚îÇ\n",
        "‚îÇ  Encoder ‚Üí (encoder_outputs, hidden, cell)                          ‚îÇ\n",
        "‚îÇ       ‚Üì                                                             ‚îÇ\n",
        "‚îÇ  Loop until <eos> or MAX_LEN:                                       ‚îÇ\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ Decoder(input, hidden, cell, encoder_outputs, mask)       ‚îÇ\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ argmax(output) ‚Üí predicted token                          ‚îÇ\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ Append to result                                          ‚îÇ\n",
        "‚îÇ       ‚Üì                                                             ‚îÇ\n",
        "‚îÇ  Output (FR): \"Un homme marche\"                                     ‚îÇ\n",
        "‚îÇ                                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**BLEU Score Evaluation:**\n",
        "- S·ª≠ d·ª•ng `nltk.translate.bleu_score`\n",
        "- Smoothing function ƒë·ªÉ x·ª≠ l√Ω n-gram = 0\n",
        "- ƒê√°nh gi√° tr√™n to√†n b·ªô t·∫≠p Test\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5.1: H√ÄM TRANSLATE (CHO C·∫¢ BASELINE V√Ä ATTENTION)\n",
        "# ==============================================================================\n",
        "\n",
        "def translate_attention(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    D·ªãch m·ªôt c√¢u ti·∫øng Anh sang ti·∫øng Ph√°p b·∫±ng Attention Model.\n",
        "    S·ª≠ d·ª•ng Greedy Decoding.\n",
        "    \"\"\"\n",
        "    MAX_LEN = 50\n",
        "    attention_model.eval()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenizer_en(sentence.lower())\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    \n",
        "    # Numericalize\n",
        "    src_indexes = [vocab_en[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)\n",
        "    \n",
        "    # Encoder forward\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden, cell = attention_model.encoder(src_tensor, src_len)\n",
        "    \n",
        "    # Mask\n",
        "    mask = (src_tensor != PAD_IDX).permute(1, 0)\n",
        "    \n",
        "    # Greedy decoding\n",
        "    trg_indexes = [SOS_IDX]\n",
        "    \n",
        "    for _ in range(MAX_LEN):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell, attention = attention_model.decoder(\n",
        "                trg_tensor, hidden, cell, encoder_outputs, mask\n",
        "            )\n",
        "        \n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        \n",
        "        if pred_token == EOS_IDX:\n",
        "            break\n",
        "    \n",
        "    # Convert to words\n",
        "    trg_tokens = [vocab_fr.get_itos()[i] for i in trg_indexes]\n",
        "    \n",
        "    if trg_tokens[0] == '<sos>':\n",
        "        trg_tokens = trg_tokens[1:]\n",
        "    if '<eos>' in trg_tokens:\n",
        "        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]\n",
        "    \n",
        "    return ' '.join(trg_tokens)\n",
        "\n",
        "\n",
        "def translate_baseline(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    D·ªãch m·ªôt c√¢u ti·∫øng Anh sang ti·∫øng Ph√°p b·∫±ng Baseline Model.\n",
        "    S·ª≠ d·ª•ng Greedy Decoding v·ªõi FIXED context vector.\n",
        "    \"\"\"\n",
        "    MAX_LEN = 50\n",
        "    baseline_model.eval()\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenizer_en(sentence.lower())\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    \n",
        "    # Numericalize\n",
        "    src_indexes = [vocab_en[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)\n",
        "    \n",
        "    # Encoder forward (baseline ch·ªâ tr·∫£ v·ªÅ hidden, cell)\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = baseline_model.encoder(src_tensor, src_len)\n",
        "    \n",
        "    # Greedy decoding\n",
        "    trg_indexes = [SOS_IDX]\n",
        "    \n",
        "    for _ in range(MAX_LEN):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = baseline_model.decoder(\n",
        "                trg_tensor, hidden, cell\n",
        "            )\n",
        "        \n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        \n",
        "        if pred_token == EOS_IDX:\n",
        "            break\n",
        "    \n",
        "    # Convert to words\n",
        "    trg_tokens = [vocab_fr.get_itos()[i] for i in trg_indexes]\n",
        "    \n",
        "    if trg_tokens[0] == '<sos>':\n",
        "        trg_tokens = trg_tokens[1:]\n",
        "    if '<eos>' in trg_tokens:\n",
        "        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]\n",
        "    \n",
        "    return ' '.join(trg_tokens)\n",
        "\n",
        "\n",
        "# Alias cho backward compatibility\n",
        "translate = translate_attention\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m translate_attention() v√† translate_baseline()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5.2: T√çNH BLEU SCORE (NLTK)\n",
        "# ==============================================================================\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu_score(test_src, test_trg, num_samples=None):\n",
        "    \"\"\"\n",
        "    T√≠nh ƒëi·ªÉm BLEU trung b√¨nh tr√™n t·∫≠p Test.\n",
        "    \n",
        "    Args:\n",
        "        test_src: List c√¢u ngu·ªìn (ti·∫øng Anh)\n",
        "        test_trg: List c√¢u ƒë√≠ch (ti·∫øng Ph√°p)\n",
        "        num_samples: S·ªë m·∫´u ƒë·ªÉ ƒë√°nh gi√° (None = to√†n b·ªô)\n",
        "        \n",
        "    Returns:\n",
        "        bleu_avg: ƒêi·ªÉm BLEU trung b√¨nh (0-1)\n",
        "    \"\"\"\n",
        "    # Smoothing ƒë·ªÉ x·ª≠ l√Ω n-gram = 0\n",
        "    smooth = SmoothingFunction().method1\n",
        "    \n",
        "    total_bleu = 0\n",
        "    count = 0\n",
        "    \n",
        "    # Gi·ªõi h·∫°n s·ªë m·∫´u n·∫øu c·∫ßn\n",
        "    if num_samples:\n",
        "        indices = random.sample(range(len(test_src)), min(num_samples, len(test_src)))\n",
        "    else:\n",
        "        indices = range(len(test_src))\n",
        "    \n",
        "    print(\"üìä ƒêang t√≠nh BLEU Score...\")\n",
        "    \n",
        "    for idx in tqdm(indices, desc=\"Calculating BLEU\"):\n",
        "        src_sentence = test_src[idx]\n",
        "        trg_sentence = test_trg[idx]\n",
        "        \n",
        "        # D·ªãch c√¢u\n",
        "        pred_sentence = translate(src_sentence)\n",
        "        \n",
        "        # Tokenize\n",
        "        pred_tokens = pred_sentence.split()\n",
        "        ref_tokens = tokenizer_fr(trg_sentence.lower())\n",
        "        \n",
        "        # NLTK sentence_bleu format\n",
        "        reference = [ref_tokens]  # List of list\n",
        "        hypothesis = pred_tokens\n",
        "        \n",
        "        try:\n",
        "            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth)\n",
        "            total_bleu += bleu\n",
        "            count += 1\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    bleu_avg = total_bleu / count if count > 0 else 0\n",
        "    return bleu_avg\n",
        "\n",
        "\n",
        "def demo_translation(test_src, test_trg, num_examples=5):\n",
        "    \"\"\"\n",
        "    Demo d·ªãch m·ªôt s·ªë c√¢u ng·∫´u nhi√™n t·ª´ t·∫≠p Test.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üîç DEMO D·ªäCH M·∫™U\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Ch·ªçn ng·∫´u nhi√™n\n",
        "    indices = random.sample(range(len(test_src)), num_examples)\n",
        "    \n",
        "    for i, idx in enumerate(indices, 1):\n",
        "        src = test_src[idx]\n",
        "        trg = test_trg[idx]\n",
        "        pred = translate(src)\n",
        "        \n",
        "        print(f\"\\n--- V√≠ d·ª• {i} ---\")\n",
        "        print(f\"üì• Source (EN):     {src}\")\n",
        "        print(f\"üìå Reference (FR):  {trg}\")\n",
        "        print(f\"ü§ñ Predicted (FR):  {pred}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: calculate_bleu_score(), demo_translation()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5.3: DEMO SO S√ÅNH BASELINE VS ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä DEMO SO S√ÅNH: BASELINE VS ATTENTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load best models\n",
        "baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))\n",
        "attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))\n",
        "baseline_model.eval()\n",
        "attention_model.eval()\n",
        "print(\"‚úÖ ƒê√£ load c·∫£ hai models\\n\")\n",
        "\n",
        "# Demo 5 c√¢u ng·∫´u nhi√™n\n",
        "indices = random.sample(range(len(test_en)), 5)\n",
        "\n",
        "for i, idx in enumerate(indices, 1):\n",
        "    src = test_en[idx]\n",
        "    trg = test_fr[idx]\n",
        "    pred_baseline = translate_baseline(src)\n",
        "    pred_attention = translate_attention(src)\n",
        "    \n",
        "    print(f\"\\n--- V√≠ d·ª• {i} ---\")\n",
        "    print(f\"üì• Source (EN):     {src}\")\n",
        "    print(f\"üìå Reference (FR):  {trg}\")\n",
        "    print(f\"üèóÔ∏è Baseline (FR):   {pred_baseline}\")\n",
        "    print(f\"üéØ Attention (FR):  {pred_attention}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5.4: ƒê√ÅNH GI√Å BLEU SCORE - BASELINE VS ATTENTION\n",
        "# ==============================================================================\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu(model_translate_fn, test_src, test_trg, num_samples=None):\n",
        "    \"\"\"\n",
        "    T√≠nh ƒëi·ªÉm BLEU trung b√¨nh cho m·ªôt h√†m translate.\n",
        "    \"\"\"\n",
        "    smooth = SmoothingFunction().method1\n",
        "    total_bleu = 0\n",
        "    count = 0\n",
        "    \n",
        "    if num_samples:\n",
        "        indices = random.sample(range(len(test_src)), min(num_samples, len(test_src)))\n",
        "    else:\n",
        "        indices = range(len(test_src))\n",
        "    \n",
        "    for idx in tqdm(indices, desc=\"Calculating BLEU\"):\n",
        "        src_sentence = test_src[idx]\n",
        "        trg_sentence = test_trg[idx]\n",
        "        \n",
        "        pred_sentence = model_translate_fn(src_sentence)\n",
        "        \n",
        "        pred_tokens = pred_sentence.split()\n",
        "        ref_tokens = tokenizer_fr(trg_sentence.lower())\n",
        "        \n",
        "        reference = [ref_tokens]\n",
        "        hypothesis = pred_tokens\n",
        "        \n",
        "        try:\n",
        "            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth)\n",
        "            total_bleu += bleu\n",
        "            count += 1\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    return total_bleu / count if count > 0 else 0\n",
        "\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä ƒê√ÅNH GI√Å BLEU SCORE: BASELINE VS ATTENTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# T√≠nh BLEU cho Baseline\n",
        "print(\"\\nüèóÔ∏è T√≠nh BLEU cho BASELINE...\")\n",
        "baseline_bleu = calculate_bleu(translate_baseline, test_en, test_fr, num_samples=None)\n",
        "\n",
        "# T√≠nh BLEU cho Attention\n",
        "print(\"\\nüéØ T√≠nh BLEU cho ATTENTION...\")\n",
        "attention_bleu = calculate_bleu(translate_attention, test_en, test_fr, num_samples=None)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä K·∫æT QU·∫¢ BLEU SCORE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üèóÔ∏è BASELINE (Fixed Context):   {baseline_bleu * 100:.2f}%\")\n",
        "print(f\"üéØ ATTENTION (Dynamic Context): {attention_bleu * 100:.2f}%\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"üìà C·∫£i thi·ªán v·ªõi Attention:     {(attention_bleu - baseline_bleu) * 100:.2f}%\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# B·∫£ng ƒë√°nh gi√°\n",
        "print(\"\"\"\n",
        "üìä H∆Ø·ªöNG D·∫™N ƒê√ÅNH GI√Å BLEU SCORE:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ BLEU Score      ‚îÇ ƒê√°nh gi√°                         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ < 10%           ‚îÇ K√©m - G·∫ßn nh∆∞ v√¥ nghƒ©a           ‚îÇ\n",
        "‚îÇ 10% - 19%       ‚îÇ Y·∫øu - Kh√≥ hi·ªÉu                   ‚îÇ\n",
        "‚îÇ 20% - 29%       ‚îÇ Trung b√¨nh - Hi·ªÉu √Ω ch√≠nh        ‚îÇ\n",
        "‚îÇ 30% - 40%       ‚îÇ Kh√° - Ch·∫•t l∆∞·ª£ng ch·∫•p nh·∫≠n ƒë∆∞·ª£c  ‚îÇ\n",
        "‚îÇ 40% - 50%       ‚îÇ T·ªët - Ch·∫•t l∆∞·ª£ng cao             ‚îÇ\n",
        "‚îÇ > 50%           ‚îÇ R·∫•t t·ªët - G·∫ßn v·ªõi ng∆∞·ªùi d·ªãch     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "üìå K·∫øt lu·∫≠n: \n",
        "- Baseline Seq2Seq v·ªõi Fixed Context Vector ho·∫°t ƒë·ªông, nh∆∞ng h·∫°n ch·∫ø.\n",
        "- Attention Seq2Seq c·∫£i thi·ªán ƒë√°ng k·ªÉ nh·ªù Dynamic Context Vector.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5.5: D·ªäCH C√ÇU T√ôY √ù\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üñäÔ∏è D·ªäCH C√ÇU T√ôY √ù\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"A man is walking with his dog.\",\n",
        "    \"Two children are playing in the park.\",\n",
        "    \"A woman is reading a book.\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result = translate(sentence)\n",
        "    print(f\"\\nüì• EN: {sentence}\")\n",
        "    print(f\"üá´üá∑ FR: {result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\\n‚úÖ PH·∫¶N 5 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 6 (Analysis)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4.6: V·∫º BI·ªÇU ƒê·ªí TRAINING HISTORY (Optional)\n",
        "# ==============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "axes[0].plot(training_history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(training_history['valid_loss'], label='Valid Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training & Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Perplexity\n",
        "axes[1].plot(training_history['train_ppl'], label='Train PPL', marker='o')\n",
        "axes[1].plot(training_history['valid_ppl'], label='Valid PPL', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Perplexity')\n",
        "axes[1].set_title('Training & Validation Perplexity')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·∫°i 'training_history.png'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
