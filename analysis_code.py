"""
==============================================================================
PH·∫¶N 5: PH√ÇN T√çCH L·ªñI & C·∫¢I TI·∫æN (BEAM SEARCH)
==============================================================================
N·ªôi dung:
1. Script ph√¢n t√≠ch l·ªói (Error Analysis)
2. Beam Search Decoding
3. N·ªôi dung b√°o c√°o (M·ª•c 9 - Ph√¢n t√≠ch l·ªói v√† ƒê·ªÅ xu·∫•t)
==============================================================================
"""

import torch
import torch.nn.functional as F
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from tqdm import tqdm
import random

# ==============================================================================
# 1. SCRIPT PH√ÇN T√çCH L·ªñI (ERROR ANALYSIS)
# ==============================================================================

def analyze_errors(test_src, test_trg, num_examples=5, bleu_threshold=0.15):
    """
    T√¨m v√† hi·ªÉn th·ªã c√°c tr∆∞·ªùng h·ª£p d·ªãch sai (BLEU th·∫•p).
    
    Args:
        test_src: List c√¢u ngu·ªìn
        test_trg: List c√¢u ƒë√≠ch
        num_examples: S·ªë v√≠ d·ª• c·∫ßn hi·ªÉn th·ªã
        bleu_threshold: Ng∆∞·ª°ng BLEU ƒë·ªÉ coi l√† d·ªãch sai
    """
    print("=" * 70)
    print(" PH√ÇN T√çCH L·ªñI - T√åM C√ÇU D·ªäCH SAI")
    print("=" * 70)
    
    smooth = SmoothingFunction().method1
    bad_examples = []
    
    print("ƒêang qu√©t t·∫≠p test ƒë·ªÉ t√¨m c√¢u d·ªãch sai...")
    
    for idx in tqdm(range(len(test_src)), desc="Analyzing"):
        src = test_src[idx]
        trg = test_trg[idx]
        pred = translate(src)
        
        # T√≠nh BLEU
        ref_tokens = tokenizer_fr(trg.lower())
        pred_tokens = pred.split()
        
        try:
            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
        except:
            bleu = 0
        
        # L∆∞u c√¢u c√≥ BLEU th·∫•p
        if bleu < bleu_threshold:
            bad_examples.append({
                'idx': idx,
                'src': src,
                'trg': trg,
                'pred': pred,
                'bleu': bleu,
                'src_len': len(tokenizer_en(src)),
                'has_unk': '<unk>' in pred
            })
    
    # S·∫Øp x·∫øp theo BLEU tƒÉng d·∫ßn (sai nh·∫•t l√™n ƒë·∫ßu)
    bad_examples.sort(key=lambda x: x['bleu'])
    
    # Hi·ªÉn th·ªã top N v√≠ d·ª•
    print(f"\n{'='*70}")
    print(f" TOP {num_examples} C√ÇU D·ªäCH SAI NH·∫§T")
    print(f"{'='*70}")
    
    for i, ex in enumerate(bad_examples[:num_examples], 1):
        print(f"\n--- V√≠ d·ª• {i} (BLEU: {ex['bleu']*100:.2f}%) ---")
        print(f"üì• Src:  {ex['src']}")
        print(f"üìå Trg:  {ex['trg']}")
        print(f"ü§ñ Pred: {ex['pred']}")
        
        # Ph√¢n t√≠ch nguy√™n nh√¢n
        reasons = []
        if ex['src_len'] > 20:
            reasons.append("C√¢u d√†i ‚Üí Context Vector b·ªã qu√° t·∫£i (bottleneck)")
        if ex['has_unk']:
            reasons.append("Xu·∫•t hi·ªán <unk> ‚Üí T·ª´ hi·∫øm kh√¥ng c√≥ trong vocab (OOV)")
        if ex['bleu'] < 0.05:
            reasons.append("D·ªãch sai ho√†n to√†n ‚Üí Model kh√¥ng n·∫Øm ƒë∆∞·ª£c ng·ªØ nghƒ©a")
        
        if reasons:
            print(f"‚ö†Ô∏è Nguy√™n nh√¢n c√≥ th·ªÉ:")
            for r in reasons:
                print(f"   - {r}")
    
    return bad_examples


# ==============================================================================
# 2. BEAM SEARCH DECODING
# ==============================================================================

def translate_beam_search(sentence: str, beam_size: int = 3, max_len: int = 50) -> str:
    """
    D·ªãch c√¢u s·ª≠ d·ª•ng Beam Search thay v√¨ Greedy Decoding.
    
    Args:
        sentence: C√¢u ti·∫øng Anh c·∫ßn d·ªãch
        beam_size: S·ªë beam (·ª©ng vi√™n) gi·ªØ l·∫°i m·ªói b∆∞·ªõc
        max_len: ƒê·ªô d√†i t·ªëi ƒëa c√¢u d·ªãch
        
    Returns:
        C√¢u ti·∫øng Ph√°p ƒë√£ d·ªãch (string)
        
    Logic:
    - Thay v√¨ ch·ªçn 1 t·ª´ t·ªët nh·∫•t (Greedy), gi·ªØ l·∫°i k ·ª©ng vi√™n t·ªët nh·∫•t
    - M·ªói ·ª©ng vi√™n c√≥ log_prob t√≠ch l≈©y
    - Cu·ªëi c√πng ch·ªçn chu·ªói c√≥ log_prob cao nh·∫•t
    """
    model.eval()
    
    # ===== 1. TOKENIZE & TENSORIZE =====
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    
    # ‚ö†Ô∏è src_len PH·∫¢I n·∫±m tr√™n CPU
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)
    
    # ===== 2. ENCODER FORWARD =====
    with torch.no_grad():
        hidden, cell = model.encoder(src_tensor, src_len)
    
    # ===== 3. KH·ªûI T·∫†O BEAM =====
    # M·ªói beam l√† tuple: (sequence, log_prob, hidden, cell, finished)
    # sequence: list c√°c token index ƒë√£ sinh
    # log_prob: t·ªïng log probability
    # finished: True n·∫øu ƒë√£ g·∫∑p <eos>
    
    initial_beam = {
        'seq': [SOS_IDX],
        'log_prob': 0.0,
        'hidden': hidden,
        'cell': cell,
        'finished': False
    }
    beams = [initial_beam]
    completed_beams = []
    
    # ===== 4. BEAM SEARCH LOOP =====
    for step in range(max_len):
        all_candidates = []
        
        for beam in beams:
            if beam['finished']:
                completed_beams.append(beam)
                continue
            
            # L·∫•y token cu·ªëi l√†m input
            last_token = beam['seq'][-1]
            input_tensor = torch.LongTensor([last_token]).to(device)
            
            with torch.no_grad():
                output, new_hidden, new_cell = model.decoder(
                    input_tensor, beam['hidden'], beam['cell']
                )
            
            # L·∫•y log probabilities
            log_probs = F.log_softmax(output, dim=1)  # [1, vocab_size]
            
            # L·∫•y top-k tokens
            topk_log_probs, topk_indices = log_probs.topk(beam_size)
            
            for i in range(beam_size):
                token_idx = topk_indices[0, i].item()
                token_log_prob = topk_log_probs[0, i].item()
                
                new_seq = beam['seq'] + [token_idx]
                new_log_prob = beam['log_prob'] + token_log_prob
                
                candidate = {
                    'seq': new_seq,
                    'log_prob': new_log_prob,
                    'hidden': new_hidden,
                    'cell': new_cell,
                    'finished': (token_idx == EOS_IDX)
                }
                all_candidates.append(candidate)
        
        # S·∫Øp x·∫øp theo log_prob gi·∫£m d·∫ßn v√† gi·ªØ top-k
        all_candidates.sort(key=lambda x: x['log_prob'], reverse=True)
        beams = all_candidates[:beam_size]
        
        # N·∫øu t·∫•t c·∫£ beams ƒë√£ finished, d·ª´ng
        if all(b['finished'] for b in beams):
            completed_beams.extend(beams)
            break
    
    # ===== 5. CH·ªåN BEAM T·ªêT NH·∫§T =====
    # Th√™m c√°c beam ch∆∞a ho√†n th√†nh v√†o completed
    completed_beams.extend([b for b in beams if not b['finished']])
    
    # Normalize log_prob theo ƒë·ªô d√†i (tr√°nh ∆∞u ti√™n c√¢u ng·∫Øn)
    for beam in completed_beams:
        beam['normalized_log_prob'] = beam['log_prob'] / len(beam['seq'])
    
    # Ch·ªçn beam c√≥ log_prob cao nh·∫•t
    best_beam = max(completed_beams, key=lambda x: x['normalized_log_prob'])
    
    # ===== 6. CONVERT TO WORDS =====
    trg_tokens = [vocab_fr.get_itos()[i] for i in best_beam['seq']]
    
    # B·ªè <sos> v√† <eos>
    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]
    
    return ' '.join(trg_tokens)


# ==============================================================================
# 3. SO S√ÅNH GREEDY VS BEAM SEARCH
# ==============================================================================

def compare_decoding_methods(test_sentences):
    """So s√°nh k·∫øt qu·∫£ Greedy v√† Beam Search."""
    
    print("\n" + "=" * 70)
    print(" SO S√ÅNH: GREEDY vs BEAM SEARCH")
    print("=" * 70)
    
    for i, sentence in enumerate(test_sentences, 1):
        greedy_result = translate(sentence)
        beam_result = translate_beam_search(sentence, beam_size=3)
        
        print(f"\n--- C√¢u {i} ---")
        print(f"üì• Input:        {sentence}")
        print(f"üîµ Greedy:       {greedy_result}")
        print(f"üü¢ Beam (k=3):   {beam_result}")
        
        # ƒê√°nh d·∫•u n·∫øu kh√°c nhau
        if greedy_result != beam_result:
            print("   ‚ö° K·∫øt qu·∫£ KH√ÅC NHAU!")
    
    print("\n" + "=" * 70)


# ==============================================================================
# 4. MAIN EXECUTION
# ==============================================================================

print("=" * 70)
print(" PH·∫¶N 5: PH√ÇN T√çCH L·ªñI & C·∫¢I TI·∫æN")
print("=" * 70)

# Load model
model.load_state_dict(torch.load('best_model.pth', map_location=device))
model.eval()
print("‚úÖ ƒê√£ load model\n")

# ----- PH√ÇN T√çCH L·ªñI -----
bad_examples = analyze_errors(test_en, test_fr, num_examples=5)

# ----- SO S√ÅNH GREEDY VS BEAM SEARCH -----
sample_sentences = [
    test_en[0],
    test_en[10],
    "A man is walking with his dog in the park."
]
compare_decoding_methods(sample_sentences)


# ==============================================================================
# 5. N·ªòI DUNG B√ÅO C√ÅO - M·ª§C 9: PH√ÇN T√çCH L·ªñI V√Ä ƒê·ªÄ XU·∫§T
# ==============================================================================

REPORT_CONTENT = """
================================================================================
                    M·ª§C 9: PH√ÇN T√çCH L·ªñI V√Ä ƒê·ªÄ XU·∫§T C·∫¢I TI·∫æN
================================================================================

9.1. PH√ÇN T√çCH NGUY√äN NH√ÇN L·ªñI
-----------------------------

Sau khi ki·ªÉm tra k·∫øt qu·∫£ d·ªãch tr√™n t·∫≠p Test, ch√∫ng t√¥i nh·∫≠n th·∫•y m√¥ h√¨nh 
Encoder-Decoder LSTM g·∫∑p ph·∫£i m·ªôt s·ªë v·∫•n ƒë·ªÅ ch√≠nh:

1. V·∫§N ƒê·ªÄ N√öT TH·∫ÆT C·ªî CHAI (BOTTLENECK):
   Ki·∫øn tr√∫c Seq2Seq truy·ªÅn th·ªëng n√©n to√†n b·ªô th√¥ng tin c·ªßa c√¢u ngu·ªìn v√†o 
   m·ªôt vector ng·ªØ c·∫£nh (context vector) c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh. V·ªõi c√°c c√¢u 
   d√†i (>20 t·ª´), Encoder g·∫∑p kh√≥ khƒÉn trong vi·ªác l∆∞u gi·ªØ t·∫•t c·∫£ th√¥ng tin, 
   d·∫´n ƒë·∫øn hi·ªán t∆∞·ª£ng "qu√™n" c√°c t·ª´ ·ªü ƒë·∫ßu c√¢u. ƒêi·ªÅu n√†y ƒë·∫∑c bi·ªát nghi√™m tr·ªçng 
   khi d·ªãch c√°c c√¢u ph·ª©c t·∫°p c√≥ nhi·ªÅu m·ªánh ƒë·ªÅ.

2. V·∫§N ƒê·ªÄ T·ª™ HI·∫æM (OOV - Out-of-Vocabulary):
   Khi g·∫∑p c√°c t·ª´ kh√¥ng c√≥ trong t·ª´ ƒëi·ªÉn (do min_freq=2 khi x√¢y d·ª±ng vocab), 
   m√¥ h√¨nh thay th·∫ø b·∫±ng token <unk>. ƒêi·ªÅu n√†y l√†m m·∫•t ƒëi √Ω nghƒ©a quan tr·ªçng, 
   ƒë·∫∑c bi·ªát v·ªõi t√™n ri√™ng, thu·∫≠t ng·ªØ chuy√™n ng√†nh, ho·∫∑c t·ª´ vi·∫øt sai ch√≠nh t·∫£.

3. V·∫§N ƒê·ªÄ GREEDY DECODING:
   Thu·∫≠t to√°n Greedy ch·ªçn t·ª´ c√≥ x√°c su·∫•t cao nh·∫•t ·ªü m·ªói b∆∞·ªõc, c√≥ th·ªÉ d·∫´n ƒë·∫øn 
   c√°c chu·ªói kh√¥ng t·ªëi ∆∞u to√†n c·ª•c. ƒê√¥i khi m·ªôt l·ª±a ch·ªçn "t·ªët h∆°n m·ªôt ch√∫t" 
   ·ªü b∆∞·ªõc hi·ªán t·∫°i l·∫°i m·ªü ra nhi·ªÅu l·ª±a ch·ªçn t·ªët h∆°n ·ªü c√°c b∆∞·ªõc sau.

9.2. ƒê·ªÄ XU·∫§T C·∫¢I TI·∫æN
--------------------

1. C∆† CH·∫æ ATTENTION:
   Thay v√¨ d·ª±a v√†o m·ªôt context vector c·ªë ƒë·ªãnh, c∆° ch·∫ø Attention cho ph√©p 
   Decoder "nh√¨n l·∫°i" v√† t·∫≠p trung v√†o c√°c ph·∫ßn kh√°c nhau c·ªßa c√¢u ngu·ªìn ·ªü 
   m·ªói b∆∞·ªõc gi·∫£i m√£. ƒêi·ªÅu n√†y gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ bottleneck v√† c·∫£i thi·ªán 
   ƒë√°ng k·ªÉ ch·∫•t l∆∞·ª£ng d·ªãch v·ªõi c√¢u d√†i.

2. SUBWORD MODELING (BPE/WordPiece):
   Thay v√¨ tokenize theo t·ª´, s·ª≠ d·ª•ng Byte-Pair Encoding (BPE) ƒë·ªÉ chia t·ª´ 
   th√†nh c√°c ƒë∆°n v·ªã nh·ªè h∆°n (subword). ƒêi·ªÅu n√†y gi√∫p x·ª≠ l√Ω t·ª´ hi·∫øm b·∫±ng 
   c√°ch bi·ªÉu di·ªÖn ch√∫ng d∆∞·ªõi d·∫°ng t·ªï h·ª£p c√°c subword ƒë√£ bi·∫øt.

3. BEAM SEARCH DECODING:
   Thay th·∫ø Greedy b·∫±ng Beam Search, gi·ªØ l·∫°i k ·ª©ng vi√™n t·ªët nh·∫•t ·ªü m·ªói b∆∞·ªõc 
   v√† ch·ªçn chu·ªói c√≥ x√°c su·∫•t t·ªïng cao nh·∫•t. Th·ª≠ nghi·ªám v·ªõi beam_size=3 cho 
   th·∫•y m·ªôt s·ªë c·∫£i thi·ªán ƒë√°ng k·ªÉ v·ªõi c√°c c√¢u ph·ª©c t·∫°p.

4. KI·∫æN TR√öC TRANSFORMER:
   V·ªÅ l√¢u d√†i, ki·∫øn tr√∫c Transformer v·ªõi Self-Attention v√† Multi-Head 
   Attention ƒë√£ ch·ª©ng minh hi·ªáu qu·∫£ v∆∞·ª£t tr·ªôi so v·ªõi RNN-based models 
   trong c√°c t√°c v·ª• d·ªãch m√°y.

================================================================================
"""

print(REPORT_CONTENT)

# L∆∞u n·ªôi dung b√°o c√°o ra file
with open('report_section9.txt', 'w', encoding='utf-8') as f:
    f.write(REPORT_CONTENT)
print("‚úÖ ƒê√£ l∆∞u n·ªôi dung b√°o c√°o v√†o 'report_section9.txt'")
