# -*- coding: utf-8 -*-
"""NLP_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PT8tP32qgfT-H4XoegaID1nlfVquCTjo

# üì¶ PH·∫¶N 1: T·∫¢I V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU

---

**M·ª•c ti√™u:**
1. T·∫£i dataset Multi30K (English ‚Üí French)
2. Tokenization v·ªõi SpaCy
3. X√¢y d·ª±ng Vocabulary v·ªõi special tokens: `<unk>`, `<pad>`, `<sos>`, `<eos>`
4. T·∫°o DataLoader v·ªõi sorting + padding (s·∫µn s√†ng cho LSTM)

---

## 1.1 Environment Setup and Dataset Preparation

Trong cell n√†y, ch√∫ng t√¥i th·ª±c hi·ªán **c√†i ƒë·∫∑t m√¥i tr∆∞·ªùng l√†m vi·ªác v√† chu·∫©n b·ªã d·ªØ li·ªáu** cho ƒë·ªì √°n Neural Machine Translation (Anh ‚Üí Ph√°p).

### Library Installation
- S·ª≠ d·ª•ng **Google Colab** l√†m m√¥i tr∆∞·ªùng th·ª±c thi.
- C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:
  - **PyTorch** v√† **TorchText** cho vi·ªác x√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq.
  - **SpaCy** cho tokenization ti·∫øng Anh v√† ti·∫øng Ph√°p.
  - **NLTK** ph·ª•c v·ª• cho b∆∞·ªõc ƒë√°nh gi√° BLEU score.
- Phi√™n b·∫£n th∆∞ vi·ªán ƒë∆∞·ª£c c·ªë ƒë·ªãnh nh·∫±m ƒë·∫£m b·∫£o **t√≠nh ·ªïn ƒë·ªãnh v√† kh·∫£ nƒÉng t√°i l·∫≠p k·∫øt qu·∫£**.

### Environment Configuration
- Thi·∫øt l·∫≠p **random seed** ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh reproducibility c·ªßa qu√° tr√¨nh hu·∫•n luy·ªán.
- T·ª± ƒë·ªông ph√°t hi·ªán v√† s·ª≠ d·ª•ng **GPU (CUDA)** n·∫øu kh·∫£ d·ª•ng, gi√∫p tƒÉng t·ªëc qu√° tr√¨nh training.
- In ra th√¥ng tin phi√™n b·∫£n th∆∞ vi·ªán v√† thi·∫øt b·ªã ƒë·ªÉ x√°c nh·∫≠n m√¥i tr∆∞·ªùng ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng.

### Dataset Preparation (Multi30K)
- T·∫£i b·ªô d·ªØ li·ªáu **Multi30K (English‚ÄìFrench)** tr·ª±c ti·∫øp t·ª´ ngu·ªìn ch√≠nh th·ª©c.
- D·ªØ li·ªáu ƒë∆∞·ª£c chia s·∫µn th√†nh ba t·∫≠p:
  - **Training**
  - **Validation**
  - **Test**
- C√°c file n√©n ƒë∆∞·ª£c gi·∫£i n√©n v√† l∆∞u tr·ªØ trong th∆∞ m·ª•c `data/` ƒë·ªÉ thu·∫≠n ti·ªán cho c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo.

B∆∞·ªõc chu·∫©n b·ªã n√†y ƒë·∫£m b·∫£o r·∫±ng **m√¥i tr∆∞·ªùng th·ª±c thi v√† d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë√£ s·∫µn s√†ng**, t·∫°o n·ªÅn t·∫£ng cho c√°c b∆∞·ªõc **ƒë·ªçc d·ªØ li·ªáu, tokenization v√† x√¢y d·ª±ng m√¥ h√¨nh Seq2Seq** ·ªü c√°c ph·∫ßn ti·∫øp theo.
"""

# ==============================================================================
# CELL 1.1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN & C·∫§U H√åNH
# ==============================================================================

# Kh·∫Øc ph·ª•c l·ªói NumPy v√† c√†i ƒë·∫∑t l·∫°i c√°c th∆∞ vi·ªán ch√≠nh
# H·∫° c·∫•p NumPy ƒë·ªÉ gi·∫£i quy·∫øt l·ªói t∆∞∆°ng th√≠ch v·ªõi Thinc/SpaCy
!pip install "numpy<2.0.0" -q

# C√†i ƒë·∫∑t th∆∞ vi·ªán (ch·∫°y tr√™n Google Colab)
!pip install torch==2.2.2 torchtext==0.17.2 -q
!pip install spacy nltk -q
!python -m spacy download en_core_web_sm -q
!python -m spacy download fr_core_news_sm -q

# Import th∆∞ vi·ªán
import torch
import torch.nn as nn
import torch.nn.functional as F  # C·∫ßn cho Attention
import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import io
import os
import random

# =============================================================================
# SEED cho Reproducibility
# =============================================================================
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"‚úÖ Torch version: {torch.__version__}")
print(f"‚úÖ Torchtext version: {torchtext.__version__}")
print(f"‚úÖ Device: {device}")
print(f"‚úÖ Seed: {SEED}")

# =============================================================================
# T·∫¢I DATASET MULTI30K (EN-FR)
# =============================================================================
!mkdir -p data
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz -O data/train.en.gz
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz -O data/train.fr.gz
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz -O data/val.en.gz
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz -O data/val.fr.gz
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz -O data/test.en.gz
!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz -O data/test.fr.gz

# Gi·∫£i n√©n
!gunzip -kf data/*.gz
!ls -la data/

print("\n‚úÖ ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu v√† th∆∞ vi·ªán!")

from google.colab import drive
drive.mount('/content/drive')

import os

BASE_DIR = "/content/drive/MyDrive/NMT_EN_FR_Final1"
os.makedirs(BASE_DIR, exist_ok=True)

print("Saving to:", BASE_DIR)

"""## 1.2 Data Loading and Inspection

Trong b∆∞·ªõc n√†y, ch√∫ng t√¥i ti·∫øn h√†nh **ƒë·ªçc v√† ki·ªÉm tra d·ªØ li·ªáu song ng·ªØ Anh‚ÄìPh√°p** t·ª´ b·ªô d·ªØ li·ªáu Multi30K, bao g·ªìm ba t·∫≠p ri√™ng bi·ªát: **training, validation v√† test**.

### Data Loading
- D·ªØ li·ªáu ƒë∆∞·ª£c ƒë·ªçc tr·ª±c ti·∫øp t·ª´ c√°c file vƒÉn b·∫£n:
  - `train.en` / `train.fr`
  - `val.en` / `val.fr`
  - `test.en` / `test.fr`
- M·ªói d√≤ng trong file t∆∞∆°ng ·ª©ng v·ªõi **m·ªôt c√¢u**, v√† c√°c c·∫∑p c√¢u Anh‚ÄìPh√°p ƒë∆∞·ª£c **cƒÉn ch·ªânh 1‚Äì1**.
- C√°c d√≤ng tr·ªëng ƒë∆∞·ª£c lo·∫°i b·ªè nh·∫±m ƒë·∫£m b·∫£o d·ªØ li·ªáu s·∫°ch.

### Data Consistency Check
- Th·ª±c hi·ªán ki·ªÉm tra s·ªë l∆∞·ª£ng c√¢u gi·ªØa hai ng√¥n ng·ªØ b·∫±ng c√°ch so s√°nh ƒë·ªô d√†i danh s√°ch c√¢u.
- ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o **kh√¥ng x·∫£y ra l·ªách c·∫∑p c√¢u**, m·ªôt y√™u c·∫ßu quan tr·ªçng trong b√†i to√°n d·ªãch m√°y.

### Dataset Statistics and Examples
- Sau khi ƒë·ªçc d·ªØ li·ªáu, h·ªá th·ªëng in ra:
  - S·ªë l∆∞·ª£ng c·∫∑p c√¢u trong t·ª´ng t·∫≠p (train/validation/test)
- Hi·ªÉn th·ªã m·ªôt s·ªë **v√≠ d·ª• c·∫∑p c√¢u ƒë·∫ßu ti√™n** t·ª´ t·∫≠p train nh·∫±m:
  - X√°c nh·∫≠n d·ªØ li·ªáu ƒë∆∞·ª£c ƒë·ªçc ƒë√∫ng
  - Ki·ªÉm tra nhanh ch·∫•t l∆∞·ª£ng v√† ƒë·ªãnh d·∫°ng d·ªØ li·ªáu

B∆∞·ªõc ki·ªÉm tra n√†y gi√∫p ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o **ƒë√∫ng, ƒë·∫ßy ƒë·ªß v√† nh·∫•t qu√°n** tr∆∞·ªõc khi chuy·ªÉn sang c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo nh∆∞ **tokenization v√† x√¢y d·ª±ng vocabulary**.

"""

# ==============================================================================
# CELL 1.2: ƒê·ªåC V√Ä KI·ªÇM TRA D·ªÆ LI·ªÜU
# ==============================================================================

def read_data(en_file, fr_file):
    """
    ƒê·ªçc d·ªØ li·ªáu song ng·ªØ t·ª´ file.

    Returns:
        en_sentences: List c√¢u ti·∫øng Anh
        fr_sentences: List c√¢u ti·∫øng Ph√°p (cƒÉn ch·ªânh 1-1)
    """
    with open(en_file, 'r', encoding='utf-8') as f:
        en_sentences = [line.strip() for line in f.readlines() if line.strip()]
    with open(fr_file, 'r', encoding='utf-8') as f:
        fr_sentences = [line.strip() for line in f.readlines() if line.strip()]

    # ƒê·∫£m b·∫£o s·ªë c√¢u kh·ªõp nhau
    assert len(en_sentences) == len(fr_sentences), "S·ªë c√¢u EN v√† FR kh√¥ng kh·ªõp!"
    return en_sentences, fr_sentences

# ƒê·ªçc d·ªØ li·ªáu train, val, test t·ª´ folder 'data/'
train_en, train_fr = read_data('data/train.en', 'data/train.fr')
val_en, val_fr = read_data('data/val.en', 'data/val.fr')
test_en, test_fr = read_data('data/test.en', 'data/test.fr')

print("=" * 50)
print("üìä TH·ªêNG K√ä D·ªÆ LI·ªÜU MULTI30K")
print("=" * 50)
print(f"   Train:      {len(train_en):,} c·∫∑p c√¢u")
print(f"   Validation: {len(val_en):,} c·∫∑p c√¢u")
print(f"   Test:       {len(test_en):,} c·∫∑p c√¢u")
print("=" * 50)

# Hi·ªÉn th·ªã v√≠ d·ª•
print("\nüìù V√ç D·ª§ 5 C·∫∂P C√ÇU ƒê·∫¶U TI√äN:")
for i in range(5):
    print(f"\nExample {i+1}:")
    print(f"   EN: {train_en[i]}")
    print(f"   FR: {train_fr[i]}")

"""## 1.3 Tokenization & Vocabulary Construction

Trong b∆∞·ªõc n√†y, ch√∫ng t√¥i th·ª±c hi·ªán **tokenization** v√† **x√¢y d·ª±ng vocabulary** cho hai ng√¥n ng·ªØ ngu·ªìn (English) v√† ƒë√≠ch (French), ƒë√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫±m chuy·ªÉn d·ªØ li·ªáu vƒÉn b·∫£n th√¥ th√†nh d·∫°ng s·ªë ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω.

### Tokenization
- S·ª≠ d·ª•ng **SpaCy tokenizer** cho c·∫£ ti·∫øng Anh (`en_core_web_sm`) v√† ti·∫øng Ph√°p (`fr_core_news_sm`).
- SpaCy ƒë∆∞·ª£c l·ª±a ch·ªçn v√¨ kh·∫£ nƒÉng x·ª≠ l√Ω t·ª´ v·ª±ng v√† d·∫•u c√¢u ·ªïn ƒë·ªãnh, ph√π h·ª£p cho b√†i to√°n d·ªãch m√°y.
- Vi·ªác t√°ch tokenizer th√†nh h√†m ri√™ng gi√∫p d·ªÖ ki·ªÉm so√°t l·ªói v√† t√°i s·ª≠ d·ª•ng trong c√°c b∆∞·ªõc sau.

### Vocabulary Construction
- Vocabulary ƒë∆∞·ª£c x√¢y d·ª±ng **ch·ªâ t·ª´ t·∫≠p train**, ƒë·∫£m b·∫£o kh√¥ng r√≤ r·ªâ th√¥ng tin t·ª´ validation/test.
- √Åp d·ª•ng c√°c r√†ng bu·ªôc:
  - `min_freq = 2`: lo·∫°i b·ªè c√°c t·ª´ xu·∫•t hi·ªán qu√° √≠t nh·∫±m gi·∫£m nhi·ªÖu.
  - `max_tokens = 10,000`: gi·ªõi h·∫°n k√≠ch th∆∞·ªõc vocabulary ƒë·ªÉ ki·ªÉm so√°t ƒë·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh.
- C√°c **special tokens** ƒë∆∞·ª£c th√™m th·ªß c√¥ng:
  - `<unk>`: t·ª´ ngo√†i vocabulary (OOV)
  - `<pad>`: padding cho c√°c chu·ªói c√≥ ƒë·ªô d√†i kh√°c nhau
  - `<sos>`: k√Ω hi·ªáu b·∫Øt ƒë·∫ßu c√¢u
  - `<eos>`: k√Ω hi·ªáu k·∫øt th√∫c c√¢u
- Ch·ªâ s·ªë m·∫∑c ƒë·ªãnh (`default_index`) ƒë∆∞·ª£c g√°n cho `<unk>` ƒë·ªÉ ƒë·∫£m b·∫£o c√°c t·ª´ ch∆∞a t·ª´ng xu·∫•t hi·ªán trong training ƒë∆∞·ª£c x·ª≠ l√Ω an to√†n.

### Ki·ªÉm tra v√† x√°c nh·∫≠n
- Sau khi x√¢y d·ª±ng vocabulary, h·ªá th·ªëng in ra:
  - K√≠ch th∆∞·ªõc vocabulary c·ªßa m·ªói ng√¥n ng·ªØ
  - Th·ª© t·ª± index c·ªßa c√°c special tokens
- Th·ª±c hi·ªán ki·ªÉm tra OOV b·∫±ng c√°ch truy v·∫•n m·ªôt t·ª´ kh√¥ng t·ªìn t·∫°i ƒë·ªÉ x√°c nh·∫≠n r·∫±ng t·ª´ n√†y ƒë∆∞·ª£c √°nh x·∫° ƒë√∫ng v·ªÅ `<unk>`.

B∆∞·ªõc n√†y ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c chu·∫©n h√≥a ƒë·∫ßy ƒë·ªß, t·∫°o ti·ªÅn ƒë·ªÅ cho c√°c b∆∞·ªõc **numericalization, padding v√† hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq** ·ªü c√°c ph·∫ßn ti·∫øp theo.

"""

# ==============================================================================
# CELL 1.3: TOKENIZATION & VOCABULARY
# ==============================================================================

# --- C·∫§U H√åNH TOKEN ƒê·∫∂C BI·ªÜT ---
UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3
SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<sos>', '<eos>']

# =============================================================================
# H√ÄM: get_tokenizers()
# Nhi·ªám v·ª•: T·∫£i tokenizer c·ªßa SpaCy cho ti·∫øng Anh v√† Ph√°p
# =============================================================================
def get_tokenizers():
    try:
        en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')
        fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')
        print("‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng Tokenizer (SpaCy)")
        return en_tokenizer, fr_tokenizer
    except OSError:
        print("‚ùå L·ªói: Ch∆∞a t√¨m th·∫•y model SpaCy. H√£y ch·∫°y l·∫°i Cell 1.1")
        return None, None

# =============================================================================
# H√ÄM: build_vocab()
# Nhi·ªám v·ª•: X√¢y d·ª±ng vocabulary t·ª´ file d·ªØ li·ªáu
# =============================================================================
def build_vocab(filepath, tokenizer):
    """
    X√¢y d·ª±ng vocabulary t·ª´ file text.

    Args:
        filepath: ƒê∆∞·ªùng d·∫´n file text
        tokenizer: Tokenizer function

    Returns:
        vocab: Vocabulary object v·ªõi special tokens
    """
    def yield_tokens(path):
        with io.open(path, encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    yield tokenizer(line.strip())

    print(f"   ƒêang x√¢y d·ª±ng vocab t·ª´ {filepath}...")

    vocab = build_vocab_from_iterator(
        yield_tokens(filepath),
        min_freq=2,           # B·ªè qua t·ª´ xu·∫•t hi·ªán < 2 l·∫ßn
        max_tokens=10000,     # Gi·ªõi h·∫°n vocabulary size
        specials=SPECIAL_SYMBOLS
    )

    # Set default index cho t·ª´ kh√¥ng c√≥ trong vocab (OOV)
    vocab.set_default_index(UNK_IDX)
    return vocab

# =============================================================================
# TH·ª∞C THI
# =============================================================================
tokenizer_en, tokenizer_fr = get_tokenizers()

if tokenizer_en and tokenizer_fr:
    print("\n X√¢y d·ª±ng Vocabulary:")
    vocab_en = build_vocab('data/train.en', tokenizer_en)
    vocab_fr = build_vocab('data/train.fr', tokenizer_fr)

    print("\n" + "=" * 50)
    print("B√ÅO C√ÅO VOCABULARY")
    print("=" * 50)
    print(f"   Vocab EN: {len(vocab_en):,} tokens (max: 10,004)")
    print(f"   Vocab FR: {len(vocab_fr):,} tokens (max: 10,004)")
    print(f"   Special tokens: <unk>={UNK_IDX}, <pad>={PAD_IDX}, <sos>={SOS_IDX}, <eos>={EOS_IDX}")

    # Ki·ªÉm tra x·ª≠ l√Ω t·ª´ l·∫° (OOV)
    test_oov = vocab_en['t·ª´_kh√¥ng_t·ªìn_t·∫°i_xyz']
    if test_oov == UNK_IDX:
        print("   OOV handling: ‚úÖ Th√†nh c√¥ng (tr·∫£ v·ªÅ index 0)")
    else:
        print(f"   OOV handling: ‚ùå Th·∫•t b·∫°i (tr·∫£ v·ªÅ {test_oov})")
    print("=" * 50)

"""## 1.4 Text Transformation (Numericalization)

Trong b∆∞·ªõc n√†y, ch√∫ng t√¥i th·ª±c hi·ªán **chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n sang d·∫°ng s·ªë (numericalization)**, m·ªôt b∆∞·ªõc trung gian quan tr·ªçng gi√∫p m√¥ h√¨nh Seq2Seq c√≥ th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu ng√¥n ng·ªØ t·ª± nhi√™n.

### Text Transformation Pipeline
Quy tr√¨nh chuy·ªÉn ƒë·ªïi ƒë∆∞·ª£c th·ª±c hi·ªán theo c√°c b∆∞·ªõc sau:

1. **Tokenization**:  
   C√¢u ƒë·∫ßu v√†o ƒë∆∞·ª£c t√°ch th√†nh c√°c token b·∫±ng tokenizer ƒë√£ x√¢y d·ª±ng ·ªü b∆∞·ªõc tr∆∞·ªõc.
2. **Vocabulary Mapping**:  
   M·ªói token ƒë∆∞·ª£c √°nh x·∫° sang ch·ªâ s·ªë t∆∞∆°ng ·ª©ng trong vocabulary.  
   C√°c token kh√¥ng t·ªìn t·∫°i trong vocabulary s·∫Ω ƒë∆∞·ª£c √°nh x·∫° v·ªÅ `<unk>`.
3. **Special Tokens Insertion**:  
   - `<sos>` (start-of-sentence) ƒë∆∞·ª£c th√™m v√†o ƒë·∫ßu c√¢u.
   - `<eos>` (end-of-sentence) ƒë∆∞·ª£c th√™m v√†o cu·ªëi c√¢u.  

K·∫øt qu·∫£ cu·ªëi c√πng l√† m·ªôt **tensor s·ªë nguy√™n 1 chi·ªÅu**, ƒë·∫°i di·ªán cho m·ªôt c√¢u ho√†n ch·ªânh s·∫µn s√†ng ƒë∆∞a v√†o m√¥ h√¨nh.

### Validation of the Transformation
ƒê·ªÉ ƒë·∫£m b·∫£o pipeline ho·∫°t ƒë·ªông ch√≠nh x√°c, ch√∫ng t√¥i:
- Ki·ªÉm tra tr·ª±c ti·∫øp t·ª´ng b∆∞·ªõc: c√¢u g·ªëc ‚Üí tokens ‚Üí tensor.
- X√°c nh·∫≠n:
  - Ki·ªÉu d·ªØ li·ªáu c·ªßa tensor (`torch.long`)
  - Th·ª© t·ª± v√† v·ªã tr√≠ c·ªßa `<sos>` v√† `<eos>`

Vi·ªác ki·ªÉm tra n√†y gi√∫p ƒë·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c chu·∫©n h√≥a ƒë√∫ng ƒë·ªãnh d·∫°ng, t·∫°o ti·ªÅn ƒë·ªÅ cho c√°c b∆∞·ªõc **padding, batching v√† hu·∫•n luy·ªán m√¥ h√¨nh** trong c√°c ph·∫ßn ti·∫øp theo.

"""

# ==============================================================================
# CELL 1.4: TEXT TRANSFORM (Numericalization)
# ==============================================================================

def text_transform(text, tokenizer, vocab):
    """
    Chuy·ªÉn ƒë·ªïi c√¢u text th√†nh tensor s·ªë.

    Pipeline: text ‚Üí tokens ‚Üí token_ids ‚Üí tensor v·ªõi <sos> v√† <eos>

    Args:
        text: C√¢u input (string)
        tokenizer: Tokenizer function
        vocab: Vocabulary object

    Returns:
        tensor: [SOS, token_ids..., EOS]
    """
    tokens = tokenizer(text)
    token_ids = [vocab[token] for token in tokens]
    return torch.tensor([SOS_IDX] + token_ids + [EOS_IDX], dtype=torch.long)

# =============================================================================
# KI·ªÇM TRA TEXT PIPELINE
# =============================================================================
print("=" * 50)
print("üîç KI·ªÇM TRA TEXT PIPELINE")
print("=" * 50)

sample_sentence = "Two young, White males are outside."
print(f"1. C√¢u g·ªëc:     '{sample_sentence}'")

# Tokenize
sample_tokens = tokenizer_en(sample_sentence)
print(f"2. Tokens:      {sample_tokens}")

# Transform
sample_tensor = text_transform(sample_sentence, tokenizer_en, vocab_en)
print(f"3. Tensor:      {sample_tensor}")
print(f"4. Shape:       {sample_tensor.shape}")
print(f"5. Dtype:       {sample_tensor.dtype}")

# Ki·ªÉm tra logic <sos> v√† <eos>
if sample_tensor[0] == SOS_IDX and sample_tensor[-1] == EOS_IDX:
    print("\n‚úÖ Logic <sos>/<eos>: ƒê√öNG (ƒë·∫ßu=2, cu·ªëi=3)")
else:
    print(f"\n‚ùå Logic <sos>/<eos>: SAI (ƒë·∫ßu={sample_tensor[0]}, cu·ªëi={sample_tensor[-1]})")
print("=" * 50)

"""## 1.5 Dataset, Collate Function and DataLoader

Trong b∆∞·ªõc n√†y, ch√∫ng t√¥i x√¢y d·ª±ng **pipeline batching ho√†n ch·ªânh** cho b√†i to√°n d·ªãch m√°y, bao g·ªìm **Dataset**, **collate function** v√† **DataLoader**. M·ª•c ti√™u l√† ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c chu·∫©n h√≥a ƒë√∫ng ƒë·ªãnh d·∫°ng v√† t∆∞∆°ng th√≠ch v·ªõi m√¥ h√¨nh **Seq2Seq s·ª≠ d·ª•ng LSTM**.

### Custom Dataset
- ƒê·ªãnh nghƒ©a l·ªõp `TranslationDataset` ƒë·ªÉ l∆∞u tr·ªØ c√°c c·∫∑p c√¢u song ng·ªØ *(source, target)* d∆∞·ªõi d·∫°ng vƒÉn b·∫£n.
- Dataset ch·ªâ ch·ªãu tr√°ch nhi·ªám qu·∫£n l√Ω d·ªØ li·ªáu, trong khi c√°c b∆∞·ªõc chuy·ªÉn ƒë·ªïi sang tensor ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü `collate_fn`.
- Ki·ªÉm tra ƒë·ªô d√†i hai danh s√°ch c√¢u nh·∫±m ƒë·∫£m b·∫£o **cƒÉn ch·ªânh 1‚Äì1 gi·ªØa source v√† target**.

### Collate Function
H√†m `collate_fn` ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x·ª≠ l√Ω t·ª´ng batch d·ªØ li·ªáu v·ªõi c√°c b∆∞·ªõc sau:

1. **Text Transformation**  
   Chuy·ªÉn c√¢u ngu·ªìn v√† c√¢u ƒë√≠ch t·ª´ d·∫°ng text sang tensor s·ªë b·∫±ng pipeline ƒë√£ x√¢y d·ª±ng ·ªü b∆∞·ªõc 1.4.

2. **Padding**  
   C√°c chu·ªói trong batch ƒë∆∞·ª£c padding v·ªÅ c√πng ƒë·ªô d√†i b·∫±ng token `<pad>`, gi√∫p t·∫°o tensor c√≥ k√≠ch th∆∞·ªõc ƒë·ªìng nh·∫•t.

3. **Length Computation and Sorting**  
   - T√≠nh ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa t·ª´ng c√¢u ngu·ªìn.
   - S·∫Øp x·∫øp c√°c c√¢u theo ƒë·ªô d√†i gi·∫£m d·∫ßn, ƒë√¢y l√† y√™u c·∫ßu b·∫Øt bu·ªôc ƒë·ªÉ s·ª≠ d·ª•ng `pack_padded_sequence` trong LSTM.

H√†m `collate_fn` tr·∫£ v·ªÅ:
- Tensor c√¢u ngu·ªìn ƒë√£ padding
- Tensor c√¢u ƒë√≠ch ƒë√£ padding
- Vector ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa c√¢u ngu·ªìn

### DataLoader
- T·∫°o c√°c `DataLoader` ri√™ng cho **training**, **validation** v√† **test**.
- D·ªØ li·ªáu training ƒë∆∞·ª£c shuffle ƒë·ªÉ tƒÉng t√≠nh ng·∫´u nhi√™n trong hu·∫•n luy·ªán.
- Validation v√† test gi·ªØ nguy√™n th·ª© t·ª± ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√°nh gi√° nh·∫•t qu√°n.
- T·∫•t c·∫£ c√°c DataLoader ƒë·ªÅu s·ª≠ d·ª•ng chung `collate_fn` nh·∫±m ƒë·∫£m b·∫£o x·ª≠ l√Ω d·ªØ li·ªáu th·ªëng nh·∫•t.

### Validation of Data Pipeline
- Th·ª±c hi·ªán ki·ªÉm tra m·ªôt batch d·ªØ li·ªáu t·ª´ `train_loader` ƒë·ªÉ x√°c nh·∫≠n:
  - K√≠ch th∆∞·ªõc tensor ƒë√∫ng ƒë·ªãnh d·∫°ng `(sequence_length, batch_size)`
  - Batch ƒë√£ ƒë∆∞·ª£c sort theo ƒë·ªô d√†i gi·∫£m d·∫ßn
- Vi·ªác ki·ªÉm tra n√†y x√°c nh·∫≠n r·∫±ng pipeline d·ªØ li·ªáu ƒë√£ **s·∫µn s√†ng cho vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq v·ªõi LSTM v√† Attention** ·ªü c√°c ph·∫ßn ti·∫øp theo.

Sau b∆∞·ªõc n√†y, to√†n b·ªô **pipeline x·ª≠ l√Ω d·ªØ li·ªáu** ƒë√£ ho√†n thi·ªán, cho ph√©p chuy·ªÉn sang **PH·∫¶N 2: X√¢y d·ª±ng m√¥ h√¨nh**.

"""

# ==============================================================================
# CELL 1.5: DATASET, COLLATE_FN & DATALOADER
# ==============================================================================

# =============================================================================
# CLASS: TranslationDataset
# =============================================================================
class TranslationDataset(Dataset):
    """
    Dataset cho d·ªØ li·ªáu song ng·ªØ.
    L∆∞u tr·ªØ c·∫∑p c√¢u (source, target) d·∫°ng text.
    """
    def __init__(self, src_list, trg_list):
        self.src_list = src_list
        self.trg_list = trg_list
        assert len(src_list) == len(trg_list), "S·ªë c√¢u source v√† target kh√¥ng kh·ªõp!"

    def __len__(self):
        return len(self.src_list)

    def __getitem__(self, idx):
        return self.src_list[idx], self.trg_list[idx]

# =============================================================================
# H√ÄM: collate_fn
# X·ª≠ l√Ω batch: padding + sorting theo ƒë·ªô d√†i (cho pack_padded_sequence)
# =============================================================================
def collate_fn(batch):
    """
    X·ª≠ l√Ω batch d·ªØ li·ªáu:
    1. Chuy·ªÉn text ‚Üí tensor
    2. Padding ƒë·ªÉ ƒë·ªìng b·ªô ƒë·ªô d√†i
    3. Sort theo ƒë·ªô d√†i gi·∫£m d·∫ßn (y√™u c·∫ßu cho pack_padded_sequence)

    Returns:
        src_padded: [src_len, batch_size]
        trg_padded: [trg_len, batch_size]
        sorted_lens: [batch_size] - ƒë·ªô d√†i th·ª±c c·ªßa t·ª´ng c√¢u source
    """
    src_batch, trg_batch = [], []

    # Chuy·ªÉn ƒë·ªïi Text ‚Üí Tensor
    for src_sample, trg_sample in batch:
        src_batch.append(text_transform(src_sample, tokenizer_en, vocab_en))
        trg_batch.append(text_transform(trg_sample, tokenizer_fr, vocab_fr))

    # Padding (ƒë·ªìng b·ªô ƒë·ªô d√†i trong batch)
    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX)
    trg_padded = pad_sequence(trg_batch, padding_value=PAD_IDX)

    # T√≠nh ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa t·ª´ng c√¢u ngu·ªìn (dtype=long cho pack_padded_sequence)
    src_lens = torch.tensor([len(x) for x in src_batch], dtype=torch.long)

    # Sort gi·∫£m d·∫ßn theo ƒë·ªô d√†i (y√™u c·∫ßu c·ªßa pack_padded_sequence v·ªõi enforce_sorted=True)
    sorted_lens, sorted_indices = torch.sort(src_lens, descending=True)

    # S·∫Øp x·∫øp l·∫°i tensors theo th·ª© t·ª± ƒë√£ sort
    src_padded = src_padded[:, sorted_indices]
    trg_padded = trg_padded[:, sorted_indices]

    return src_padded, trg_padded, sorted_lens

# =============================================================================
# T·∫†O DATALOADER
# =============================================================================
BATCH_SIZE = 64

# T·∫°o Dataset
train_dataset = TranslationDataset(train_en, train_fr)
valid_dataset = TranslationDataset(val_en, val_fr)
test_dataset = TranslationDataset(test_en, test_fr)

# T·∫°o DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,           # Shuffle cho training
    collate_fn=collate_fn
)
valid_loader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn
)
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn
)

# =============================================================================
# KI·ªÇM TRA DATALOADER
# =============================================================================
print("=" * 50)
print("üîç KI·ªÇM TRA DATALOADER")
print("=" * 50)

try:
    src, trg, src_len = next(iter(train_loader))

    print(f"1. Source shape:   {src.shape} (seq_len, batch_size)")
    print(f"2. Target shape:   {trg.shape} (seq_len, batch_size)")
    print(f"3. Lengths shape:  {src_len.shape}")
    print(f"   - C√¢u d√†i nh·∫•t:  {src_len[0]} tokens")
    print(f"   - C√¢u ng·∫Øn nh·∫•t: {src_len[-1]} tokens")

    # Ki·ªÉm tra sorting
    if src_len[0] >= src_len[-1]:
        print("\n‚úÖ Batch ƒë√£ SORT theo ƒë·ªô d√†i (gi·∫£m d·∫ßn)")
        print("   ‚Üí S·∫µn s√†ng cho pack_padded_sequence!")
    else:
        print("\n‚ùå L·ªói: Batch ch∆∞a ƒë∆∞·ª£c sort ƒë√∫ng!")

    print("\n" + "-" * 50)
    print(f"üìä S·ªê BATCH:")
    print(f"   Train:      {len(train_loader)} batches")
    print(f"   Validation: {len(valid_loader)} batches")
    print(f"   Test:       {len(test_loader)} batches")
    print("=" * 50)
    print("\n‚úÖ PH·∫¶N 1 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 2 (Model)!")

except Exception as e:
    print(f"\n‚ùå L·ªñI: {e}")
    print("G·ª£i √Ω: Ki·ªÉm tra l·∫°i collate_fn c√≥ tr·∫£ v·ªÅ ƒë√∫ng 3 gi√° tr·ªã kh√¥ng?")

"""# üèóÔ∏è PH·∫¶N 2: X√ÇY D·ª∞NG M√î H√åNH BASELINE SEQ2SEQ

---

## Ki·∫øn tr√∫c Encoder-Decoder LSTM (Kh√¥ng Attention)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     BASELINE SEQ2SEQ ARCHITECTURE              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                ‚îÇ
‚îÇ  INPUT (EN)       ENCODER           DECODER        OUTPUT (FR) ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  "A man walks" ‚Üí [LSTM x2] ‚Üí (h,c) ‚Üí [LSTM x2] ‚Üí "Un homme..." ‚îÇ
‚îÇ                              ‚Üë                                 ‚îÇ
‚îÇ                       Context Vector                           ‚îÇ
‚îÇ                  (Fixed representation)                        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**C√¥ng th·ª©c:**
- Encoder: `(h_t, c_t) = LSTM(embed(x_t), (h_{t-1}, c_{t-1}))`
- Decoder: `(h_t, c_t) = LSTM(embed(y_{t-1}), (h'_{t-1}, c'_{t-1}))`
- Output:  `p(y_t) = softmax(Linear(h_t))`

**Hyperparameters:**

| Parameter | Value | M√¥ t·∫£ |
|-----------|-------|-------|
| `EMB_DIM` | 256 | Embedding dimension |
| `HID_DIM` | 512 | Hidden state dimension |
| `N_LAYERS` | 2 | S·ªë l·ªõp LSTM |
| `DROPOUT` | 0.5 | Dropout rate |

---

## 2.1 Baseline Seq2Seq Model Definition (Fixed Context Vector)

Trong ph·∫ßn n√†y, ch√∫ng t√¥i x√¢y d·ª±ng **m√¥ h√¨nh Seq2Seq c∆° b·∫£n (Baseline)** d·ª±a tr√™n ki·∫øn tr√∫c **Encoder‚ÄìDecoder s·ª≠ d·ª•ng LSTM**, **kh√¥ng c√≥ Attention**.  
M·ª•c ti√™u c·ªßa m√¥ h√¨nh n√†y l√† **th·ª±c hi·ªán d·ªãch m√°y v·ªõi context vector c·ªë ƒë·ªãnh**, ƒë√∫ng theo y√™u c·∫ßu c∆° b·∫£n c·ªßa ƒë·ªì √°n.

### Encoder
- Encoder ƒë∆∞·ª£c c√†i ƒë·∫∑t b·∫±ng **LSTM nhi·ªÅu l·ªõp**, c√≥ nhi·ªám v·ª• ƒë·ªçc to√†n b·ªô c√¢u ngu·ªìn.
- ƒê·ªÉ x·ª≠ l√Ω c√°c chu·ªói c√≥ ƒë·ªô d√†i kh√°c nhau, ch√∫ng t√¥i s·ª≠ d·ª•ng:
  - `pack_padded_sequence` k·∫øt h·ª£p v·ªõi batch ƒë√£ ƒë∆∞·ª£c **sort theo ƒë·ªô d√†i gi·∫£m d·∫ßn**.
- Sau khi ƒë·ªçc xong c√¢u ngu·ªìn, Encoder **t√≥m t·∫Øt to√†n b·ªô th√¥ng tin c·ªßa c√¢u** v√†o:
  - **Hidden states**
  - **Cell states**
- Trong m√¥ h√¨nh baseline n√†y, **hidden v√† cell states cu·ªëi c√πng c·ªßa Encoder ƒë√≥ng vai tr√≤ l√† context vector c·ªë ƒë·ªãnh**, ƒë∆∞·ª£c truy·ªÅn tr·ª±c ti·∫øp sang Decoder.

> L∆∞u √Ω: Encoder **kh√¥ng tr·∫£ v·ªÅ to√†n b·ªô encoder outputs**, v√¨ baseline Seq2Seq **kh√¥ng s·ª≠ d·ª•ng c∆° ch·∫ø Attention**.

### Decoder
- Decoder c≈©ng ƒë∆∞·ª£c c√†i ƒë·∫∑t b·∫±ng **LSTM nhi·ªÅu l·ªõp**.
- T·∫°i m·ªói timestep, Decoder:
  - Nh·∫≠n token ƒë·∫ßu v√†o hi·ªán t·∫°i
  - S·ª≠ d·ª•ng **context vector c·ªë ƒë·ªãnh (hidden & cell t·ª´ Encoder)** ƒë·ªÉ sinh ra token ti·∫øp theo c·ªßa c√¢u ƒë√≠ch.
- M·ªói output c·ªßa Decoder ƒë∆∞·ª£c √°nh x·∫° qua m·ªôt l·ªõp `Linear` ƒë·ªÉ d·ª± ƒëo√°n ph√¢n ph·ªëi x√°c su·∫•t tr√™n vocabulary ƒë√≠ch.

### Seq2Seq Model
- L·ªõp `Seq2Seq` ch·ªãu tr√°ch nhi·ªám ƒëi·ªÅu ph·ªëi qu√° tr√¨nh:
  - Encode c√¢u ngu·ªìn
  - Decode c√¢u ƒë√≠ch theo t·ª´ng timestep
- M√¥ h√¨nh h·ªó tr·ª£ **teacher forcing**, cho ph√©p s·ª≠ d·ª•ng token ground truth v·ªõi m·ªôt t·ª∑ l·ªá x√°c ƒë·ªãnh trong qu√° tr√¨nh hu·∫•n luy·ªán.
- Output c·ªßa m√¥ h√¨nh l√† m·ªôt tensor ch·ª©a logits cho to√†n b·ªô c√¢u ƒë√≠ch.

### Weight Initialization
- C√°c tham s·ªë c·ªßa m√¥ h√¨nh ƒë∆∞·ª£c kh·ªüi t·∫°o b·∫±ng **uniform distribution** trong kho·∫£ng `[-0.08, 0.08]`.
- Vi·ªác kh·ªüi t·∫°o n√†y gi√∫p m√¥ h√¨nh:
  - H·ªôi t·ª• ·ªïn ƒë·ªãnh h∆°n trong giai ƒëo·∫°n ƒë·∫ßu hu·∫•n luy·ªán
  - Tr√°nh hi·ªán t∆∞·ª£ng gradient qu√° l·ªõn

M√¥ h√¨nh Baseline Seq2Seq n√†y ƒë√≥ng vai tr√≤ **m√¥ h√¨nh c∆° s·ªü (baseline)** ƒë·ªÉ:
- Ch·ª©ng minh c∆° ch·∫ø **context vector c·ªë ƒë·ªãnh**
- L√†m ƒë·ªëi ch·ª©ng cho m√¥ h√¨nh **Seq2Seq c√≥ Attention** ƒë∆∞·ª£c tr√¨nh b√†y ·ªü c√°c ph·∫ßn ti·∫øp theo.
"""

# ==============================================================================
# CELL 2.1: ƒê·ªäNH NGHƒ®A C√ÅC CLASS MODEL
# ==============================================================================

# =============================================================================
# CLASS: Encoder
# =============================================================================
class Encoder(nn.Module):
    """
    Encoder LSTM cho Seq2Seq.

    Nhi·ªám v·ª•: ƒê·ªçc c√¢u ngu·ªìn v√† t·∫°o context vector (hidden, cell states).
    S·ª≠ d·ª•ng pack_padded_sequence ƒë·ªÉ x·ª≠ l√Ω padding hi·ªáu qu·∫£.

    Args:
        input_dim: K√≠ch th∆∞·ªõc vocabulary ngu·ªìn
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: S·ªë l·ªõp LSTM
        dropout: Dropout rate
    """
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_len):
        """
        Forward pass c·ªßa Encoder.

        Args:
            src: [src_len, batch_size] - Tensor c√¢u ngu·ªìn
            src_len: [batch_size] - ƒê·ªô d√†i th·ª±c c·ªßa m·ªói c√¢u

        Returns:
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states

        Note: Baseline KH√îNG tr·∫£ v·ªÅ encoder_outputs (s·∫Ω th√™m khi c√≥ Attention)
        """
        # src: [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        # embedded: [src_len, batch_size, emb_dim]

        # Pack ƒë·ªÉ LSTM kh√¥ng x·ª≠ l√Ω padding tokens
        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)

        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]

        return hidden, cell


# =============================================================================
# CLASS: Decoder
# =============================================================================
class Decoder(nn.Module):
    """
    Decoder LSTM cho Seq2Seq (Baseline - kh√¥ng Attention).

    Nhi·ªám v·ª•: Sinh c√¢u ƒë√≠ch t·ª´ng token m·ªôt, d·ª±a tr√™n context t·ª´ Encoder.

    Args:
        output_dim: K√≠ch th∆∞·ªõc vocabulary ƒë√≠ch
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: S·ªë l·ªõp LSTM
        dropout: Dropout rate
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        """
        Forward pass c·ªßa Decoder (1 timestep).

        Args:
            input: [batch_size] - Token hi·ªán t·∫°i
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states

        Returns:
            prediction: [batch_size, output_dim] - Logits cho vocabulary
            hidden: [n_layers, batch_size, hid_dim] - Updated hidden
            cell: [n_layers, batch_size, hid_dim] - Updated cell
        """
        # input: [batch_size] ‚Üí [1, batch_size]
        input = input.unsqueeze(0)

        embedded = self.dropout(self.embedding(input))
        # embedded: [1, batch_size, emb_dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [1, batch_size, hid_dim]

        prediction = self.fc_out(output.squeeze(0))
        # prediction: [batch_size, output_dim]

        return prediction, hidden, cell


# =============================================================================
# CLASS: Seq2Seq
# =============================================================================
class Seq2Seq(nn.Module):
    """
    M√¥ h√¨nh Seq2Seq Baseline (Encoder-Decoder kh√¥ng Attention).

    ƒêi·ªÅu ph·ªëi Encoder v√† Decoder, x·ª≠ l√Ω teacher forcing.

    Args:
        encoder: Encoder instance
        decoder: Decoder instance
        device: 'cuda' ho·∫∑c 'cpu'
    """
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

        # ƒê·∫£m b·∫£o encoder v√† decoder t∆∞∆°ng th√≠ch
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must match!"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers of encoder and decoder must match!"

    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Forward pass c·ªßa Seq2Seq.

        Args:
            src: [src_len, batch_size] - C√¢u ngu·ªìn
            src_len: [batch_size] - ƒê·ªô d√†i c√¢u ngu·ªìn
            trg: [trg_len, batch_size] - C√¢u ƒë√≠ch
            teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng ground truth (0.0 - 1.0)

        Returns:
            outputs: [trg_len, batch_size, output_dim] - Logits cho m·ªói timestep
        """
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        # Tensor l∆∞u output c·ªßa decoder t·∫°i m·ªói timestep
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        # Encode c√¢u ngu·ªìn
        hidden, cell = self.encoder(src, src_len)

        # Token ƒë·∫ßu ti√™n l√† <sos>
        input = trg[0, :]

        # Decode t·ª´ng timestep
        for t in range(1, trg_len):
            # Forward decoder
            output, hidden, cell = self.decoder(input, hidden, cell)

            # L∆∞u output
            outputs[t] = output

            # Teacher Forcing: d√πng ground truth ho·∫∑c prediction
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1

        return outputs


# =============================================================================
# H√ÄM: init_weights
# =============================================================================
def init_weights(m):
    """
    Kh·ªüi t·∫°o weights cho model.
    - Weights: Uniform distribution [-0.08, 0.08]
    - Biases: 0
    """
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.uniform_(param.data, -0.08, 0.08)
        else:
            nn.init.constant_(param.data, 0)


print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: Encoder, Decoder, Seq2Seq, init_weights")

"""## 2.2 Baseline Model Initialization

Trong cell n√†y, ch√∫ng t√¥i ti·∫øn h√†nh **kh·ªüi t·∫°o v√† ki·ªÉm tra m√¥ h√¨nh Seq2Seq Baseline**, s·ª≠ d·ª•ng **context vector c·ªë ƒë·ªãnh** theo ƒë√∫ng y√™u c·∫ßu c∆° b·∫£n c·ªßa ƒë·ªì √°n.

### Model Configuration
- M√¥ h√¨nh bao g·ªìm:
  - **Encoder LSTM**
  - **Decoder LSTM**
- Kh√¥ng s·ª≠ d·ª•ng **Attention mechanism**.
- Context vector ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi:
  - **Hidden state cu·ªëi c√πng**
  - **Cell state cu·ªëi c√πng**
  c·ªßa Encoder, v√† ƒë∆∞·ª£c truy·ªÅn tr·ª±c ti·∫øp sang Decoder.

C√°c hyperparameters ch√≠nh:
- Embedding dimension cho Encoder v√† Decoder
- Hidden state dimension
- S·ªë l·ªõp LSTM
- Dropout rate  
Nh·ªØng tham s·ªë n√†y ƒë∆∞·ª£c l·ª±a ch·ªçn nh·∫±m c√¢n b·∫±ng gi·ªØa:
- Kh·∫£ nƒÉng bi·ªÉu di·ªÖn c·ªßa m√¥ h√¨nh
- Chi ph√≠ t√≠nh to√°n trong qu√° tr√¨nh hu·∫•n luy·ªán

### Baseline Model Initialization
- Encoder v√† Decoder ƒë∆∞·ª£c k·∫øt h·ª£p th√†nh m√¥ h√¨nh `Seq2Seq`.
- M√¥ h√¨nh ƒë∆∞·ª£c ƒë·∫∑t t√™n r√µ r√†ng l√† **`baseline_model`** ƒë·ªÉ ph√¢n bi·ªát v·ªõi m√¥ h√¨nh c√≥ Attention ·ªü c√°c ph·∫ßn sau.
- √Åp d·ª•ng **weight initialization** th·ªëng nh·∫•t ƒë·ªÉ:
  - Gi√∫p qu√° tr√¨nh hu·∫•n luy·ªán ·ªïn ƒë·ªãnh
  - Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa kh·ªüi t·∫°o ng·∫´u nhi√™n

### Model Inspection
- Th·ª±c hi·ªán th·ªëng k√™:
  - K√≠ch th∆∞·ªõc vocabulary ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
  - S·ªë l∆∞·ª£ng tham s·ªë hu·∫•n luy·ªán
- C√°c th√¥ng tin n√†y gi√∫p ƒë√°nh gi√° s∆° b·ªô ƒë·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh baseline.

### Forward Pass Validation
- Th·ª±c hi·ªán m·ªôt **forward pass ki·ªÉm tra** v·ªõi m·ªôt batch d·ªØ li·ªáu t·ª´ t·∫≠p hu·∫•n luy·ªán.
- Ki·ªÉm tra:
  - T√≠nh t∆∞∆°ng th√≠ch gi·ªØa d·ªØ li·ªáu v√† m√¥ h√¨nh
  - K√≠ch th∆∞·ªõc tensor ƒë·∫ßu ra c·ªßa m√¥ h√¨nh
- Vi·ªác ki·ªÉm tra n√†y nh·∫±m ƒë·∫£m b·∫£o r·∫±ng **Baseline Seq2Seq model ho·∫°t ƒë·ªông ƒë√∫ng v·ªÅ m·∫∑t h√¨nh d·∫°ng d·ªØ li·ªáu**, s·∫µn s√†ng cho b∆∞·ªõc hu·∫•n luy·ªán ·ªü ph·∫ßn ti·∫øp theo.

M√¥ h√¨nh Baseline n√†y ƒë√≥ng vai tr√≤ **m√¥ h√¨nh c∆° s·ªü (baseline)**, s·ª≠ d·ª•ng **context vector c·ªë ƒë·ªãnh**, v√† ƒë∆∞·ª£c d√πng l√†m **ƒë·ªëi ch·ª©ng** khi so s√°nh v·ªõi m√¥ h√¨nh Seq2Seq c√≥ Attention ·ªü c√°c ph·∫ßn sau c·ªßa ƒë·ªì √°n.

"""

# ==============================================================================
# CELL 2.2: KH·ªûI T·∫†O BASELINE MODEL (FIXED CONTEXT VECTOR)
# ==============================================================================
# ‚ö†Ô∏è ƒê√ÇY L√Ä M√î H√åNH BASELINE - S·ª¨ D·ª§NG CONTEXT VECTOR C·ªê ƒê·ªäNH
# (hidden state cu·ªëi c√πng c·ªßa Encoder, KH√îNG c√≥ Attention)

# =============================================================================
# HYPERPARAMETERS
# =============================================================================
INPUT_DIM = len(vocab_en)    # Vocab size ti·∫øng Anh
OUTPUT_DIM = len(vocab_fr)   # Vocab size ti·∫øng Ph√°p
ENC_EMB_DIM = 256            # Encoder embedding dim
DEC_EMB_DIM = 256            # Decoder embedding dim
HID_DIM = 512                # Hidden state dim
N_LAYERS = 2                 # S·ªë l·ªõp LSTM
ENC_DROPOUT = 0.5            # Encoder dropout
DEC_DROPOUT = 0.5            # Decoder dropout

# =============================================================================
# KH·ªûI T·∫†O BASELINE MODEL
# =============================================================================
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

# ‚ö†Ô∏è QUAN TR·ªåNG: ƒê·∫∑t t√™n bi·∫øn l√† baseline_model ƒë·ªÉ ph√¢n bi·ªát v·ªõi attention_model
baseline_model = Seq2Seq(enc, dec, device).to(device)

# √Åp d·ª•ng weight initialization
baseline_model.apply(init_weights)

# ƒê·∫øm parameters
baseline_params = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)

print("=" * 60)
print("üèóÔ∏è M√î H√åNH BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)")
print("=" * 60)
print("üìå ƒê·∫∑c ƒëi·ªÉm: S·ª≠ d·ª•ng CONTEXT VECTOR C·ªê ƒê·ªäNH")
print("   (Ch·ªâ d√πng hidden state cu·ªëi c√πng c·ªßa Encoder)")
print("-" * 60)
print(f"Device:           {device}")
print(f"Input dim (EN):   {INPUT_DIM:,}")
print(f"Output dim (FR):  {OUTPUT_DIM:,}")
print(f"Embedding dim:    {ENC_EMB_DIM}")
print(f"Hidden dim:       {HID_DIM}")
print(f"Num layers:       {N_LAYERS}")
print(f"Dropout:          {ENC_DROPOUT}")
print("-" * 60)
print(f"Total parameters: {baseline_params:,}")
print("=" * 60)

# =============================================================================
# KI·ªÇM TRA K·∫æT N·ªêI DATA ‚Üí MODEL
# =============================================================================
print("\n" + "=" * 60)
print("üîç KI·ªÇM TRA FORWARD PASS (BASELINE)")
print("=" * 60)

try:
    # L·∫•y 1 batch t·ª´ train_loader
    src, trg, src_len = next(iter(train_loader))
    src = src.to(device)
    trg = trg.to(device)

    print(f"Input (src):      {src.shape}  [seq_len, batch_size]")
    print(f"Input (src_len):  {src_len.shape}  [batch_size]")
    print(f"Target (trg):     {trg.shape}  [seq_len, batch_size]")

    # Forward pass v·ªõi baseline_model
    output = baseline_model(src, src_len, trg)

    print(f"Output:           {output.shape}  [seq_len, batch_size, vocab_size]")

    # Validate output shape
    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)
    if output.shape == expected_shape:
        print("\n‚úÖ BASELINE FORWARD PASS TH√ÄNH C√îNG!")
        print("   ‚Üí Baseline Model s·∫µn s√†ng cho Training.")
    else:
        print(f"\n‚ö†Ô∏è Shape kh√¥ng kh·ªõp! Expected: {expected_shape}")

except Exception as e:
    print(f"\n‚ùå L·ªñI: {e}")
    import traceback
    traceback.print_exc()

print("=" * 60)
print("\n‚úÖ PH·∫¶N 2 HO√ÄN T·∫§T - Baseline Model ƒë√£ s·∫µn s√†ng!")

"""# üéØ PH·∫¶N 3: SEQ2SEQ + LUONG ATTENTION

---

## Ki·∫øn tr√∫c Encoder-Decoder LSTM v·ªõi Attention

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SEQ2SEQ + LUONG ATTENTION ARCHITECTURE               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  ENCODER                              DECODER                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  "A man walks"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                 ‚îÇ
‚îÇ       ‚Üì               ‚îÇ                                                 ‚îÇ
‚îÇ   [LSTM x2]           ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ       ‚Üì               ‚îÇ              ‚îÇ   ATTENTION     ‚îÇ               ‚îÇ
‚îÇ  encoder_outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (Luong General)‚îÇ               ‚îÇ
‚îÇ  (h‚ÇÅ, h‚ÇÇ, ..., h‚Çô)    ‚îÇ              ‚îÇ                 ‚îÇ               ‚îÇ
‚îÇ       ‚Üì               ‚îÇ              ‚îÇ  score = h‚Çú·µÄW‚Çêh‚Çõ‚îÇ               ‚îÇ
‚îÇ  (hidden, cell) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Œ± = softmax    ‚îÇ               ‚îÇ
‚îÇ                       ‚îÇ              ‚îÇ  c = Œ£ Œ±·µ¢h·µ¢     ‚îÇ               ‚îÇ
‚îÇ                       ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                       ‚îÇ                       ‚Üì                         ‚îÇ
‚îÇ                       ‚îÇ              context_vector                     ‚îÇ
‚îÇ                       ‚îÇ                       ‚Üì                         ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [embed; context]                 ‚îÇ
‚îÇ                                              ‚Üì                          ‚îÇ
‚îÇ                                         [LSTM x2]                       ‚îÇ
‚îÇ                                              ‚Üì                          ‚îÇ
‚îÇ                                    [hidden; context]                    ‚îÇ
‚îÇ                                              ‚Üì                          ‚îÇ
‚îÇ                                         Linear ‚Üí vocab                  ‚îÇ
‚îÇ                                              ‚Üì                          ‚îÇ
‚îÇ                                      "Un homme marche"                  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Luong Attention (General)

**C√¥ng th·ª©c:**

$$score(h_t, h_s) = h_t^T \cdot W_a \cdot h_s$$

$$\alpha = softmax(score)$$

$$context = \sum_i \alpha_i \cdot h_i$$

**Tham kh·∫£o:** Luong et al. (2015) - "Effective Approaches to Attention-based NMT"

---

## 3.1 Attention Mechanism and Attention-based Seq2Seq Model

Trong ph·∫ßn n√†y, ch√∫ng t√¥i m·ªü r·ªông m√¥ h√¨nh Seq2Seq baseline b·∫±ng c√°ch **t√≠ch h·ª£p c∆° ch·∫ø Attention**, nh·∫±m kh·∫Øc ph·ª•c h·∫°n ch·∫ø c·ªßa **context vector c·ªë ƒë·ªãnh** trong m√¥ h√¨nh c∆° b·∫£n.

### Motivation for Attention
Trong m√¥ h√¨nh Seq2Seq baseline, to√†n b·ªô c√¢u ngu·ªìn ƒë∆∞·ª£c n√©n v√†o m·ªôt **context vector duy nh·∫•t** (hidden state cu·ªëi c·ªßa Encoder).  
C√°ch ti·∫øp c·∫≠n n√†y th∆∞·ªùng g·∫∑p kh√≥ khƒÉn khi x·ª≠ l√Ω c√°c c√¢u d√†i, do th√¥ng tin c√≥ th·ªÉ b·ªã m·∫•t trong qu√° tr√¨nh n√©n.

C∆° ch·∫ø **Attention** cho ph√©p Decoder:
- Truy c·∫≠p **to√†n b·ªô c√°c hidden states c·ªßa Encoder**
- T·∫≠p trung v√†o nh·ªØng ph·∫ßn quan tr·ªçng c·ªßa c√¢u ngu·ªìn t·∫°i m·ªói timestep
- C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch, ƒë·∫∑c bi·ªát v·ªõi c√°c c√¢u d√†i v√† c·∫•u tr√∫c ph·ª©c t·∫°p

### Luong General Attention
Ch√∫ng t√¥i s·ª≠ d·ª•ng **Luong General Attention** (Luong et al., 2015), v·ªõi h√†m score ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:

\[
\text{score}(h_t, h_s) = h_t^\top W_a h_s
\]

Trong ƒë√≥:
- \(h_t\): hidden state hi·ªán t·∫°i c·ªßa Decoder
- \(h_s\): hidden state c·ªßa Encoder t·∫°i t·ª´ng timestep
- \(W_a\): ma tr·∫≠n h·ªçc ƒë∆∞·ª£c

Attention weights ƒë∆∞·ª£c chu·∫©n h√≥a b·∫±ng **softmax**, sau ƒë√≥ d√πng ƒë·ªÉ t√≠nh **context vector ƒë·ªông** t·∫°i m·ªói timestep c·ªßa Decoder.

### Encoder with Attention Support
- EncoderAttention ƒë∆∞·ª£c m·ªü r·ªông t·ª´ Encoder baseline.
- Ngo√†i hidden v√† cell states cu·ªëi, EncoderAttention **tr·∫£ v·ªÅ to√†n b·ªô encoder outputs**.
- C√°c encoder outputs n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√≠nh attention weights v√† context vector ƒë·ªông.

### Decoder with Attention
- DecoderAttention t√≠ch h·ª£p Luong Attention v√†o qu√° tr√¨nh sinh token.
- T·∫°i m·ªói timestep:
  1. Decoder t√≠nh attention weights d·ª±a tr√™n hidden state hi·ªán t·∫°i.
  2. T·∫°o context vector b·∫±ng weighted sum c·ªßa encoder outputs.
  3. K·∫øt h·ª£p context vector v·ªõi embedding c·ªßa input token.
  4. Sinh ra token ti·∫øp theo c·ªßa c√¢u ƒë√≠ch.
- Vi·ªác k·∫øt h·ª£p n√†y cho ph√©p Decoder **ƒëi·ªÅu ch·ªânh v√πng t·∫≠p trung** tr√™n c√¢u ngu·ªìn theo t·ª´ng timestep.

### Attention-based Seq2Seq Model
- L·ªõp `Seq2SeqAttention` ƒëi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh encode‚Äìdecode v·ªõi Attention.
- M√¥ h√¨nh:
  - T·∫°o **mask cho padding positions** ƒë·ªÉ Attention kh√¥ng t·∫≠p trung v√†o c√°c token `<pad>`.
  - H·ªó tr·ª£ **teacher forcing** t∆∞∆°ng t·ª± m√¥ h√¨nh baseline.
- ƒê√¢y l√† **m√¥ h√¨nh ch√≠nh** ƒë∆∞·ª£c s·ª≠ d·ª•ng cho hu·∫•n luy·ªán, ƒë√°nh gi√° BLEU v√† inference trong c√°c ph·∫ßn ti·∫øp theo.

M√¥ h√¨nh Seq2Seq v·ªõi Attention n√†y ƒë√≥ng vai tr√≤ l√† **phi√™n b·∫£n m·ªü r·ªông** c·ªßa m√¥ h√¨nh baseline, gi√∫p so s√°nh tr·ª±c ti·∫øp hi·ªáu qu·∫£ gi·ªØa:
- **Context vector c·ªë ƒë·ªãnh (Baseline)**
- **Context vector ƒë·ªông v·ªõi Attention**
"""

# ==============================================================================
# CELL 3.1: ATTENTION MECHANISM + ENCODER/DECODER V·ªöI ATTENTION
# ==============================================================================

# =============================================================================
# CLASS: Attention (Luong General)
# =============================================================================
class Attention(nn.Module):
    """
    Luong General Attention.

    C√¥ng th·ª©c: score(h_t, h_s) = h_t^T * W_a * h_s

    Args:
        hid_dim: Hidden dimension c·ªßa encoder/decoder

    Reference: "Effective Approaches to Attention-based NMT" (Luong et al., 2015)
    """
    def __init__(self, hid_dim):
        super().__init__()
        # W_a: Linear layer ƒë·ªÉ t√≠nh score
        self.W_a = nn.Linear(hid_dim, hid_dim, bias=False)

    def forward(self, decoder_hidden, encoder_outputs, mask=None):
        """
        T√≠nh attention weights v√† context vector.

        Args:
            decoder_hidden: [batch_size, hid_dim] - Hidden state hi·ªán t·∫°i c·ªßa decoder
            encoder_outputs: [src_len, batch_size, hid_dim] - T·∫•t c·∫£ hidden states c·ªßa encoder
            mask: [batch_size, src_len] - Mask cho padding (1=valid, 0=pad)

        Returns:
            context: [batch_size, hid_dim] - Context vector
            attention_weights: [batch_size, src_len] - Attention weights
        """
        # decoder_hidden: [batch_size, hid_dim]
        # encoder_outputs: [src_len, batch_size, hid_dim]

        src_len = encoder_outputs.shape[0]

        # Reshape decoder_hidden: [batch_size, hid_dim] ‚Üí [batch_size, 1, hid_dim]
        decoder_hidden = decoder_hidden.unsqueeze(1)

        # Permute encoder_outputs: [src_len, batch_size, hid_dim] ‚Üí [batch_size, src_len, hid_dim]
        encoder_outputs_perm = encoder_outputs.permute(1, 0, 2)

        # T√≠nh W_a * h_s: [batch_size, src_len, hid_dim]
        energy = self.W_a(encoder_outputs_perm)

        # T√≠nh h_t^T * (W_a * h_s): [batch_size, 1, hid_dim] x [batch_size, hid_dim, src_len]
        # ‚Üí [batch_size, 1, src_len]
        attention_scores = torch.bmm(decoder_hidden, energy.permute(0, 2, 1))

        # Squeeze: [batch_size, 1, src_len] ‚Üí [batch_size, src_len]
        attention_scores = attention_scores.squeeze(1)

        # √Åp d·ª•ng mask (n·∫øu c√≥) - ƒë·∫∑t padding positions = -inf
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)

        # Softmax ƒë·ªÉ c√≥ attention weights: [batch_size, src_len]
        attention_weights = F.softmax(attention_scores, dim=1)

        # T√≠nh context vector: weighted sum c·ªßa encoder outputs
        # [batch_size, 1, src_len] x [batch_size, src_len, hid_dim] ‚Üí [batch_size, 1, hid_dim]
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs_perm)

        # Squeeze: [batch_size, 1, hid_dim] ‚Üí [batch_size, hid_dim]
        context = context.squeeze(1)

        return context, attention_weights


# =============================================================================
# CLASS: EncoderAttention (S·ª≠a t·ª´ Encoder ƒë·ªÉ tr·∫£ v·ªÅ encoder_outputs)
# =============================================================================
class EncoderAttention(nn.Module):
    """
    Encoder LSTM cho Seq2Seq + Attention.

    Kh√°c v·ªõi Encoder baseline: Tr·∫£ v·ªÅ th√™m encoder_outputs cho Attention.

    Args:
        input_dim: K√≠ch th∆∞·ªõc vocabulary ngu·ªìn
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: S·ªë l·ªõp LSTM
        dropout: Dropout rate
    """
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_len):
        """
        Forward pass c·ªßa Encoder (cho Attention).

        Args:
            src: [src_len, batch_size] - Tensor c√¢u ngu·ªìn
            src_len: [batch_size] - ƒê·ªô d√†i th·ª±c c·ªßa m·ªói c√¢u

        Returns:
            encoder_outputs: [src_len, batch_size, hid_dim] - T·∫•t c·∫£ hidden states
            hidden: [n_layers, batch_size, hid_dim] - Hidden state cu·ªëi
            cell: [n_layers, batch_size, hid_dim] - Cell state cu·ªëi
        """
        # src: [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        # embedded: [src_len, batch_size, emb_dim]

        # Pack ƒë·ªÉ LSTM kh√¥ng x·ª≠ l√Ω padding
        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)

        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)

        # Unpack ƒë·ªÉ l·∫•y encoder_outputs cho Attention
        encoder_outputs, _ = pad_packed_sequence(packed_outputs)
        # encoder_outputs: [src_len, batch_size, hid_dim]
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]

        return encoder_outputs, hidden, cell


# =============================================================================
# CLASS: DecoderAttention (Decoder v·ªõi Luong Attention)
# =============================================================================
class DecoderAttention(nn.Module):
    """
    Decoder LSTM v·ªõi Luong Attention.

    Ki·∫øn tr√∫c:
    1. Embed input token
    2. T√≠nh attention weights v√† context vector
    3. Concat [embedding; context] ‚Üí LSTM input
    4. LSTM forward
    5. Concat [hidden; context] ‚Üí Linear ‚Üí vocab

    Args:
        output_dim: K√≠ch th∆∞·ªõc vocabulary ƒë√≠ch
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: S·ªë l·ªõp LSTM
        dropout: Dropout rate
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = Attention(hid_dim)

        # LSTM nh·∫≠n: embedding + context = emb_dim + hid_dim
        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)

        # Linear nh·∫≠n: hidden + context = hid_dim * 2
        self.fc_out = nn.Linear(hid_dim * 2, output_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell, encoder_outputs, mask):
        """
        Forward pass c·ªßa Decoder v·ªõi Attention (1 timestep).

        Args:
            input: [batch_size] - Token hi·ªán t·∫°i
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states
            encoder_outputs: [src_len, batch_size, hid_dim] - Encoder outputs
            mask: [batch_size, src_len] - Mask cho padding

        Returns:
            prediction: [batch_size, output_dim] - Logits cho vocabulary
            hidden: [n_layers, batch_size, hid_dim] - Updated hidden
            cell: [n_layers, batch_size, hid_dim] - Updated cell
            attention_weights: [batch_size, src_len] - Attention weights
        """
        # input: [batch_size] ‚Üí [1, batch_size]
        input = input.unsqueeze(0)

        # Embed
        embedded = self.dropout(self.embedding(input))
        # embedded: [1, batch_size, emb_dim]

        # T√≠nh Attention: d√πng hidden state c·ªßa layer cu·ªëi c√πng
        # hidden[-1]: [batch_size, hid_dim]
        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)
        # context: [batch_size, hid_dim]
        # attention_weights: [batch_size, src_len]

        # Concat embedding v√† context cho LSTM input
        # embedded: [1, batch_size, emb_dim]
        # context: [batch_size, hid_dim] ‚Üí [1, batch_size, hid_dim]
        lstm_input = torch.cat([embedded, context.unsqueeze(0)], dim=2)
        # lstm_input: [1, batch_size, emb_dim + hid_dim]

        # LSTM forward
        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
        # output: [1, batch_size, hid_dim]

        # Concat hidden v√† context cho prediction
        # output.squeeze(0): [batch_size, hid_dim]
        # context: [batch_size, hid_dim]
        combined = torch.cat([output.squeeze(0), context], dim=1)
        # combined: [batch_size, hid_dim * 2]

        # Prediction
        prediction = self.fc_out(combined)
        # prediction: [batch_size, output_dim]

        return prediction, hidden, cell, attention_weights


# =============================================================================
# CLASS: Seq2SeqAttention
# =============================================================================
class Seq2SeqAttention(nn.Module):
    """
    M√¥ h√¨nh Seq2Seq v·ªõi Luong Attention (M√¥ h√¨nh ch√≠nh).

    ƒêi·ªÅu ph·ªëi EncoderAttention v√† DecoderAttention.

    Args:
        encoder: EncoderAttention instance
        decoder: DecoderAttention instance
        device: 'cuda' ho·∫∑c 'cpu'
        pad_idx: Index c·ªßa padding token
    """
    def __init__(self, encoder, decoder, device, pad_idx):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.pad_idx = pad_idx

        # ƒê·∫£m b·∫£o encoder v√† decoder t∆∞∆°ng th√≠ch
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must match!"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers of encoder and decoder must match!"

    def create_mask(self, src):
        """
        T·∫°o mask cho padding positions.

        Args:
            src: [src_len, batch_size]

        Returns:
            mask: [batch_size, src_len] - 1 cho valid, 0 cho padding
        """
        mask = (src != self.pad_idx).permute(1, 0)
        # mask: [batch_size, src_len]
        return mask

    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Forward pass c·ªßa Seq2Seq v·ªõi Attention.

        Args:
            src: [src_len, batch_size] - C√¢u ngu·ªìn
            src_len: [batch_size] - ƒê·ªô d√†i c√¢u ngu·ªìn
            trg: [trg_len, batch_size] - C√¢u ƒë√≠ch
            teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng ground truth

        Returns:
            outputs: [trg_len, batch_size, output_dim] - Logits cho m·ªói timestep
        """
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        # Tensor l∆∞u outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        # Encode - gi·ªù tr·∫£ v·ªÅ th√™m encoder_outputs
        encoder_outputs, hidden, cell = self.encoder(src, src_len)
        # encoder_outputs: [src_len, batch_size, hid_dim]
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]

        # T·∫°o mask cho padding
        mask = self.create_mask(src)
        # mask: [batch_size, src_len]

        # Token ƒë·∫ßu ti√™n l√† <sos>
        input = trg[0, :]

        # Decode t·ª´ng timestep
        for t in range(1, trg_len):
            # Forward decoder v·ªõi attention
            output, hidden, cell, attention_weights = self.decoder(
                input, hidden, cell, encoder_outputs, mask
            )
            # output: [batch_size, output_dim]
            # attention_weights: [batch_size, src_len]

            # L∆∞u output
            outputs[t] = output

            # Teacher Forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1

        return outputs


print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: Attention, EncoderAttention, DecoderAttention, Seq2SeqAttention")

"""### 3.2. Attention Model Initialization and Sanity Check

Cell n√†y kh·ªüi t·∫°o m√¥ h√¨nh Seq2Seq v·ªõi c∆° ch·∫ø Luong Attention
v√† ki·ªÉm tra forward pass ƒë·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh ho·∫°t ƒë·ªông ƒë√∫ng
tr∆∞·ªõc khi b∆∞·ªõc v√†o giai ƒëo·∫°n hu·∫•n luy·ªán.

"""

# ==============================================================================
# CELL 3.2: KH·ªûI T·∫†O ATTENTION MODEL V√Ä KI·ªÇM TRA
# ==============================================================================

# =============================================================================
# KH·ªûI T·∫†O MODEL V·ªöI ATTENTION
# =============================================================================
# Hyperparameters gi·ªØ nguy√™n t·ª´ baseline
enc_attn = EncoderAttention(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec_attn = DecoderAttention(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

# ‚ö†Ô∏è QUAN TR·ªåNG: ƒê·∫∑t t√™n bi·∫øn l√† attention_model ƒë·ªÉ ph√¢n bi·ªát v·ªõi baseline_model
attention_model = Seq2SeqAttention(enc_attn, dec_attn, device, PAD_IDX).to(device)

# √Åp d·ª•ng weight initialization
attention_model.apply(init_weights)

# ƒê·∫øm parameters
attention_params = sum(p.numel() for p in attention_model.parameters() if p.requires_grad)

print("=" * 60)
print("üéØ M√î H√åNH SEQ2SEQ + LUONG ATTENTION")
print("=" * 60)
print("üìå ƒê·∫∑c ƒëi·ªÉm: S·ª≠ d·ª•ng DYNAMIC CONTEXT VECTOR")
print("   (Attention weights thay ƒë·ªïi theo t·ª´ng timestep)")
print("-" * 60)
print(f"Device:           {device}")
print(f"Input dim (EN):   {INPUT_DIM:,}")
print(f"Output dim (FR):  {OUTPUT_DIM:,}")
print(f"Embedding dim:    {ENC_EMB_DIM}")
print(f"Hidden dim:       {HID_DIM}")
print(f"Num layers:       {N_LAYERS}")
print(f"Dropout:          {ENC_DROPOUT}")
print("-" * 60)
print(f"Attention params: {attention_params:,}")
print(f"Baseline params:  {baseline_params:,}")
print(f"Th√™m (Attention): {attention_params - baseline_params:,}")
print("=" * 60)

# =============================================================================
# KI·ªÇM TRA FORWARD PASS
# =============================================================================
print("\n" + "=" * 60)
print("üîç KI·ªÇM TRA FORWARD PASS (ATTENTION MODEL)")
print("=" * 60)

try:
    # L·∫•y 1 batch t·ª´ train_loader
    src, trg, src_len = next(iter(train_loader))
    src = src.to(device)
    trg = trg.to(device)

    print(f"Input (src):      {src.shape}  [seq_len, batch_size]")
    print(f"Input (src_len):  {src_len.shape}  [batch_size]")
    print(f"Target (trg):     {trg.shape}  [seq_len, batch_size]")

    # Test Encoder
    encoder_outputs, hidden, cell = attention_model.encoder(src, src_len)
    print(f"\nEncoder outputs:  {encoder_outputs.shape}  [src_len, batch_size, hid_dim]")
    print(f"Hidden:           {hidden.shape}  [n_layers, batch_size, hid_dim]")
    print(f"Cell:             {cell.shape}  [n_layers, batch_size, hid_dim]")

    # Test full forward pass
    output = attention_model(src, src_len, trg)
    print(f"\nOutput:           {output.shape}  [trg_len, batch_size, vocab_size]")

    # Validate output shape
    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)
    if output.shape == expected_shape:
        print("\n‚úÖ ATTENTION FORWARD PASS TH√ÄNH C√îNG!")
        print("   ‚Üí Attention Model s·∫µn s√†ng cho Training.")
    else:
        print(f"\n‚ö†Ô∏è Shape kh√¥ng kh·ªõp! Expected: {expected_shape}")

except Exception as e:
    print(f"\n‚ùå L·ªñI: {e}")
    import traceback
    traceback.print_exc()

print("=" * 60)

# ‚ö†Ô∏è KH√îNG g√°n l·∫°i bi·∫øn - s·ª≠ d·ª•ng baseline_model v√† attention_model ri√™ng bi·ªát
# ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c·∫£ hai m√¥ h√¨nh t·ªìn t·∫°i ƒë·ªôc l·∫≠p ƒë·ªÉ so s√°nh

print("\n" + "=" * 60)
print("üìå HAI M√î H√åNH ƒê√É KH·ªûI T·∫†O:")
print("   1. baseline_model  - Seq2Seq v·ªõi Fixed Context Vector")
print("   2. attention_model - Seq2Seq v·ªõi Luong Attention")
print("=" * 60)
print("\n‚úÖ PH·∫¶N 3 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 4 (Training)!")

"""# üöÄ PH·∫¶N 4: TRAINING PROCESS

---

## Quy tr√¨nh hu·∫•n luy·ªán

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         TRAINING LOOP                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  for epoch in range(N_EPOCHS):                                      ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ train_loss = train(model, train_loader, ...)               ‚îÇ
‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ Forward pass (v·ªõi Teacher Forcing 0.5)             ‚îÇ
‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ T√≠nh loss (b·ªè <sos>, ignore <pad>)                 ‚îÇ
‚îÇ      ‚îÇ       ‚îú‚îÄ‚îÄ Backward + Gradient Clipping                       ‚îÇ
‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Update weights                                     ‚îÇ
‚îÇ      ‚îÇ                                                              ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ valid_loss = evaluate(model, valid_loader, ...)            ‚îÇ
‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Forward pass (kh√¥ng Teacher Forcing)               ‚îÇ
‚îÇ      ‚îÇ                                                              ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ if valid_loss < best_loss:                                 ‚îÇ
‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ Save checkpoint (best_model.pth)                   ‚îÇ
‚îÇ      ‚îÇ                                                              ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ if no_improvement >= PATIENCE:                             ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ EARLY STOPPING                                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Hyperparameters (theo ƒê·ªí √ÅN):**

| Parameter | Value | M√¥ t·∫£ |
|-----------|-------|-------|
| `N_EPOCHS` | 20 | S·ªë epoch t·ªëi ƒëa |
| `LEARNING_RATE` | 0.001 | Learning rate (Adam) |
| `CLIP` | 1.0 | Gradient clipping |
| `TEACHER_FORCING` | 0.5 | T·ª∑ l·ªá Teacher Forcing |
| `PATIENCE` | 3 | Early Stopping patience |

---

## 4.a Training Configuration

Cell n√†y thi·∫øt l·∫≠p c√°c hyperparameters cho qu√° tr√¨nh hu·∫•n luy·ªán,
bao g·ªìm s·ªë epoch t·ªëi ƒëa, learning rate, teacher forcing ratio v√†
c√°c tham s·ªë li√™n quan ƒë·∫øn regularization.
"""

# ==============================================================================
# CELL 4a): C·∫§U H√åNH TRAINING V√Ä HELPER FUNCTIONS
# ==============================================================================

import time
import math

# =============================================================================
# HYPERPARAMETERS
# =============================================================================
N_EPOCHS = 20
CLIP = 1.0                       # Gradient clipping
LEARNING_RATE = 0.001            # Learning rate
PATIENCE = 3                     # Early Stopping patience
TEACHER_FORCING_RATIO = 0.5      # T·ª∑ l·ªá Teacher Forcing

# =============================================================================
# HELPER FUNCTION
# =============================================================================
def epoch_time(start_time, end_time):
    """T√≠nh th·ªùi gian ch·∫°y 1 epoch (ph√∫t, gi√¢y)."""
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

print("‚úÖ ƒê√£ c·∫•u h√¨nh hyperparameters:")
print(f"   N_EPOCHS = {N_EPOCHS}")
print(f"   LEARNING_RATE = {LEARNING_RATE}")
print(f"   CLIP = {CLIP}")
print(f"   TEACHER_FORCING_RATIO = {TEACHER_FORCING_RATIO}")
print(f"   PATIENCE = {PATIENCE}")

"""## 4.b H√†m Hu·∫•n luy·ªán v√† ƒê√°nh gi√° (Training & Evaluation Functions)

Trong cell n√†y, ch√∫ng t√¥i ƒë·ªãnh nghƒ©a **hai h√†m d√πng chung cho to√†n b·ªô qu√° tr√¨nh hu·∫•n luy·ªán**:

### `train()`
- Hu·∫•n luy·ªán m√¥ h√¨nh trong **1 epoch**
- C√≥ s·ª≠ d·ª•ng **Teacher Forcing**
- Th·ª±c hi·ªán:
  - Forward pass
  - T√≠nh loss (b·ªè `<sos>`, ignore `<pad>`)
  - Backpropagation
  - Gradient clipping
  - C·∫≠p nh·∫≠t tr·ªçng s·ªë

### `evaluate()`
- ƒê√°nh gi√° m√¥ h√¨nh tr√™n **validation/test set**
- **Kh√¥ng s·ª≠ d·ª•ng teacher forcing**
- Ch·ªâ d√πng forward pass ƒë·ªÉ ƒëo loss

üëâ Hai h√†m n√†y ƒë∆∞·ª£c **t√°i s·ª≠ d·ª•ng cho c·∫£ Baseline v√† Attention model**, gi√∫p tr√°nh l·∫∑p code v√† ƒë·∫£m b·∫£o so s√°nh c√¥ng b·∫±ng.

"""

# ==============================================================================
# CELL 4b): H√ÄM TRAIN V√Ä EVALUATE
# ==============================================================================

def train(model, iterator, optimizer, criterion, clip, device, teacher_forcing_ratio=0.5):
    """
    Hu·∫•n luy·ªán model trong 1 epoch.

    Args:
        model: M√¥ h√¨nh Seq2Seq (v·ªõi Attention)
        iterator: DataLoader train
        optimizer: Adam optimizer
        criterion: CrossEntropyLoss (v·ªõi ignore_index=PAD_IDX)
        clip: Gradient clipping value
        device: 'cuda' ho·∫∑c 'cpu'
        teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng Teacher Forcing

    Returns:
        epoch_loss: Loss trung b√¨nh c·ªßa epoch
    """
    model.train()
    epoch_loss = 0

    progress_bar = tqdm(iterator, desc="Training", leave=False)

    for batch in progress_bar:
        # ===== 1. UNPACK BATCH =====
        src, trg, src_len = batch

        # Chuy·ªÉn src, trg l√™n device
        src = src.to(device)         # [src_len, batch_size]
        trg = trg.to(device)         # [trg_len, batch_size]
        # ‚ö†Ô∏è src_len PH·∫¢I n·∫±m tr√™n CPU cho pack_padded_sequence!

        # ===== 2. FORWARD PASS =====
        optimizer.zero_grad()

        # Forward v·ªõi teacher_forcing_ratio
        output = model(src, src_len, trg, teacher_forcing_ratio)
        # output: [trg_len, batch_size, output_dim]

        # ===== 3. T√çNH LOSS =====
        # üìå LOGIC SLICING:
        # - output[0] l√† zeros tensor (do loop b·∫Øt ƒë·∫ßu t·ª´ t=1)
        # - trg[0] l√† <sos> token
        # - Ph·∫£i b·ªè c·∫£ hai tr∆∞·ªõc khi t√≠nh loss
        output_dim = output.shape[-1]

        output = output[1:]   # [trg_len-1, batch_size, output_dim]
        trg = trg[1:]         # [trg_len-1, batch_size]

        # Reshape v·ªÅ 2D cho CrossEntropyLoss
        output = output.reshape(-1, output_dim)  # [(trg_len-1)*batch_size, output_dim]
        trg = trg.reshape(-1)                    # [(trg_len-1)*batch_size]

        loss = criterion(output, trg)

        # ===== 4. BACKWARD PASS =====
        loss.backward()

        # Gradient clipping ƒë·ªÉ tr√°nh exploding gradient
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion, device):
    """
    ƒê√°nh gi√° model tr√™n t·∫≠p validation/test.

    Args:
        model: M√¥ h√¨nh Seq2Seq
        iterator: DataLoader val/test
        criterion: CrossEntropyLoss
        device: 'cuda' ho·∫∑c 'cpu'

    Returns:
        epoch_loss: Loss trung b√¨nh
    """
    model.eval()
    epoch_loss = 0

    with torch.no_grad():
        for batch in tqdm(iterator, desc="Evaluating", leave=False):
            src, trg, src_len = batch

            src = src.to(device)
            trg = trg.to(device)
            # src_len gi·ªØ nguy√™n tr√™n CPU

            # Forward v·ªõi teacher_forcing_ratio = 0 (kh√¥ng d√πng ground truth)
            output = model(src, src_len, trg, teacher_forcing_ratio=0)

            output_dim = output.shape[-1]

            output = output[1:]
            trg = trg[1:]

            output = output.reshape(-1, output_dim)
            trg = trg.reshape(-1)

            loss = criterion(output, trg)
            epoch_loss += loss.item()

    return epoch_loss / len(iterator)


print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a: train(), evaluate()")

"""## 4.1 Training Baseline Seq2Seq Model

Cell n√†y hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq baseline s·ª≠ d·ª•ng context vector c·ªë ƒë·ªãnh.
Qu√° tr√¨nh hu·∫•n luy·ªán √°p d·ª•ng early stopping d·ª±a tr√™n validation loss
ƒë·ªÉ tr√°nh overfitting.


**M·ª•c ti√™u:** Hu·∫•n luy·ªán m√¥ h√¨nh Baseline ƒë·ªÉ ch·ª©ng minh ho·∫°t ƒë·ªông c·ªßa context vector c·ªë ƒë·ªãnh.

"""

# ==============================================================================
# CELL 4.1: HU·∫§N LUY·ªÜN BASELINE SEQ2SEQ (ƒê√É T·ªêI ∆ØU)
# ==============================================================================

from tqdm.auto import tqdm
import time
import math

print("=" * 60)
print("üèóÔ∏è HU·∫§N LUY·ªÜN BASELINE SEQ2SEQ ")
print("=" * 60)
print("üìå M√¥ h√¨nh n√†y s·ª≠ d·ª•ng context vector C·ªê ƒê·ªäNH")
print("   (Ch·ªâ hidden state cu·ªëi c√πng c·ªßa Encoder)")
print("=" * 60 + "\n")

# =============================================================================
# C·∫§U H√åNH TRAINING CHO BASELINE
# =============================================================================
BASELINE_EPOCHS = 20
LEARNING_RATE = 0.001
CLIP = 1.0
TEACHER_FORCING_RATIO = 0.5
PATIENCE = 3

# Optimizer & Criterion cho Baseline
baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)
baseline_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# History tracking
baseline_history = {
    'train_loss': [],
    'valid_loss': [],
    'train_ppl': [],
    'valid_ppl': []
}

# Bi·∫øn Early Stopping
best_baseline_loss = float('inf')
epochs_without_improvement = 0

print(f"Epochs:           {BASELINE_EPOCHS}")
print(f"Learning Rate:    {LEARNING_RATE}")
print(f"Gradient Clip:    {CLIP}")
print(f"Teacher Forcing:  {TEACHER_FORCING_RATIO}")
print("=" * 60 + "\n")

# =============================================================================
# TRAINING LOOP CHO BASELINE (G·ªåI H√ÄM T·ª™ CELL 4B)
# =============================================================================

for epoch in range(BASELINE_EPOCHS):
    start_time = time.time()

    # ===== 1. TRAIN (D√πng h√†m chung, kh√¥ng vi·∫øt l·∫°i v√≤ng l·∫∑p) =====
    train_loss = train(
        baseline_model,
        train_loader,
        baseline_optimizer,
        baseline_criterion,
        CLIP,
        device,
        TEACHER_FORCING_RATIO
    )

    # ===== 2. EVALUATE (D√πng h√†m chung) =====
    valid_loss = evaluate(
        baseline_model,
        valid_loader,
        baseline_criterion,
        device
    )

    end_time = time.time()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    # ===== 3. T√çNH PPL & L∆ØU HISTORY =====
    train_ppl = math.exp(train_loss)
    valid_ppl = math.exp(valid_loss)

    baseline_history['train_loss'].append(train_loss)
    baseline_history['valid_loss'].append(valid_loss)
    baseline_history['train_ppl'].append(train_ppl)
    baseline_history['valid_ppl'].append(valid_ppl)

    # ===== 4. CHECKPOINT & EARLY STOPPING =====
    BASELINE_CKPT = '/content/drive/MyDrive/NMT_EN_FR_Final1/baseline_model.pth'

    if valid_loss < best_baseline_loss:
        best_baseline_loss = valid_loss
        epochs_without_improvement = 0 # Reset counter
        torch.save(baseline_model.state_dict(), BASELINE_CKPT)
        save_status = "‚úÖ Model saved!"
    else:
        epochs_without_improvement += 1
        save_status = f"‚ö†Ô∏è No improvement ({epochs_without_improvement}/{PATIENCE})"

    print(f"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s")
    print(f"\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}")
    print(f"\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f} {save_status}")
    print("-" * 60)

    # Logic Early Stopping
    if epochs_without_improvement >= PATIENCE:
        print("\n" + "=" * 60)
        print(f"‚õî EARLY STOPPING: Val loss kh√¥ng gi·∫£m sau {PATIENCE} epochs")
        print("=" * 60)
        break

print("\n" + "=" * 60)
print("üéâ BASELINE TRAINING HO√ÄN T·∫§T!")
print("=" * 60)

"""## 4.2 HU·∫§N LUY·ªÜN ATTENTION SEQ2SEQ

---

**M·ª•c ti√™u:** Hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq + Luong Attention v·ªõi Early Stopping.

**So s√°nh v·ªõi Baseline:**
- Baseline: Context vector C·ªê ƒê·ªäNH (ch·ªâ hidden state cu·ªëi)
- Attention: Context vector ƒê·ªòNG (weighted sum c·ªßa t·∫•t c·∫£ encoder outputs)

---
"""

# ==============================================================================
# CELL 4.2: KH·ªûI T·∫†O OPTIMIZER & CRITERION CHO ATTENTION MODEL
# ==============================================================================

attention_optimizer = torch.optim.Adam(attention_model.parameters(), lr=LEARNING_RATE)

# KH·ªûI T·∫†O SCHEDULER
# N·∫øu val_loss kh√¥ng gi·∫£m sau 1 epoch (patience=1), gi·∫£m LR ƒëi 10 l·∫ßn (factor=0.5)
attention_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    attention_optimizer, mode='min', factor=0.5, patience=1, verbose=True
)


# Loss function: CrossEntropyLoss v·ªõi ignore_index ƒë·ªÉ b·ªè qua PAD token
attention_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# C·∫•u h√¨nh training
N_EPOCHS = 20                    # S·ªë epoch t·ªëi ƒëa
PATIENCE = 3                     # Early Stopping patience

print("=" * 60)
print("üéØ C·∫§U H√åNH HU·∫§N LUY·ªÜN ATTENTION MODEL")
print("=" * 60)
print(f"Device:              {device}")
print(f"Model:               {attention_model.__class__.__name__}")
print(f"Total Parameters:    {attention_params:,}")
print("-" * 60)
print(f"Optimizer:           Adam (lr={LEARNING_RATE})")
print(f"Loss:                CrossEntropyLoss (ignore_index={PAD_IDX})")
print(f"Epochs:              {N_EPOCHS} (max)")
print(f"Gradient Clip:       {CLIP}")
print(f"Teacher Forcing:     {TEACHER_FORCING_RATIO}")
print(f"Early Stopping:      patience={PATIENCE}")
print(f"Batch Size:          {BATCH_SIZE}")
print("=" * 60)

"""## 4.3 Training Seq2Seq Model with Luong Attention

Cell n√†y hu·∫•n luy·ªán m√¥ h√¨nh Seq2Seq v·ªõi c∆° ch·∫ø Luong Attention.
So v·ªõi baseline, m√¥ h√¨nh attention s·ª≠ d·ª•ng dynamic context vector
v√† c√≥ s·ªë l∆∞·ª£ng tham s·ªë l·ªõn h∆°n. Early stopping ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ
l∆∞u checkpoint t·ªët nh·∫•t.

"""

# ==============================================================================
# CELL 4.3: V√íNG L·∫∂P HU·∫§N LUY·ªÜN ATTENTION MODEL (V·ªöI SCHEDULER & EARLY STOPPING)
# ==============================================================================

# Bi·∫øn theo d√µi
best_valid_loss = float('inf')
epochs_without_improvement = 0
attention_history = {
    'train_loss': [],
    'valid_loss': [],
    'train_ppl': [],
    'valid_ppl': []
}

print("=" * 60)
print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN ATTENTION MODEL")
print("=" * 60 + "\n")

for epoch in range(N_EPOCHS):
    start_time = time.time()

    # ===== 1. TRAIN (D√πng h√†m t·ª´ Cell 4b) =====
    train_loss = train(attention_model, train_loader, attention_optimizer, attention_criterion, CLIP, device, TEACHER_FORCING_RATIO)

    # ===== 2. EVALUATE (D√πng h√†m t·ª´ Cell 4b) =====
    valid_loss = evaluate(attention_model, valid_loader, attention_criterion, device)

    # 3. C·∫¨P NH·∫¨T SCHEDULER SAU M·ªñI EPOCH
    attention_scheduler.step(valid_loss)

    end_time = time.time()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    # ===== 4. T√çNH PERPLEXITY & L∆ØU HISTORY =====
    train_ppl = math.exp(train_loss)
    valid_ppl = math.exp(valid_loss)

    attention_history['train_loss'].append(train_loss)
    attention_history['valid_loss'].append(valid_loss)
    attention_history['train_ppl'].append(train_ppl)
    attention_history['valid_ppl'].append(valid_ppl)

    # ===== 5. CHECKPOINT & EARLY STOPPING =====
    ATTENTION_CKPT = '/content/drive/MyDrive/NMT_EN_FR_Final1/attention_best_model.pth'

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        epochs_without_improvement = 0
        torch.save(attention_model.state_dict(), ATTENTION_CKPT)
        save_status = "‚úÖ Model saved!"
    else:
        epochs_without_improvement += 1
        save_status = f"‚ö†Ô∏è No improvement ({epochs_without_improvement}/{PATIENCE})"

    # ===== 6. IN K·∫æT QU·∫¢ =====
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')
    print(f'\t{save_status}')
    print("-" * 60)

    if epochs_without_improvement >= PATIENCE:
        print("\n" + "=" * 60)
        print(f"‚õî EARLY STOPPING: Val loss kh√¥ng gi·∫£m sau {PATIENCE} epochs")
        print("=" * 60)
        break

print("\n" + "=" * 60)
print("üéâ ATTENTION TRAINING HO√ÄN T·∫§T!")
print(f"Best Validation PPL:  {math.exp(best_valid_loss):.3f}")
print("=" * 60)

"""## 4.4 ƒê√°nh gi√° cu·ªëi c√πng tr√™n t·∫≠p Test (Baseline vs Attention)

Sau khi hu·∫•n luy·ªán xong c·∫£ hai m√¥ h√¨nh, ch√∫ng t√¥i ti·∫øn h√†nh:

- Load **checkpoint t·ªët nh·∫•t** c·ªßa Baseline v√† Attention
- ƒê√°nh gi√° tr√™n **t·∫≠p Test (ch∆∞a t·ª´ng th·∫•y khi training)**
- So s√°nh:
  - Test Loss
  - Test Perplexity (PPL)

M·ª•c ti√™u:
- Ki·ªÉm tra kh·∫£ nƒÉng **generalization**
- ƒê√°nh gi√° m·ª©c ƒë·ªô c·∫£i thi·ªán khi s·ª≠ d·ª•ng Attention

"""

# ==============================================================================
# CELL 4.4: SO S√ÅNH BASELINE VS ATTENTION (TEST LOSS & PPL)
# ==============================================================================

print("=" * 60)
print("üìä SO S√ÅNH HI·ªÜU SU·∫§T: BASELINE VS ATTENTION")
print("=" * 60)

# ƒê∆∞·ªùng d·∫´n checkpoint (ƒë√£ mount Drive ·ªü ƒë·∫ßu notebook)
baseline_path = f"{BASE_DIR}/baseline_model.pth"
attention_path = f"{BASE_DIR}/attention_best_model.pth"

# Load model (KH√îNG weights_only)
baseline_model.load_state_dict(torch.load(baseline_path, map_location=device))
attention_model.load_state_dict(torch.load(attention_path, map_location=device))

baseline_model.eval()
attention_model.eval()

# D√πng CHUNG criterion
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# Evaluate
baseline_test_loss = evaluate(baseline_model, test_loader, criterion, device)
attention_test_loss = evaluate(attention_model, test_loader, criterion, device)

baseline_test_ppl = math.exp(baseline_test_loss)
attention_test_ppl = math.exp(attention_test_loss)

# In k·∫øt qu·∫£
print("\n" + "-" * 60)
print(f"{'Model':<30} | {'Test Loss':<10} | {'Test PPL':<10}")
print("-" * 60)
print(f"{'Baseline (Fixed Context)':<30} | {baseline_test_loss:>10.3f} | {baseline_test_ppl:>10.3f}")
print(f"{'Attention (Dynamic Context)':<30} | {attention_test_loss:>10.3f} | {attention_test_ppl:>10.3f}")
print("-" * 60)

print(f"\nüìà C·∫£i thi·ªán khi d√πng Attention:")
print(f"   - Loss gi·∫£m: {baseline_test_loss - attention_test_loss:.3f}")
print(f"   - PPL gi·∫£m:  {baseline_test_ppl - attention_test_ppl:.3f}")
print("=" * 60)

"""# üìä PH·∫¶N 5: INFERENCE & BLEU EVALUATION

---

## Quy tr√¨nh Inference

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        GREEDY DECODING                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Input (EN): "A man walks"                                          ‚îÇ
‚îÇ       ‚Üì                                                             ‚îÇ
‚îÇ  Tokenize + Numericalize                                            ‚îÇ
‚îÇ       ‚Üì                                                             ‚îÇ
‚îÇ  Encoder ‚Üí (encoder_outputs, hidden, cell)                          ‚îÇ
‚îÇ       ‚Üì                                                             ‚îÇ
‚îÇ  Loop until <eos> or MAX_LEN:                                       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ Decoder(input, hidden, cell, encoder_outputs, mask)       ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ argmax(output) ‚Üí predicted token                          ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ Append to result                                          ‚îÇ
‚îÇ       ‚Üì                                                             ‚îÇ
‚îÇ  Output (FR): "Un homme marche"                                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**BLEU Score Evaluation:**
- S·ª≠ d·ª•ng `nltk.translate.bleu_score`
- Smoothing function ƒë·ªÉ x·ª≠ l√Ω n-gram = 0
- ƒê√°nh gi√° tr√™n to√†n b·ªô t·∫≠p Test

---

## 5.1 C∆° ch·∫ø suy lu·∫≠n (Inference Pipeline)

ƒê·ªÉ d·ªãch m·ªôt c√¢u m·ªõi t·ª´ ti·∫øng Anh sang ti·∫øng Ph√°p, m√¥ h√¨nh th·ª±c hi·ªán quy tr√¨nh **Greedy Decoding**:

1.  **Tokenization:** Chuy·ªÉn c√¢u ti·∫øng Anh th√†nh chu·ªói token s·ªë.
2.  **Encoding:** Encoder ƒë·ªçc chu·ªói s·ªë v√† t·∫°o ra context (hidden states).
3.  **Decoding:**
    * B·∫Øt ƒë·∫ßu v·ªõi token `<sos>`.
    * T·∫°i m·ªói b∆∞·ªõc th·ªùi gian, Decoder d·ª± ƒëo√°n t·ª´ ti·∫øp theo c√≥ x√°c su·∫•t cao nh·∫•t (`argmax`).
    * T·ª´ d·ª± ƒëo√°n n√†y ƒë∆∞·ª£c d√πng l√†m ƒë·∫ßu v√†o cho b∆∞·ªõc ti·∫øp theo.
    * L·∫∑p l·∫°i cho ƒë·∫øn khi g·∫∑p token `<eos>` ho·∫∑c ƒë·∫°t ƒë·ªô d√†i t·ªëi ƒëa.

D∆∞·ªõi ƒë√¢y, ch√∫ng t√¥i ƒë·ªãnh nghƒ©a h√†m `translate_model` chung cho c·∫£ hai ki·∫øn tr√∫c, v√† c√°c h√†m wrapper `translate_baseline`, `translate_attention` ƒë·ªÉ ti·ªán s·ª≠ d·ª•ng.
"""

# ==============================================================================
# CELL 5.1: H√ÄM TRANSLATE (INFERENCE)
# ==============================================================================

def translate_model(sentence, model, device, max_len=50, is_attention=True):
    """
    H√†m d·ªãch chung cho c·∫£ 2 lo·∫°i model.
    Args:
        is_attention (bool): True n·∫øu d√πng Attention Model, False n·∫øu d√πng Baseline.
    """
    model.eval()

    # 1. Tokenize & Numericalize
    if isinstance(sentence, str):
        tokens = tokenizer_en(sentence.lower())
    else:
        tokens = [token.lower() for token in sentence]

    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]

    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)

    with torch.no_grad():
        # 2. Encoder
        if is_attention:
            encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)
            mask = model.create_mask(src_tensor)
        else:
            hidden, cell = model.encoder(src_tensor, src_len)

        # 3. Decoder Loop
        trg_indexes = [SOS_IDX]

        for _ in range(max_len):
            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)

            if is_attention:
                output, hidden, cell, _ = model.decoder(trg_tensor, hidden, cell, encoder_outputs, mask)
            else:
                output, hidden, cell = model.decoder(trg_tensor, hidden, cell)

            pred_token = output.argmax(1).item()
            trg_indexes.append(pred_token)

            if pred_token == EOS_IDX:
                break

    # 4. Convert to tokens
    trg_tokens = [vocab_fr.get_itos()[i] for i in trg_indexes]

    # Remove <sos> and <eos>
    if trg_tokens and trg_tokens[0] == '<sos>': trg_tokens.pop(0)
    if trg_tokens and trg_tokens[-1] == '<eos>': trg_tokens.pop()

    return ' '.join(trg_tokens)

# Wrappers ƒë·ªÉ ti·ªán s·ª≠ d·ª•ng
def translate_attention(sentence):
    return translate_model(sentence, attention_model, device, is_attention=True)

def translate_baseline(sentence):
    return translate_model(sentence, baseline_model, device, is_attention=False)

# Y√™u c·∫ßu b·∫Øt bu·ªôc c·ªßa ƒë·ªì √°n: h√†m translate(sentence)
translate = translate_attention

print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m translate(), translate_baseline(), translate_attention()")

"""## 5.2 ƒê√°nh gi√° ƒë·ªãnh l∆∞·ª£ng: BLEU Score

**BLEU (Bilingual Evaluation Understudy)** l√† ti√™u chu·∫©n v√†ng ƒë·ªÉ ƒë√°nh gi√° h·ªá th·ªëng d·ªãch m√°y. N√≥ ƒëo l∆∞·ªùng ƒë·ªô tr√πng kh·ªõp c·ªßa c√°c n-gram (1-gram, 2-gram, ..., 4-gram) gi·ªØa c√¢u m√°y d·ªãch v√† c√¢u tham chi·∫øu (c·ªßa con ng∆∞·ªùi).

* **Th∆∞ vi·ªán:** S·ª≠ d·ª•ng `nltk.translate.bleu_score`.
* **Smoothing:** √Åp d·ª•ng `SmoothingFunction` ƒë·ªÉ tr√°nh l·ªói t√≠nh to√°n khi kh√¥ng c√≥ n-gram n√†o tr√πng kh·ªõp (ƒë·∫∑c bi·ªát v·ªõi c√¢u ng·∫Øn).
* **Ph·∫°m vi:** T√≠nh tr√™n t·∫≠p **Test Set** (d·ªØ li·ªáu m√¥ h√¨nh ch∆∞a t·ª´ng th·∫•y).
"""

# ==============================================================================
# CELL 5.2: T√çNH BLEU SCORE (NLTK)
# ==============================================================================

from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

def calculate_bleu_score(model_fn, test_src, test_trg, num_samples=200):
    smooth = SmoothingFunction().method1

    references = []
    hypotheses = []

    indices = random.sample(range(len(test_src)), min(num_samples, len(test_src)))
    print(f"üìä ƒêang t√≠nh BLEU tr√™n {len(indices)} m·∫´u...")

    for idx in tqdm(indices, desc="Calculating BLEU", leave=False):
        src = test_src[idx]
        trg = test_trg[idx]

        try:
            pred = model_fn(src)

            hyp_tokens = tokenizer_fr(pred.lower())
            ref_tokens = [tokenizer_fr(trg.lower())]

            hypotheses.append(hyp_tokens)
            references.append(ref_tokens)
        except Exception:
            continue

    bleu = corpus_bleu(references, hypotheses, smoothing_function=smooth)
    return bleu

"""## 5.3 ƒê√°nh gi√° ƒë·ªãnh t√≠nh: So s√°nh th·ª±c t·∫ø (Qualitative Analysis)

Tr∆∞·ªõc khi nh√¨n v√†o c√°c con s·ªë kh√¥ khan, ch√∫ng ta h√£y xem x√©t tr·ª±c quan ch·∫•t l∆∞·ª£ng b·∫£n d·ªãch. Cell b√™n d∆∞·ªõi s·∫Ω ch·ªçn ng·∫´u nhi√™n 5 c√¢u t·ª´ t·∫≠p Test v√† hi·ªÉn th·ªã k·∫øt qu·∫£ c·ªßa c·∫£ hai m√¥ h√¨nh.

**M·ª•c ti√™u quan s√°t:**
* M√¥ h√¨nh Attention c√≥ d·ªãch ƒë∆∞·ª£c c√°c t·ª´ ·ªü cu·ªëi c√¢u d√†i t·ªët h∆°n Baseline kh√¥ng?
* C·∫•u tr√∫c ng·ªØ ph√°p ti·∫øng Ph√°p c√≥ ch√≠nh x√°c kh√¥ng?
* C√°ch x·ª≠ l√Ω c√°c t·ª´ hi·∫øm (unknown tokens).
"""

# ==============================================================================
# CELL 5.3: DEMO SO S√ÅNH BASELINE VS ATTENTION
# ==============================================================================

print("=" * 70)
print("üìä DEMO SO S√ÅNH: BASELINE VS ATTENTION")
print("=" * 70)

# ƒê·∫£m b·∫£o model ƒëang ·ªü ch·∫ø ƒë·ªô evaluation
baseline_model.eval()
attention_model.eval()

# Ch·ªçn ng·∫´u nhi√™n 5 c√¢u trong t·∫≠p test
num_samples = 5
indices = random.sample(range(len(test_en)), num_samples)

for i, idx in enumerate(indices, 1):
    src_sentence = test_en[idx]
    ref_sentence = test_fr[idx]

    # D·ªãch b·∫±ng Baseline v√† Attention
    pred_baseline = translate_baseline(src_sentence)
    pred_attention = translate_attention(src_sentence)

    # In k·∫øt qu·∫£
    print(f"\n--- V√≠ d·ª• {i} ---")
    print(f"üì• Source (EN):")
    print(f"    {src_sentence}")

    print(f"üìå Reference (FR):")
    print(f"    {ref_sentence}")

    print(f"üèóÔ∏è Baseline Translation (FR):")
    print(f"    {pred_baseline}")

    print(f"üéØ Attention Translation (FR):")
    print(f"    {pred_attention}")

    print("-" * 70)

print("\n" + "=" * 70)
print("‚úÖ K·∫æT TH√öC DEMO SO S√ÅNH BASELINE VS ATTENTION")
print("=" * 70)

"""## 5.4 T·ªïng k·∫øt hi·ªáu su·∫•t: Baseline vs. Attention

ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t ƒë·ªÉ k·∫øt lu·∫≠n hi·ªáu qu·∫£ c·ªßa ƒë·ªì √°n. Ch√∫ng t√¥i so s√°nh hai m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p Test d·ª±a tr√™n hai ch·ªâ s·ªë:
1.  **Test Loss / Perplexity (PPL):** ƒêo l∆∞·ªùng ƒë·ªô "b·ªëi r·ªëi" c·ªßa m√¥ h√¨nh. C√†ng th·∫•p c√†ng t·ªët.
2.  **BLEU Score:** ƒêo l∆∞·ªùng ƒë·ªô ch√≠nh x√°c c·ªßa b·∫£n d·ªãch. C√†ng cao c√†ng t·ªët.

> **K·ª≥ v·ªçng:** M√¥ h√¨nh Attention s·∫Ω c√≥ BLEU Score cao h∆°n v√† PPL th·∫•p h∆°n ƒë√°ng k·ªÉ so v·ªõi Baseline nh·ªù kh·∫£ nƒÉng truy c·∫≠p context linh ho·∫°t.
"""

# ==============================================================================
# CELL 5.4: ƒê√ÅNH GI√Å BLEU SCORE - BASELINE VS ATTENTION
# ==============================================================================

print("=" * 70)
print("üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å BLEU SCORE")
print("=" * 70)

print("Checking Baseline Model...")
bleu_baseline = calculate_bleu_score(
    translate_baseline, test_en, test_fr, num_samples=200
)

print("Checking Attention Model...")
bleu_attention = calculate_bleu_score(
    translate_attention, test_en, test_fr, num_samples=200
)

print("\n" + "-" * 70)
print(f"{'Model':<30} | {'BLEU Score':<15}")
print("-" * 70)
print(f"{'Baseline (Fixed Context)':<30} | {bleu_baseline*100:>10.2f}")
print(f"{'Attention (Dynamic Context)':<30} | {bleu_attention*100:>10.2f}")
print("-" * 70)

improvement = (bleu_attention - bleu_baseline) * 100
print(f"üìà C·∫£i thi·ªán: +{improvement:.2f}")
print("=" * 70)


# B·∫£ng ƒë√°nh gi√° (ƒê√£ ƒëi·ªÅu ch·ªânh cho quy m√¥ ƒê·ªì √°n / Dataset nh·ªè)
print("""
üìä H∆Ø·ªöNG D·∫™N ƒê√ÅNH GI√Å BLEU SCORE (Context: Multi30K / LSTM Model):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BLEU Score      ‚îÇ ƒê√°nh gi√° (Quy m√¥ ƒë·ªì √°n sinh vi√™n)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ < 10%           ‚îÇ C·∫ßn c·∫£i thi·ªán - Model ch∆∞a h·ªôi t·ª• t·ªët        ‚îÇ
‚îÇ 10% - 19%       ‚îÇ ƒê·∫°t - Model ho·∫°t ƒë·ªông ƒë√∫ng logic c∆° b·∫£n      ‚îÇ
‚îÇ 20% - 29%       ‚îÇ Kh√°/T·ªët - Ch·∫•t l∆∞·ª£ng d·ªãch ch·∫•p nh·∫≠n ƒë∆∞·ª£c     ‚îÇ
‚îÇ 30% - 40%       ‚îÇ Xu·∫•t s·∫Øc - (Th∆∞·ªùng c·∫ßn Transformer)          ‚îÇ
‚îÇ > 40%           ‚îÇ SOTA - Ch·∫•t l∆∞·ª£ng th∆∞∆°ng m·∫°i                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


""")

"""## 5.5 Th·ª≠ nghi·ªám t·ª± do

T·∫°i ƒë√¢y, ch√∫ng ta c√≥ th·ªÉ nh·∫≠p b·∫•t k·ª≥ c√¢u ti·∫øng Anh n√†o (ngo√†i t·∫≠p d·ªØ li·ªáu) ƒë·ªÉ ki·ªÉm tra kh·∫£ nƒÉng t·ªïng qu√°t h√≥a c·ªßa m√¥ h√¨nh Attention.
"""

# ==============================================================================
# CELL 5.5: D·ªäCH C√ÇU T√ôY √ù
# ==============================================================================

print("=" * 70)
print("üñäÔ∏è D·ªäCH C√ÇU T√ôY √ù")
print("=" * 70)

test_sentences = [
    "I love machine learning.",
    "The weather is beautiful today.",
    "A man is walking with his dog.",
    "Two children are playing in the park.",
    "A woman is reading a book.",
]

for sentence in test_sentences:
    result = translate(sentence)
    print(f"\nüì• EN: {sentence}")
    print(f"üá´üá∑ FR: {result}")

print("\n" + "=" * 70)
print("\n‚úÖ PH·∫¶N 5 HO√ÄN T·∫§T - S·∫µn s√†ng cho PH·∫¶N 6 (Analysis)!")

"""# üìä PH·∫¶N 6: C√ÅC H√ÄM B·ªî SUNG CHO B√ÅO C√ÅO

---

## C√°c c√¥ng c·ª• ph√¢n t√≠ch n√¢ng cao

**Bao g·ªìm:**
1. `plot_history()` - V·∫Ω bi·ªÉu ƒë·ªì Loss v·ªõi highlight epoch t·ªët nh·∫•t
2. `analyze_model_performance()` - Ph√¢n t√≠ch l·ªói v·ªõi BLEU score
3. `translate_beam_search()` - Beam Search decoding

---

## 6.1 Tr·ª±c quan h√≥a qu√° tr√¨nh hu·∫•n luy·ªán

H√†m `plot_history` s·∫Ω v·∫Ω bi·ªÉu ƒë·ªì **Loss** theo t·ª´ng Epoch.
* **Tr·ª•c tung:** Gi√° tr·ªã Loss.
* **Tr·ª•c ho√†nh:** S·ªë Epoch.
* **ƒêi·ªÉm xanh l√°:** ƒê√°nh d·∫•u Epoch m√† t·∫°i ƒë√≥ m√¥ h√¨nh ƒë·∫°t hi·ªáu su·∫•t t·ªët nh·∫•t (Best Validation Loss).

Bi·ªÉu ƒë·ªì n√†y gi√∫p nh·∫≠n di·ªán hi·ªán t∆∞·ª£ng **Overfitting** (khi Train Loss gi·∫£m nh∆∞ng Val Loss tƒÉng) ho·∫∑c **Underfitting**.
"""

# ==============================================================================
# CELL 6.1: H√ÄM V·∫º BI·ªÇU ƒê·ªí LOSS
# ==============================================================================

import matplotlib.pyplot as plt
import os

def plot_history(history, title="Training History", filename="training_history.png"):
    """
    V·∫Ω bi·ªÉu ƒë·ªì Train Loss v√† Valid Loss t·ª´ history.

    Args:
        history: Dict ch·ª©a 'train_loss' v√† 'valid_loss' (list)
        title: Ti√™u ƒë·ªÅ bi·ªÉu ƒë·ªì
        filename: T√™n file PNG ƒë·ªÉ l∆∞u

    Returns:
        None (l∆∞u file PNG)
    """
    plt.figure(figsize=(10, 6))

    epochs = range(1, len(history['train_loss']) + 1)

    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)
    plt.plot(epochs, history['valid_loss'], 'r-s', label='Valid Loss', linewidth=2, markersize=6)

    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(epochs)

    # Highlight best epoch
    best_epoch = history['valid_loss'].index(min(history['valid_loss'])) + 1
    best_loss = min(history['valid_loss'])
    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best: Epoch {best_epoch}')
    plt.scatter([best_epoch], [best_loss], color='green', s=100, zorder=5, marker='*')

    plt.tight_layout()

    # Construct the full path using BASE_DIR
    save_path = os.path.join(BASE_DIR, filename)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()

    print(f"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {save_path}")
    print(f"   Best Epoch: {best_epoch} | Best Valid Loss: {best_loss:.4f}")

print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m plot_history()")

"""## 6.2 Ph√¢n t√≠ch l·ªói (Error Analysis)

Ch·ªâ nh√¨n v√†o ƒëi·ªÉm BLEU trung b√¨nh l√† ch∆∞a ƒë·ªß. H√†m `analyze_model_performance` s·∫Ω gi√∫p ch√∫ng ta "m·ªï x·∫ª" k·∫øt qu·∫£:
* **Best Cases:** Nh·ªØng c√¢u m√¥ h√¨nh d·ªãch t·ªët nh·∫•t (BLEU cao). Th∆∞·ªùng l√† c√¢u ng·∫Øn, c·∫•u tr√∫c ƒë∆°n gi·∫£n.
* **Worst Cases:** Nh·ªØng c√¢u m√¥ h√¨nh d·ªãch t·ªá nh·∫•t (BLEU th·∫•p). Gi√∫p nh·∫≠n di·ªán ƒëi·ªÉm y·∫øu c·ªßa m√¥ h√¨nh (v√≠ d·ª•: c√¢u qu√° d√†i, nhi·ªÅu t·ª´ v·ª±ng l·∫°, c·∫•u tr√∫c ƒë·∫£o ng·ªØ).
"""

# ==============================================================================
# CELL 6.2: H√ÄM PH√ÇN T√çCH HI·ªÜU SU·∫§T M√î H√åNH
# ==============================================================================

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def analyze_model_performance(translate_fn, src_sentences, trg_sentences, num_examples=5):
    """
    Ph√¢n t√≠ch hi·ªáu su·∫•t m√¥ h√¨nh: t√¨m c√¢u BLEU cao nh·∫•t v√† th·∫•p nh·∫•t.

    Args:
        translate_fn: H√†m d·ªãch (translate_attention ho·∫∑c translate_baseline)
        src_sentences: List c√¢u ngu·ªìn (ti·∫øng Anh)
        trg_sentences: List c√¢u ƒë√≠ch ground truth (ti·∫øng Ph√°p)
        num_examples: S·ªë c√¢u hi·ªÉn th·ªã cho m·ªói nh√≥m (cao/th·∫•p)

    Returns:
        Dict ch·ª©a 'best' v√† 'worst' examples
    """
    smooth = SmoothingFunction().method1
    results = []

    print("=" * 80)
    print("üìä PH√ÇN T√çCH HI·ªÜU SU·∫§T M√î H√åNH")
    print("=" * 80)
    print(f"ƒêang ƒë√°nh gi√° {len(src_sentences)} c√¢u...")

    for idx in tqdm(range(len(src_sentences)), desc="Analyzing"):
        src = src_sentences[idx]
        trg = trg_sentences[idx]
        pred = translate_fn(src)

        # T√≠nh BLEU
        pred_tokens = pred.split()
        ref_tokens = tokenizer_fr(trg.lower())

        try:
            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
        except:
            bleu = 0.0

        results.append({
            'idx': idx,
            'src': src,
            'trg': trg,
            'pred': pred,
            'bleu': bleu
        })

    # S·∫Øp x·∫øp theo BLEU
    results_sorted = sorted(results, key=lambda x: x['bleu'], reverse=True)

    best_examples = results_sorted[:num_examples]
    worst_examples = results_sorted[-num_examples:]

    # T√≠nh th·ªëng k√™
    bleu_scores = [r['bleu'] for r in results]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)

    print("\n" + "-" * 80)
    print(f"üìà TH·ªêNG K√ä T·ªîNG QUAN")
    print("-" * 80)
    print(f"   T·ªïng s·ªë c√¢u:     {len(results)}")
    print(f"   BLEU trung b√¨nh: {avg_bleu * 100:.2f}%")
    print(f"   BLEU cao nh·∫•t:   {max(bleu_scores) * 100:.2f}%")
    print(f"   BLEU th·∫•p nh·∫•t:  {min(bleu_scores) * 100:.2f}%")

    # In c√¢u BLEU cao nh·∫•t
    print("\n" + "=" * 80)
    print(f"üèÜ TOP {num_examples} C√ÇU BLEU CAO NH·∫§T")
    print("=" * 80)

    for i, ex in enumerate(best_examples, 1):
        print(f"\n--- Rank {i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")

    # In c√¢u BLEU th·∫•p nh·∫•t
    print("\n" + "=" * 80)
    print(f"‚ö†Ô∏è TOP {num_examples} C√ÇU BLEU TH·∫§P NH·∫§T")
    print("=" * 80)

    for i, ex in enumerate(worst_examples, 1):
        print(f"\n--- Rank {len(results) - num_examples + i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")

    print("\n" + "=" * 80)

    return {
        'avg_bleu': avg_bleu,
        'best': best_examples,
        'worst': worst_examples,
        'all_results': results
    }

print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m analyze_model_performance()")

"""## 6.3 K·ªπ thu·∫≠t n√¢ng cao: Beam Search Decoding (Bonus)

M·∫∑c ƒë·ªãnh, ch√∫ng ta d√πng **Greedy Decoding** (ch·ªçn t·ª´ t·ªët nh·∫•t ·ªü t·ª´ng b∆∞·ªõc). Tuy nhi√™n, c√°ch n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn k·∫øt qu·∫£ c·ª•c b·ªô kh√¥ng t·ªëi ∆∞u.

**Beam Search** l√† m·ªôt chi·∫øn l∆∞·ª£c t√¨m ki·∫øm t·ªët h∆°n:
* Thay v√¨ ch·ªçn 1 t·ª´, n√≥ gi·ªØ l·∫°i **k** ·ª©ng c·ª≠ vi√™n t·ªët nh·∫•t (k = beam_size) t·∫°i m·ªói b∆∞·ªõc.
* N√≥ kh√°m ph√° nhi·ªÅu nh√°nh d·ªãch song song v√† ch·ªçn ra chu·ªói c√≥ t·ªïng x√°c su·∫•t cao nh·∫•t cu·ªëi c√πng.

Cell b√™n d∆∞·ªõi c√†i ƒë·∫∑t gi·∫£i thu·∫≠t Beam Search ƒë·ªÉ so s√°nh v·ªõi Greedy Decoding.
"""

# ==============================================================================
# CELL 6.3: H√ÄM BEAM SEARCH DECODING (ƒêI·ªÇM C·ªòNG)
# ==============================================================================

def translate_beam_search(sentence, model, beam_size=3, max_len=50):
    """
    D·ªãch c√¢u s·ª≠ d·ª•ng Beam Search Decoding (cho Attention Model).

    Args:
        sentence: C√¢u ti·∫øng Anh c·∫ßn d·ªãch
        model: Attention model (Seq2SeqAttention)
        beam_size: S·ªë beam gi·ªØ l·∫°i m·ªói b∆∞·ªõc
        max_len: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa c√¢u d·ªãch

    Returns:
        str: C√¢u ti·∫øng Ph√°p ƒë∆∞·ª£c d·ªãch
    """
    model.eval()

    # Tokenize v√† numericalize
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)

    # Encode
    with torch.no_grad():
        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)

    # Mask
    mask = (src_tensor != PAD_IDX).permute(1, 0)

    # Beam Search initialization
    beams = [(0.0, [SOS_IDX], hidden, cell)]
    completed = []

    for _ in range(max_len):
        new_beams = []

        for score, tokens, h, c in beams:
            if tokens[-1] == EOS_IDX:
                completed.append((score, tokens))
                continue

            trg_tensor = torch.LongTensor([tokens[-1]]).to(device)

            with torch.no_grad():
                output, new_h, new_c, _ = model.decoder(trg_tensor, h, c, encoder_outputs, mask)

            # Log probabilities
            log_probs = F.log_softmax(output, dim=1)

            # Top-k candidates
            topk_log_probs, topk_indices = log_probs.topk(beam_size)

            for i in range(beam_size):
                new_score = score + topk_log_probs[0, i].item()
                new_token = topk_indices[0, i].item()
                new_tokens = tokens + [new_token]
                new_beams.append((new_score, new_tokens, new_h, new_c))

        # Keep top beams
        new_beams.sort(key=lambda x: x[0], reverse=True)
        beams = new_beams[:beam_size]

        if len(beams) == 0:
            break

    # Add remaining beams to completed
    for score, tokens, h, c in beams:
        completed.append((score, tokens))

    # Ch·ªçn beam c√≥ score cao nh·∫•t (normalize by length)
    if completed:
        completed.sort(key=lambda x: x[0] / len(x[1]), reverse=True)
        best_tokens = completed[0][1]
    else:
        best_tokens = [SOS_IDX]

    # Convert to words
    trg_tokens = [vocab_fr.get_itos()[i] for i in best_tokens]

    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]

    return ' '.join(trg_tokens)

print("‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m translate_beam_search()")

"""## 6.4 T·ªïng h·ª£p k·∫øt qu·∫£ ph√¢n t√≠ch

Cell cu·ªëi c√πng n√†y s·∫Ω ch·∫°y to√†n b·ªô c√°c h√†m ph√¢n t√≠ch ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n:
1.  V·∫Ω bi·ªÉu ƒë·ªì Loss (n·∫øu c√≥ l·ªãch s·ª≠ hu·∫•n luy·ªán).
2.  Ph√¢n t√≠ch 100 c√¢u m·∫´u ƒë·ªÉ t√¨m ra c√¢u d·ªãch t·ªët nh·∫•t/t·ªá nh·∫•t.
3.  So s√°nh tr·ª±c ti·∫øp k·∫øt qu·∫£ d·ªãch c·ªßa Greedy Decoding v√† Beam Search tr√™n c√πng m·ªôt c√¢u ƒë·∫ßu v√†o.
"""

# ==============================================================================
# CELL 6.4: DEMO S·ª¨ D·ª§NG C√ÅC H√ÄM B·ªî SUNG
# ==============================================================================

print("=" * 80)
print("üöÄ DEMO C√ÅC H√ÄM B·ªî SUNG")
print("=" * 80)

# ------------------------------------------------------------------------------
# 1. V·∫º BI·ªÇU ƒê·ªí LOSS (CH·ªà V·∫º NH·ªÆNG HISTORY TH·ª∞C S·ª∞ T·ªíN T·∫†I)
# ------------------------------------------------------------------------------
print("\nüìä 1. V·∫º BI·ªÇU ƒê·ªí LOSS")
print("-" * 50)

# Baseline
if 'baseline_history' in globals() and isinstance(baseline_history, dict):
    if baseline_history.get('train_loss'):
        plot_history(
            baseline_history,
            title="Baseline Seq2Seq Training",
            filename="baseline_loss.png"
        )
    else:
        print("‚ö†Ô∏è baseline_history t·ªìn t·∫°i nh∆∞ng kh√¥ng c√≥ d·ªØ li·ªáu loss.")
else:
    print("‚ÑπÔ∏è Kh√¥ng v·∫Ω Baseline loss (baseline_history kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c·∫ßn thi·∫øt).")

# Attention
if 'attention_history' in globals() and isinstance(attention_history, dict):
    if attention_history.get('train_loss'):
        plot_history(
            attention_history,
            title="Attention Seq2Seq Training",
            filename="attention_loss.png"
        )
    else:
        print("‚ö†Ô∏è attention_history t·ªìn t·∫°i nh∆∞ng kh√¥ng c√≥ d·ªØ li·ªáu loss.")
else:
    print("‚ùå Kh√¥ng t√¨m th·∫•y attention_history. H√£y ch·∫°y l·∫°i Cell 4.3.")

# ------------------------------------------------------------------------------
# 2. PH√ÇN T√çCH L·ªñI (ATTENTION MODEL ‚Äì DEMO TR√äN SUBSET)
# ------------------------------------------------------------------------------
print("\nüìä 2. PH√ÇN T√çCH L·ªñI (ATTENTION MODEL ‚Äì 100 C√ÇU)")
print("-" * 50)

# ƒê·∫£m b·∫£o model ·ªü ch·∫ø ƒë·ªô eval
attention_model.eval()

attention_analysis = analyze_model_performance(
    translate_attention,
    test_en[:100],   # Ch·ªâ d√πng 100 c√¢u ƒë·ªÉ demo (NHANH + ƒê·ª¶ √ù)
    test_fr[:100],
    num_examples=3
)

print("\n‚úÖ Ho√†n t·∫•t ph√¢n t√≠ch l·ªói cho Attention model.")

# ------------------------------------------------------------------------------
# 3. DEMO BEAM SEARCH vs GREEDY (ƒêI·ªÇM C·ªòNG)
# ------------------------------------------------------------------------------
print("\nüìä 3. DEMO BEAM SEARCH (ATTENTION MODEL)")
print("-" * 50)

demo_sentences = [
    "A man is walking with his dog.",
    "Two children are playing in the park."
]

for sent in demo_sentences:
    greedy_translation = translate_attention(sent)
    beam_translation = translate_beam_search(
        sent,
        attention_model,
        beam_size=3
    )

    print(f"\nüì• EN: {sent}")
    print(f"   Greedy Decoding: {greedy_translation}")
    print(f"   Beam Search (k=3): {beam_translation}")

print("\n" + "=" * 80)
print("‚úÖ HO√ÄN T·∫§T CELL 6.4 ‚Äì T·∫§T C·∫¢ DEMO ƒê√É CH·∫†Y ·ªîN ƒê·ªäNH")
print("=" * 80)

"""## 6.5 Tr·ª±c quan h√≥a c∆° ch·∫ø Attention (Attention Maps)

**C√°ch ƒë·ªçc bi·ªÉu ƒë·ªì:**
* **Tr·ª•c ho√†nh (X):** C√¢u ngu·ªìn (Ti·∫øng Anh).
* **Tr·ª•c tung (Y):** C√¢u ƒë√≠ch (Ti·∫øng Ph√°p - do m√¥ h√¨nh d·ªãch).
* **√î s√°ng m√†u:** Th·ªÉ hi·ªán tr·ªçng s·ªë Attention ($\alpha$) cao. Nghƒ©a l√† t·∫°i b∆∞·ªõc d·ªãch t·ª´ ti·∫øng Ph√°p ƒë√≥, m√¥ h√¨nh ƒëang t·∫≠p trung m·∫°nh v√†o t·ª´ ti·∫øng Anh t∆∞∆°ng ·ª©ng.

> **K·ª≥ v·ªçng:** C√°c √¥ s√°ng m√†u n√™n ch·∫°y theo ƒë∆∞·ªùng ch√©o (t·ª´ tr√°i tr√™n xu·ªëng ph·∫£i d∆∞·ªõi), th·ªÉ hi·ªán s·ª± t∆∞∆°ng ƒë·ªìng v·ªÅ th·ª© t·ª± t·ª´. N·∫øu c√≥ ƒë·∫£o ng·ªØ (v√≠ d·ª•: t√≠nh t·ª´ ƒë·ª©ng tr∆∞·ªõc danh t·ª´), ƒë∆∞·ªùng ch√©o s·∫Ω b·ªã l·ªách, ch·ª©ng t·ªè Attention ho·∫°t ƒë·ªông ƒë√∫ng.
"""

# ==============================================================================
# CELL 6.5 (FINAL): T·ªîNG H·ª¢P K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (FULL 4 TR∆Ø·ªúNG H·ª¢P)
# ==============================================================================
import pandas as pd
import torch
import torch.nn.functional as F
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
from tqdm.auto import tqdm

# --- 1. H√†m t√≠nh to√°n ch·ªâ s·ªë (Metrics) ---
def evaluate_metrics(name, translate_func, src_data, trg_data, num_samples=None):
    print(f"\nüìä ƒêang ƒë√°nh gi√°: {name}...")

    # M·∫∑c ƒë·ªãnh ch·∫°y h·∫øt t·∫≠p test.
    if num_samples:
        indices = range(num_samples)
    else:
        indices = range(len(src_data))

    smooth = SmoothingFunction().method1
    refs = []
    hyps = []
    sentence_scores = []

    for idx in tqdm(indices, desc=f"Eval {name}"):
        src = src_data[idx]
        trg = trg_data[idx]
        try:
            pred = translate_func(src)
            pred_tokens = pred.split()
            trg_tokens = tokenizer_fr(trg.lower())

            hyps.append(pred_tokens)
            refs.append([trg_tokens])

            sent_score = sentence_bleu([trg_tokens], pred_tokens, smoothing_function=smooth)
            sentence_scores.append(sent_score)
        except Exception:
            continue

    corpus_score = corpus_bleu(refs, hyps, smoothing_function=smooth) * 100
    if len(sentence_scores) > 0:
        avg_sentence_score = (sum(sentence_scores) / len(sentence_scores)) * 100
    else:
        avg_sentence_score = 0

    return corpus_score, avg_sentence_score

# --- 2. ƒê·ªãnh nghƒ©a Beam Search RI√äNG cho BASELINE ---
# (V√¨ h√†m c≈© ch·ªâ ch·∫°y ƒë∆∞·ª£c cho Attention model)
def translate_beam_search_baseline(sentence, model, beam_size=3, max_len=50):
    model.eval()
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)

    with torch.no_grad():
        # Baseline Encoder ch·ªâ tr·∫£ v·ªÅ hidden, cell (kh√¥ng c√≥ encoder_outputs)
        hidden, cell = model.encoder(src_tensor, src_len)

    beams = [(0.0, [SOS_IDX], hidden, cell)]
    completed = []

    for _ in range(max_len):
        new_beams = []
        for score, tokens, h, c in beams:
            if tokens[-1] == EOS_IDX:
                completed.append((score, tokens))
                continue

            trg_tensor = torch.LongTensor([tokens[-1]]).to(device)
            with torch.no_grad():
                # Baseline Decoder ch·ªâ nh·∫≠n input, hidden, cell
                output, new_h, new_c = model.decoder(trg_tensor, h, c)

            log_probs = F.log_softmax(output, dim=1)
            topk_log_probs, topk_indices = log_probs.topk(beam_size)

            for i in range(beam_size):
                new_score = score + topk_log_probs[0, i].item()
                new_token = topk_indices[0, i].item()
                new_tokens = tokens + [new_token]
                new_beams.append((new_score, new_tokens, new_h, new_c))

        new_beams.sort(key=lambda x: x[0], reverse=True)
        beams = new_beams[:beam_size]
        if not beams: break

    for score, tokens, h, c in beams:
        completed.append((score, tokens))

    if completed:
        completed.sort(key=lambda x: x[0] / len(x[1]), reverse=True)
        best_tokens = completed[0][1]
    else:
        best_tokens = [SOS_IDX]

    trg_tokens = [vocab_fr.get_itos()[i] for i in best_tokens]
    if trg_tokens[0] == '<sos>': trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens: trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]

    return ' '.join(trg_tokens)

# Wrappers
def run_beam_baseline(sent):
    return translate_beam_search_baseline(sent, baseline_model, beam_size=3)

def run_beam_attention(sent):
    return translate_beam_search(sent, attention_model, beam_size=3)

# --- 3. Ch·∫°y ƒë√°nh gi√° FULL 4 TR∆Ø·ªúNG H·ª¢P ---
print("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√° to√†n di·ªán (4 tr∆∞·ªùng h·ª£p)...")

# 1. Baseline - Greedy
c_base_g, s_base_g = evaluate_metrics("Baseline (Greedy)", translate_baseline, test_en, test_fr)

# 2. Baseline - Beam Search (M·ªõi th√™m)
c_base_b, s_base_b = evaluate_metrics("Baseline (Beam Search)", run_beam_baseline, test_en, test_fr)

# 3. Attention - Greedy
c_attn_g, s_attn_g = evaluate_metrics("Attention (Greedy)", translate_attention, test_en, test_fr)

# 4. Attention - Beam Search
c_attn_b, s_attn_b = evaluate_metrics("Attention (Beam Search)", run_beam_attention, test_en, test_fr)

# --- 4. Xu·∫•t b·∫£ng k·∫øt qu·∫£ ---
results_data = {
    'M√¥ h√¨nh': [
        'Baseline (LSTM thu·∫ßn)',
        'Baseline (LSTM thu·∫ßn)',
        'Advanced (Attention)',
        'Advanced (Attention)'
    ],
    'Decoding Strategy': [
        'Greedy Decoding',
        'Beam Search (k=3)',
        'Greedy Decoding',
        'Beam Search (k=3)'
    ],
    'Corpus BLEU': [c_base_g, c_base_b, c_attn_g, c_attn_b],
    'Average Sentence BLEU': [s_base_g, s_base_b, s_attn_g, s_attn_b]
}

df_results = pd.DataFrame(results_data)

print("\n" + "="*80)
print("üèÜ B·∫¢NG T·ªîNG H·ª¢P K·∫æT QU·∫¢ ƒê·∫¶Y ƒê·ª¶")
print("="*80)
print(df_results.to_markdown(index=False, floatfmt=".2f"))
print("-" * 80)
print(f"üìå Ghi ch√∫: ƒê√£ c·∫≠p nh·∫≠t ƒë·ªß 4 tr∆∞·ªùng h·ª£p (2 models x 2 strategies).")
print("="*80)

# ==============================================================================
#  L∆ØU ·∫¢NH BI·ªÇU ƒê·ªí V√ÄO GOOGLE DRIVE
# ==============================================================================
import shutil
import os
from google.colab import drive

# 1. K·∫øt n·ªëi v·ªõi Google Drive (Mount)
# N·∫øu ƒë√£ mount ·ªü Cell 1.1 r·ªìi th√¨ d√≤ng n√†y s·∫Ω b√°o "Drive already mounted", kh√¥ng sao c·∫£.
drive.mount('/content/drive')

# 2. ƒê·ªãnh nghƒ©a th∆∞ m·ª•c l∆∞u trong Drive
# B·∫°n c√≥ th·ªÉ ƒë·ªïi t√™n th∆∞ m·ª•c "NMT_EN_FR_Final" th√†nh t√™n kh√°c t√πy √Ω
DRIVE_FOLDER = "/content/drive/MyDrive/NMT_EN_FR_Final"

if not os.path.exists(DRIVE_FOLDER):
    os.makedirs(DRIVE_FOLDER, exist_ok=True)
    print(f"üìÇ ƒê√£ t·∫°o th∆∞ m·ª•c m·ªõi tr√™n Drive: {DRIVE_FOLDER}")
else:
    print(f"üìÇ L∆∞u v√†o th∆∞ m·ª•c c√≥ s·∫µn: {DRIVE_FOLDER}")

# 3. Danh s√°ch c√°c file ·∫£nh c·∫ßn l∆∞u
# (ƒê·∫£m b·∫£o t√™n file kh·ªõp v·ªõi t√™n b·∫°n ƒë√£ ƒë·∫∑t trong h√†m plot_history ·ªü Cell 6.1)
image_files = ['baseline_loss.png', 'attention_loss.png']

print("-" * 50)

# 4. Th·ª±c hi·ªán Copy
for filename in image_files:
    # Ki·ªÉm tra xem file ·∫£nh c√≥ t·ªìn t·∫°i trong m√¥i tr∆∞·ªùng Colab hi·ªán t·∫°i kh√¥ng
    if os.path.exists(filename):
        destination = os.path.join(DRIVE_FOLDER, filename)
        shutil.copy(filename, destination)
        print(f"‚úÖ ƒê√É L∆ØU TH√ÄNH C√îNG: {filename}")
        print(f"   üëâ ƒê∆∞·ªùng d·∫´n: {destination}")
    else:
        print(f"‚ö†Ô∏è KH√îNG T√åM TH·∫§Y: {filename}")
        print("   (G·ª£i √Ω: B·∫°n ƒë√£ ch·∫°y Cell 6.1 ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ch∆∞a?)")
    print("-" * 50)

print("\nüéâ Ho√†n t·∫•t! B·∫°n h√£y v√†o Google Drive ki·ªÉm tra nh√©.")

"""# H·∫øt"""