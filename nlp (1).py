#!/usr/bin/env python
# coding: utf-8

# # ğŸ“¦ PHáº¦N 1: Táº¢I VÃ€ Xá»¬ LÃ Dá»® LIá»†U
# 
# ---
# 
# **Má»¥c tiÃªu:**
# 1. Táº£i dataset Multi30K (English â†’ French)
# 2. Tokenization vá»›i SpaCy
# 3. XÃ¢y dá»±ng Vocabulary vá»›i special tokens: `<unk>`, `<pad>`, `<sos>`, `<eos>`
# 4. Táº¡o DataLoader vá»›i sorting + padding (sáºµn sÃ ng cho LSTM)
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 1.1: CÃ€I Äáº¶T THÆ¯ VIá»†N & Cáº¤U HÃŒNH
# ==============================================================================

# CÃ i Ä‘áº·t thÆ° viá»‡n (cháº¡y trÃªn Google Colab)
# 'pip install torch==2.2.2 torchtext==0.17.2 -q')
# 'pip install spacy nltk -q')
# 'python -m spacy download en_core_web_sm -q')
# 'python -m spacy download fr_core_news_sm -q')

# Import thÆ° viá»‡n
import torch
import torch.nn as nn
import torch.nn.functional as F  # Cáº§n cho Attention
import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import io
import os
import random

# =============================================================================
# SEED cho Reproducibility
# =============================================================================
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"âœ… Torch version: {torch.__version__}")
print(f"âœ… Torchtext version: {torchtext.__version__}")
print(f"âœ… Device: {device}")
print(f"âœ… Seed: {SEED}")

# =============================================================================
# Táº¢I DATASET MULTI30K (EN-FR)
# =============================================================================
# 'mkdir -p data')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz -O data/train.en.gz')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz -O data/train.fr.gz')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz -O data/val.en.gz')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz -O data/val.fr.gz')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz -O data/test.en.gz')
# 'wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz -O data/test.fr.gz')

# Giáº£i nÃ©n
# 'gunzip -kf data/*.gz')
# 'ls -la data/')

print("\nâœ… ÄÃ£ chuáº©n bá»‹ xong dá»¯ liá»‡u vÃ  thÆ° viá»‡n!")


# In[23]:


# ==============================================================================
# CELL 1.2: Äá»ŒC VÃ€ KIá»‚M TRA Dá»® LIá»†U
# ==============================================================================

def read_data(en_file, fr_file):
    """
    Äá»c dá»¯ liá»‡u song ngá»¯ tá»« file.
    
    Returns:
        en_sentences: List cÃ¢u tiáº¿ng Anh
        fr_sentences: List cÃ¢u tiáº¿ng PhÃ¡p (cÄƒn chá»‰nh 1-1)
    """
    with open(en_file, 'r', encoding='utf-8') as f:
        en_sentences = [line.strip() for line in f.readlines() if line.strip()]
    with open(fr_file, 'r', encoding='utf-8') as f:
        fr_sentences = [line.strip() for line in f.readlines() if line.strip()]
    
    # Äáº£m báº£o sá»‘ cÃ¢u khá»›p nhau
    assert len(en_sentences) == len(fr_sentences), "Sá»‘ cÃ¢u EN vÃ  FR khÃ´ng khá»›p!"
    return en_sentences, fr_sentences

# Äá»c dá»¯ liá»‡u train, val, test tá»« folder 'data/'
train_en, train_fr = read_data('data/train.en', 'data/train.fr')
val_en, val_fr = read_data('data/val.en', 'data/val.fr')
test_en, test_fr = read_data('data/test.en', 'data/test.fr')

print("=" * 50)
print("ğŸ“Š THá»NG KÃŠ Dá»® LIá»†U MULTI30K")
print("=" * 50)
print(f"   Train:      {len(train_en):,} cáº·p cÃ¢u")
print(f"   Validation: {len(val_en):,} cáº·p cÃ¢u")
print(f"   Test:       {len(test_en):,} cáº·p cÃ¢u")
print("=" * 50)

# Hiá»ƒn thá»‹ vÃ­ dá»¥
print("\nğŸ“ VÃ Dá»¤ 5 Cáº¶P CÃ‚U Äáº¦U TIÃŠN:")
for i in range(5):
    print(f"\nExample {i+1}:")
    print(f"   EN: {train_en[i]}")
    print(f"   FR: {train_fr[i]}")


# In[24]:


# ==============================================================================
# CELL 1.3: TOKENIZATION & VOCABULARY
# ==============================================================================

# --- Cáº¤U HÃŒNH TOKEN Äáº¶C BIá»†T ---
UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3
SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<sos>', '<eos>']

# =============================================================================
# HÃ€M: get_tokenizers()
# Nhiá»‡m vá»¥: Táº£i tokenizer cá»§a SpaCy cho tiáº¿ng Anh vÃ  PhÃ¡p
# =============================================================================
def get_tokenizers():
    try:
        en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')
        fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')
        print("âœ… ÄÃ£ táº£i thÃ nh cÃ´ng Tokenizer (SpaCy)")
        return en_tokenizer, fr_tokenizer
    except OSError:
        print("âŒ Lá»—i: ChÆ°a tÃ¬m tháº¥y model SpaCy. HÃ£y cháº¡y láº¡i Cell 1.1")
        return None, None

# =============================================================================
# HÃ€M: build_vocab()
# Nhiá»‡m vá»¥: XÃ¢y dá»±ng vocabulary tá»« file dá»¯ liá»‡u
# =============================================================================
def build_vocab(filepath, tokenizer):
    """
    XÃ¢y dá»±ng vocabulary tá»« file text.
    
    Args:
        filepath: ÄÆ°á»ng dáº«n file text
        tokenizer: Tokenizer function
    
    Returns:
        vocab: Vocabulary object vá»›i special tokens
    """
    def yield_tokens(path):
        with io.open(path, encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    yield tokenizer(line.strip())
    
    print(f"   Äang xÃ¢y dá»±ng vocab tá»« {filepath}...")
    
    vocab = build_vocab_from_iterator(
        yield_tokens(filepath),
        min_freq=2,           # Bá» qua tá»« xuáº¥t hiá»‡n < 2 láº§n
        max_tokens=10000,     # Giá»›i háº¡n vocabulary size
        specials=SPECIAL_SYMBOLS
    )
    
    # Set default index cho tá»« khÃ´ng cÃ³ trong vocab (OOV)
    vocab.set_default_index(UNK_IDX)
    return vocab

# =============================================================================
# THá»°C THI
# =============================================================================
tokenizer_en, tokenizer_fr = get_tokenizers()

if tokenizer_en and tokenizer_fr:
    print("\nğŸ“š XÃ¢y dá»±ng Vocabulary:")
    vocab_en = build_vocab('data/train.en', tokenizer_en)
    vocab_fr = build_vocab('data/train.fr', tokenizer_fr)
    
    print("\n" + "=" * 50)
    print("ğŸ“Š BÃO CÃO VOCABULARY")
    print("=" * 50)
    print(f"   Vocab EN: {len(vocab_en):,} tokens (max: 10,004)")
    print(f"   Vocab FR: {len(vocab_fr):,} tokens (max: 10,004)")
    print(f"   Special tokens: <unk>={UNK_IDX}, <pad>={PAD_IDX}, <sos>={SOS_IDX}, <eos>={EOS_IDX}")
    
    # Kiá»ƒm tra xá»­ lÃ½ tá»« láº¡ (OOV)
    test_oov = vocab_en['tá»«_khÃ´ng_tá»“n_táº¡i_xyz']
    if test_oov == UNK_IDX:
        print("   OOV handling: âœ… ThÃ nh cÃ´ng (tráº£ vá» index 0)")
    else:
        print(f"   OOV handling: âŒ Tháº¥t báº¡i (tráº£ vá» {test_oov})")
    print("=" * 50)


# In[25]:


# ==============================================================================
# CELL 1.4: TEXT TRANSFORM (Numericalization)
# ==============================================================================

def text_transform(text, tokenizer, vocab):
    """
    Chuyá»ƒn Ä‘á»•i cÃ¢u text thÃ nh tensor sá»‘.
    
    Pipeline: text â†’ tokens â†’ token_ids â†’ tensor vá»›i <sos> vÃ  <eos>
    
    Args:
        text: CÃ¢u input (string)
        tokenizer: Tokenizer function
        vocab: Vocabulary object
    
    Returns:
        tensor: [SOS, token_ids..., EOS]
    """
    tokens = tokenizer(text)
    token_ids = [vocab[token] for token in tokens]
    return torch.tensor([SOS_IDX] + token_ids + [EOS_IDX], dtype=torch.long)

# =============================================================================
# KIá»‚M TRA TEXT PIPELINE
# =============================================================================
print("=" * 50)
print("ğŸ” KIá»‚M TRA TEXT PIPELINE")
print("=" * 50)

sample_sentence = "Two young, White males are outside."
print(f"1. CÃ¢u gá»‘c:     '{sample_sentence}'")

# Tokenize
sample_tokens = tokenizer_en(sample_sentence)
print(f"2. Tokens:      {sample_tokens}")

# Transform
sample_tensor = text_transform(sample_sentence, tokenizer_en, vocab_en)
print(f"3. Tensor:      {sample_tensor}")
print(f"4. Shape:       {sample_tensor.shape}")
print(f"5. Dtype:       {sample_tensor.dtype}")

# Kiá»ƒm tra logic <sos> vÃ  <eos>
if sample_tensor[0] == SOS_IDX and sample_tensor[-1] == EOS_IDX:
    print("\nâœ… Logic <sos>/<eos>: ÄÃšNG (Ä‘áº§u=2, cuá»‘i=3)")
else:
    print(f"\nâŒ Logic <sos>/<eos>: SAI (Ä‘áº§u={sample_tensor[0]}, cuá»‘i={sample_tensor[-1]})")
print("=" * 50)


# In[26]:


# ==============================================================================
# CELL 1.5: DATASET, COLLATE_FN & DATALOADER
# ==============================================================================

# =============================================================================
# CLASS: TranslationDataset
# =============================================================================
class TranslationDataset(Dataset):
    """
    Dataset cho dá»¯ liá»‡u song ngá»¯.
    LÆ°u trá»¯ cáº·p cÃ¢u (source, target) dáº¡ng text.
    """
    def __init__(self, src_list, trg_list):
        self.src_list = src_list
        self.trg_list = trg_list
        assert len(src_list) == len(trg_list), "Sá»‘ cÃ¢u source vÃ  target khÃ´ng khá»›p!"

    def __len__(self):
        return len(self.src_list)

    def __getitem__(self, idx):
        return self.src_list[idx], self.trg_list[idx]

# =============================================================================
# HÃ€M: collate_fn
# Xá»­ lÃ½ batch: padding + sorting theo Ä‘á»™ dÃ i (cho pack_padded_sequence)
# =============================================================================
def collate_fn(batch):
    """
    Xá»­ lÃ½ batch dá»¯ liá»‡u:
    1. Chuyá»ƒn text â†’ tensor
    2. Padding Ä‘á»ƒ Ä‘á»“ng bá»™ Ä‘á»™ dÃ i
    3. Sort theo Ä‘á»™ dÃ i giáº£m dáº§n (yÃªu cáº§u cho pack_padded_sequence)
    
    Returns:
        src_padded: [src_len, batch_size]
        trg_padded: [trg_len, batch_size]
        sorted_lens: [batch_size] - Ä‘á»™ dÃ i thá»±c cá»§a tá»«ng cÃ¢u source
    """
    src_batch, trg_batch = [], []

    # Chuyá»ƒn Ä‘á»•i Text â†’ Tensor
    for src_sample, trg_sample in batch:
        src_batch.append(text_transform(src_sample, tokenizer_en, vocab_en))
        trg_batch.append(text_transform(trg_sample, tokenizer_fr, vocab_fr))

    # Padding (Ä‘á»“ng bá»™ Ä‘á»™ dÃ i trong batch)
    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX)
    trg_padded = pad_sequence(trg_batch, padding_value=PAD_IDX)

    # TÃ­nh Ä‘á»™ dÃ i thá»±c táº¿ cá»§a tá»«ng cÃ¢u nguá»“n (dtype=long cho pack_padded_sequence)
    src_lens = torch.tensor([len(x) for x in src_batch], dtype=torch.long)

    # Sort giáº£m dáº§n theo Ä‘á»™ dÃ i (yÃªu cáº§u cá»§a pack_padded_sequence vá»›i enforce_sorted=True)
    sorted_lens, sorted_indices = torch.sort(src_lens, descending=True)

    # Sáº¯p xáº¿p láº¡i tensors theo thá»© tá»± Ä‘Ã£ sort
    src_padded = src_padded[:, sorted_indices]
    trg_padded = trg_padded[:, sorted_indices]

    return src_padded, trg_padded, sorted_lens

# =============================================================================
# Táº O DATALOADER
# =============================================================================
BATCH_SIZE = 64

# Táº¡o Dataset
train_dataset = TranslationDataset(train_en, train_fr)
valid_dataset = TranslationDataset(val_en, val_fr)
test_dataset = TranslationDataset(test_en, test_fr)

# Táº¡o DataLoader
train_loader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True,           # Shuffle cho training
    collate_fn=collate_fn
)
valid_loader = DataLoader(
    valid_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False,
    collate_fn=collate_fn
)
test_loader = DataLoader(
    test_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False,
    collate_fn=collate_fn
)

# =============================================================================
# KIá»‚M TRA DATALOADER
# =============================================================================
print("=" * 50)
print("ğŸ” KIá»‚M TRA DATALOADER")
print("=" * 50)

try:
    src, trg, src_len = next(iter(train_loader))
    
    print(f"1. Source shape:   {src.shape} (seq_len, batch_size)")
    print(f"2. Target shape:   {trg.shape} (seq_len, batch_size)")
    print(f"3. Lengths shape:  {src_len.shape}")
    print(f"   - CÃ¢u dÃ i nháº¥t:  {src_len[0]} tokens")
    print(f"   - CÃ¢u ngáº¯n nháº¥t: {src_len[-1]} tokens")
    
    # Kiá»ƒm tra sorting
    if src_len[0] >= src_len[-1]:
        print("\nâœ… Batch Ä‘Ã£ SORT theo Ä‘á»™ dÃ i (giáº£m dáº§n)")
        print("   â†’ Sáºµn sÃ ng cho pack_padded_sequence!")
    else:
        print("\nâŒ Lá»—i: Batch chÆ°a Ä‘Æ°á»£c sort Ä‘Ãºng!")
    
    print("\n" + "-" * 50)
    print(f"ğŸ“Š Sá» BATCH:")
    print(f"   Train:      {len(train_loader)} batches")
    print(f"   Validation: {len(valid_loader)} batches")
    print(f"   Test:       {len(test_loader)} batches")
    print("=" * 50)
    print("\nâœ… PHáº¦N 1 HOÃ€N Táº¤T - Sáºµn sÃ ng cho PHáº¦N 2 (Model)!")

except Exception as e:
    print(f"\nâŒ Lá»–I: {e}")
    print("Gá»£i Ã½: Kiá»ƒm tra láº¡i collate_fn cÃ³ tráº£ vá» Ä‘Ãºng 3 giÃ¡ trá»‹ khÃ´ng?")


# # ğŸ—ï¸ PHáº¦N 2: XÃ‚Y Dá»°NG MÃ” HÃŒNH BASELINE SEQ2SEQ
# 
# ---
# 
# ## Kiáº¿n trÃºc Encoder-Decoder LSTM (KhÃ´ng Attention)
# 
# ```
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                     BASELINE SEQ2SEQ ARCHITECTURE                â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                  â”‚
# â”‚  INPUT (EN)       ENCODER           DECODER        OUTPUT (FR)  â”‚
# â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’      â”€â”€â”€â”€â”€â”€â”€â”€â†’         â”€â”€â”€â”€â”€â”€â”€â”€â†’      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚
# â”‚                                                                  â”‚
# â”‚  "A man walks" â†’ [LSTM x2] â†’ (h,c) â†’ [LSTM x2] â†’ "Un homme..." â”‚
# â”‚                              â†‘                                   â”‚
# â”‚                       Context Vector                             â”‚
# â”‚                  (Fixed representation)                          â”‚
# â”‚                                                                  â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# **CÃ´ng thá»©c:**
# - Encoder: `(h_t, c_t) = LSTM(embed(x_t), (h_{t-1}, c_{t-1}))`
# - Decoder: `(h_t, c_t) = LSTM(embed(y_{t-1}), (h'_{t-1}, c'_{t-1}))`
# - Output:  `p(y_t) = softmax(Linear(h_t))`
# 
# **Hyperparameters:**
# 
# | Parameter | Value | MÃ´ táº£ |
# |-----------|-------|-------|
# | `EMB_DIM` | 256 | Embedding dimension |
# | `HID_DIM` | 512 | Hidden state dimension |
# | `N_LAYERS` | 2 | Sá»‘ lá»›p LSTM |
# | `DROPOUT` | 0.5 | Dropout rate |
# 
# ---

# In[27]:


# ==============================================================================
# CELL 2.1: Äá»ŠNH NGHÄ¨A CÃC CLASS MODEL
# ==============================================================================

# =============================================================================
# CLASS: Encoder
# =============================================================================
class Encoder(nn.Module):
    """
    Encoder LSTM cho Seq2Seq.
    
    Nhiá»‡m vá»¥: Äá»c cÃ¢u nguá»“n vÃ  táº¡o context vector (hidden, cell states).
    Sá»­ dá»¥ng pack_padded_sequence Ä‘á»ƒ xá»­ lÃ½ padding hiá»‡u quáº£.
    
    Args:
        input_dim: KÃ­ch thÆ°á»›c vocabulary nguá»“n
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: Sá»‘ lá»›p LSTM
        dropout: Dropout rate
    """
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_len):
        """
        Forward pass cá»§a Encoder.
        
        Args:
            src: [src_len, batch_size] - Tensor cÃ¢u nguá»“n
            src_len: [batch_size] - Äá»™ dÃ i thá»±c cá»§a má»—i cÃ¢u
        
        Returns:
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states
        
        Note: Baseline KHÃ”NG tráº£ vá» encoder_outputs (sáº½ thÃªm khi cÃ³ Attention)
        """
        # src: [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        # embedded: [src_len, batch_size, emb_dim]
        
        # Pack Ä‘á»ƒ LSTM khÃ´ng xá»­ lÃ½ padding tokens
        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)
        
        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]
        
        return hidden, cell


# =============================================================================
# CLASS: Decoder
# =============================================================================
class Decoder(nn.Module):
    """
    Decoder LSTM cho Seq2Seq (Baseline - khÃ´ng Attention).
    
    Nhiá»‡m vá»¥: Sinh cÃ¢u Ä‘Ã­ch tá»«ng token má»™t, dá»±a trÃªn context tá»« Encoder.
    
    Args:
        output_dim: KÃ­ch thÆ°á»›c vocabulary Ä‘Ã­ch
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: Sá»‘ lá»›p LSTM
        dropout: Dropout rate
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        """
        Forward pass cá»§a Decoder (1 timestep).
        
        Args:
            input: [batch_size] - Token hiá»‡n táº¡i
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states
        
        Returns:
            prediction: [batch_size, output_dim] - Logits cho vocabulary
            hidden: [n_layers, batch_size, hid_dim] - Updated hidden
            cell: [n_layers, batch_size, hid_dim] - Updated cell
        """
        # input: [batch_size] â†’ [1, batch_size]
        input = input.unsqueeze(0)
        
        embedded = self.dropout(self.embedding(input))
        # embedded: [1, batch_size, emb_dim]
        
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [1, batch_size, hid_dim]
        
        prediction = self.fc_out(output.squeeze(0))
        # prediction: [batch_size, output_dim]
        
        return prediction, hidden, cell


# =============================================================================
# CLASS: Seq2Seq
# =============================================================================
class Seq2Seq(nn.Module):
    """
    MÃ´ hÃ¬nh Seq2Seq Baseline (Encoder-Decoder khÃ´ng Attention).
    
    Äiá»u phá»‘i Encoder vÃ  Decoder, xá»­ lÃ½ teacher forcing.
    
    Args:
        encoder: Encoder instance
        decoder: Decoder instance
        device: 'cuda' hoáº·c 'cpu'
    """
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        # Äáº£m báº£o encoder vÃ  decoder tÆ°Æ¡ng thÃ­ch
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must match!"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers of encoder and decoder must match!"

    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Forward pass cá»§a Seq2Seq.
        
        Args:
            src: [src_len, batch_size] - CÃ¢u nguá»“n
            src_len: [batch_size] - Äá»™ dÃ i cÃ¢u nguá»“n
            trg: [trg_len, batch_size] - CÃ¢u Ä‘Ã­ch
            teacher_forcing_ratio: Tá»· lá»‡ sá»­ dá»¥ng ground truth (0.0 - 1.0)
        
        Returns:
            outputs: [trg_len, batch_size, output_dim] - Logits cho má»—i timestep
        """
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # Tensor lÆ°u output cá»§a decoder táº¡i má»—i timestep
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # Encode cÃ¢u nguá»“n
        hidden, cell = self.encoder(src, src_len)
        
        # Token Ä‘áº§u tiÃªn lÃ  <sos>
        input = trg[0, :]
        
        # Decode tá»«ng timestep
        for t in range(1, trg_len):
            # Forward decoder
            output, hidden, cell = self.decoder(input, hidden, cell)
            
            # LÆ°u output
            outputs[t] = output
            
            # Teacher Forcing: dÃ¹ng ground truth hoáº·c prediction
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1
        
        return outputs


# =============================================================================
# HÃ€M: init_weights
# =============================================================================
def init_weights(m):
    """
    Khá»Ÿi táº¡o weights cho model.
    - Weights: Uniform distribution [-0.08, 0.08]
    - Biases: 0
    """
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.uniform_(param.data, -0.08, 0.08)
        else:
            nn.init.constant_(param.data, 0)


print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a: Encoder, Decoder, Seq2Seq, init_weights")


# In[28]:


# ==============================================================================
# CELL 2.2: KHá»I Táº O BASELINE MODEL (FIXED CONTEXT VECTOR)
# ==============================================================================
# âš ï¸ ÄÃ‚Y LÃ€ MÃ” HÃŒNH BASELINE - Sá»¬ Dá»¤NG CONTEXT VECTOR Cá» Äá»ŠNH
# (hidden state cuá»‘i cÃ¹ng cá»§a Encoder, KHÃ”NG cÃ³ Attention)

# =============================================================================
# HYPERPARAMETERS
# =============================================================================
INPUT_DIM = len(vocab_en)    # Vocab size tiáº¿ng Anh
OUTPUT_DIM = len(vocab_fr)   # Vocab size tiáº¿ng PhÃ¡p
ENC_EMB_DIM = 256            # Encoder embedding dim
DEC_EMB_DIM = 256            # Decoder embedding dim
HID_DIM = 512                # Hidden state dim
N_LAYERS = 2                 # Sá»‘ lá»›p LSTM
ENC_DROPOUT = 0.5            # Encoder dropout
DEC_DROPOUT = 0.5            # Decoder dropout

# =============================================================================
# KHá»I Táº O BASELINE MODEL
# =============================================================================
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

# âš ï¸ QUAN TRá»ŒNG: Äáº·t tÃªn biáº¿n lÃ  baseline_model Ä‘á»ƒ phÃ¢n biá»‡t vá»›i attention_model
baseline_model = Seq2Seq(enc, dec, device).to(device)

# Ãp dá»¥ng weight initialization
baseline_model.apply(init_weights)

# Äáº¿m parameters
baseline_params = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)

print("=" * 60)
print("ğŸ—ï¸ MÃ” HÃŒNH BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)")
print("=" * 60)
print("ğŸ“Œ Äáº·c Ä‘iá»ƒm: Sá»­ dá»¥ng CONTEXT VECTOR Cá» Äá»ŠNH")
print("   (Chá»‰ dÃ¹ng hidden state cuá»‘i cÃ¹ng cá»§a Encoder)")
print("-" * 60)
print(f"Device:           {device}")
print(f"Input dim (EN):   {INPUT_DIM:,}")
print(f"Output dim (FR):  {OUTPUT_DIM:,}")
print(f"Embedding dim:    {ENC_EMB_DIM}")
print(f"Hidden dim:       {HID_DIM}")
print(f"Num layers:       {N_LAYERS}")
print(f"Dropout:          {ENC_DROPOUT}")
print("-" * 60)
print(f"Total parameters: {baseline_params:,}")
print("=" * 60)

# =============================================================================
# KIá»‚M TRA Káº¾T Ná»I DATA â†’ MODEL
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ” KIá»‚M TRA FORWARD PASS (BASELINE)")
print("=" * 60)

try:
    # Láº¥y 1 batch tá»« train_loader
    src, trg, src_len = next(iter(train_loader))
    src = src.to(device)
    trg = trg.to(device)
    
    print(f"Input (src):      {src.shape}  [seq_len, batch_size]")
    print(f"Input (src_len):  {src_len.shape}  [batch_size]")
    print(f"Target (trg):     {trg.shape}  [seq_len, batch_size]")
    
    # Forward pass vá»›i baseline_model
    output = baseline_model(src, src_len, trg)
    
    print(f"Output:           {output.shape}  [seq_len, batch_size, vocab_size]")
    
    # Validate output shape
    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)
    if output.shape == expected_shape:
        print("\nâœ… BASELINE FORWARD PASS THÃ€NH CÃ”NG!")
        print("   â†’ Baseline Model sáºµn sÃ ng cho Training.")
    else:
        print(f"\nâš ï¸ Shape khÃ´ng khá»›p! Expected: {expected_shape}")
    
except Exception as e:
    print(f"\nâŒ Lá»–I: {e}")
    import traceback
    traceback.print_exc()

print("=" * 60)
print("\nâœ… PHáº¦N 2 HOÃ€N Táº¤T - Baseline Model Ä‘Ã£ sáºµn sÃ ng!")


# # ğŸ¯ PHáº¦N 3: SEQ2SEQ + LUONG ATTENTION (MÃ” HÃŒNH CHÃNH)
# 
# ---
# 
# ## Kiáº¿n trÃºc Encoder-Decoder LSTM vá»›i Attention
# 
# ```
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                    SEQ2SEQ + LUONG ATTENTION ARCHITECTURE               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                         â”‚
# â”‚  ENCODER                              DECODER                           â”‚
# â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                              â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
# â”‚                                                                         â”‚
# â”‚  "A man walks"  â”€â”€â”€â”€â”€â”€â”                                                 â”‚
# â”‚       â†“               â”‚                                                 â”‚
# â”‚   [LSTM x2]           â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
# â”‚       â†“               â”‚              â”‚   ATTENTION     â”‚               â”‚
# â”‚  encoder_outputs â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (Luong General)â”‚               â”‚
# â”‚  (hâ‚, hâ‚‚, ..., hâ‚™)    â”‚              â”‚                 â”‚               â”‚
# â”‚       â†“               â”‚              â”‚  score = hâ‚œáµ€Wâ‚hâ‚›â”‚               â”‚
# â”‚  (hidden, cell) â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Î± = softmax    â”‚               â”‚
# â”‚                       â”‚              â”‚  c = Î£ Î±áµ¢háµ¢     â”‚               â”‚
# â”‚                       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
# â”‚                       â”‚                       â†“                         â”‚
# â”‚                       â”‚              context_vector                     â”‚
# â”‚                       â”‚                       â†“                         â”‚
# â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [embed; context]                 â”‚
# â”‚                                              â†“                          â”‚
# â”‚                                         [LSTM x2]                       â”‚
# â”‚                                              â†“                          â”‚
# â”‚                                    [hidden; context]                    â”‚
# â”‚                                              â†“                          â”‚
# â”‚                                         Linear â†’ vocab                  â”‚
# â”‚                                              â†“                          â”‚
# â”‚                                      "Un homme marche"                  â”‚
# â”‚                                                                         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# ## Luong Attention (General)
# 
# **CÃ´ng thá»©c:**
# 
# $$score(h_t, h_s) = h_t^T \cdot W_a \cdot h_s$$
# 
# $$\alpha = softmax(score)$$
# 
# $$context = \sum_i \alpha_i \cdot h_i$$
# 
# **Tham kháº£o:** Luong et al. (2015) - "Effective Approaches to Attention-based NMT"
# 
# ---

# In[1]:


# ==============================================================================
# CELL 3.1: ATTENTION MECHANISM + ENCODER/DECODER Vá»šI ATTENTION
# ==============================================================================

# =============================================================================
# CLASS: Attention (Luong General)
# =============================================================================
class Attention(nn.Module):
    """
    Luong General Attention.
    
    CÃ´ng thá»©c: score(h_t, h_s) = h_t^T * W_a * h_s
    
    Args:
        hid_dim: Hidden dimension cá»§a encoder/decoder
    
    Reference: "Effective Approaches to Attention-based NMT" (Luong et al., 2015)
    """
    def __init__(self, hid_dim):
        super().__init__()
        # W_a: Linear layer Ä‘á»ƒ tÃ­nh score
        self.W_a = nn.Linear(hid_dim, hid_dim, bias=False)
    
    def forward(self, decoder_hidden, encoder_outputs, mask=None):
        """
        TÃ­nh attention weights vÃ  context vector.
        
        Args:
            decoder_hidden: [batch_size, hid_dim] - Hidden state hiá»‡n táº¡i cá»§a decoder
            encoder_outputs: [src_len, batch_size, hid_dim] - Táº¥t cáº£ hidden states cá»§a encoder
            mask: [batch_size, src_len] - Mask cho padding (1=valid, 0=pad)
        
        Returns:
            context: [batch_size, hid_dim] - Context vector
            attention_weights: [batch_size, src_len] - Attention weights
        """
        # decoder_hidden: [batch_size, hid_dim]
        # encoder_outputs: [src_len, batch_size, hid_dim]
        
        src_len = encoder_outputs.shape[0]
        
        # Reshape decoder_hidden: [batch_size, hid_dim] â†’ [batch_size, 1, hid_dim]
        decoder_hidden = decoder_hidden.unsqueeze(1)
        
        # Permute encoder_outputs: [src_len, batch_size, hid_dim] â†’ [batch_size, src_len, hid_dim]
        encoder_outputs_perm = encoder_outputs.permute(1, 0, 2)
        
        # TÃ­nh W_a * h_s: [batch_size, src_len, hid_dim]
        energy = self.W_a(encoder_outputs_perm)
        
        # TÃ­nh h_t^T * (W_a * h_s): [batch_size, 1, hid_dim] x [batch_size, hid_dim, src_len]
        # â†’ [batch_size, 1, src_len]
        attention_scores = torch.bmm(decoder_hidden, energy.permute(0, 2, 1))
        
        # Squeeze: [batch_size, 1, src_len] â†’ [batch_size, src_len]
        attention_scores = attention_scores.squeeze(1)
        
        # Ãp dá»¥ng mask (náº¿u cÃ³) - Ä‘áº·t padding positions = -inf
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)
        
        # Softmax Ä‘á»ƒ cÃ³ attention weights: [batch_size, src_len]
        attention_weights = F.softmax(attention_scores, dim=1)
        
        # TÃ­nh context vector: weighted sum cá»§a encoder outputs
        # [batch_size, 1, src_len] x [batch_size, src_len, hid_dim] â†’ [batch_size, 1, hid_dim]
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs_perm)
        
        # Squeeze: [batch_size, 1, hid_dim] â†’ [batch_size, hid_dim]
        context = context.squeeze(1)
        
        return context, attention_weights


# =============================================================================
# CLASS: EncoderAttention (Sá»­a tá»« Encoder Ä‘á»ƒ tráº£ vá» encoder_outputs)
# =============================================================================
class EncoderAttention(nn.Module):
    """
    Encoder LSTM cho Seq2Seq + Attention.
    
    KhÃ¡c vá»›i Encoder baseline: Tráº£ vá» thÃªm encoder_outputs cho Attention.
    
    Args:
        input_dim: KÃ­ch thÆ°á»›c vocabulary nguá»“n
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: Sá»‘ lá»›p LSTM
        dropout: Dropout rate
    """
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_len):
        """
        Forward pass cá»§a Encoder (cho Attention).
        
        Args:
            src: [src_len, batch_size] - Tensor cÃ¢u nguá»“n
            src_len: [batch_size] - Äá»™ dÃ i thá»±c cá»§a má»—i cÃ¢u
        
        Returns:
            encoder_outputs: [src_len, batch_size, hid_dim] - Táº¥t cáº£ hidden states
            hidden: [n_layers, batch_size, hid_dim] - Hidden state cuá»‘i
            cell: [n_layers, batch_size, hid_dim] - Cell state cuá»‘i
        """
        # src: [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        # embedded: [src_len, batch_size, emb_dim]
        
        # Pack Ä‘á»ƒ LSTM khÃ´ng xá»­ lÃ½ padding
        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), enforce_sorted=True)
        
        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        
        # Unpack Ä‘á»ƒ láº¥y encoder_outputs cho Attention
        encoder_outputs, _ = pad_packed_sequence(packed_outputs)
        # encoder_outputs: [src_len, batch_size, hid_dim]
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]
        
        return encoder_outputs, hidden, cell


# =============================================================================
# CLASS: DecoderAttention (Decoder vá»›i Luong Attention)
# =============================================================================
class DecoderAttention(nn.Module):
    """
    Decoder LSTM vá»›i Luong Attention.
    
    Kiáº¿n trÃºc:
    1. Embed input token
    2. TÃ­nh attention weights vÃ  context vector
    3. Concat [embedding; context] â†’ LSTM input
    4. LSTM forward
    5. Concat [hidden; context] â†’ Linear â†’ vocab
    
    Args:
        output_dim: KÃ­ch thÆ°á»›c vocabulary Ä‘Ã­ch
        emb_dim: Embedding dimension
        hid_dim: Hidden state dimension
        n_layers: Sá»‘ lá»›p LSTM
        dropout: Dropout rate
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = Attention(hid_dim)
        
        # LSTM nháº­n: embedding + context = emb_dim + hid_dim
        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)
        
        # Linear nháº­n: hidden + context = hid_dim * 2
        self.fc_out = nn.Linear(hid_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell, encoder_outputs, mask):
        """
        Forward pass cá»§a Decoder vá»›i Attention (1 timestep).
        
        Args:
            input: [batch_size] - Token hiá»‡n táº¡i
            hidden: [n_layers, batch_size, hid_dim] - Hidden states
            cell: [n_layers, batch_size, hid_dim] - Cell states
            encoder_outputs: [src_len, batch_size, hid_dim] - Encoder outputs
            mask: [batch_size, src_len] - Mask cho padding
        
        Returns:
            prediction: [batch_size, output_dim] - Logits cho vocabulary
            hidden: [n_layers, batch_size, hid_dim] - Updated hidden
            cell: [n_layers, batch_size, hid_dim] - Updated cell
            attention_weights: [batch_size, src_len] - Attention weights
        """
        # input: [batch_size] â†’ [1, batch_size]
        input = input.unsqueeze(0)
        
        # Embed
        embedded = self.dropout(self.embedding(input))
        # embedded: [1, batch_size, emb_dim]
        
        # TÃ­nh Attention: dÃ¹ng hidden state cá»§a layer cuá»‘i cÃ¹ng
        # hidden[-1]: [batch_size, hid_dim]
        context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)
        # context: [batch_size, hid_dim]
        # attention_weights: [batch_size, src_len]
        
        # Concat embedding vÃ  context cho LSTM input
        # embedded: [1, batch_size, emb_dim]
        # context: [batch_size, hid_dim] â†’ [1, batch_size, hid_dim]
        lstm_input = torch.cat([embedded, context.unsqueeze(0)], dim=2)
        # lstm_input: [1, batch_size, emb_dim + hid_dim]
        
        # LSTM forward
        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
        # output: [1, batch_size, hid_dim]
        
        # Concat hidden vÃ  context cho prediction
        # output.squeeze(0): [batch_size, hid_dim]
        # context: [batch_size, hid_dim]
        combined = torch.cat([output.squeeze(0), context], dim=1)
        # combined: [batch_size, hid_dim * 2]
        
        # Prediction
        prediction = self.fc_out(combined)
        # prediction: [batch_size, output_dim]
        
        return prediction, hidden, cell, attention_weights


# =============================================================================
# CLASS: Seq2SeqAttention
# =============================================================================
class Seq2SeqAttention(nn.Module):
    """
    MÃ´ hÃ¬nh Seq2Seq vá»›i Luong Attention (MÃ´ hÃ¬nh chÃ­nh).
    
    Äiá»u phá»‘i EncoderAttention vÃ  DecoderAttention.
    
    Args:
        encoder: EncoderAttention instance
        decoder: DecoderAttention instance
        device: 'cuda' hoáº·c 'cpu'
        pad_idx: Index cá»§a padding token
    """
    def __init__(self, encoder, decoder, device, pad_idx):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.pad_idx = pad_idx
        
        # Äáº£m báº£o encoder vÃ  decoder tÆ°Æ¡ng thÃ­ch
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must match!"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers of encoder and decoder must match!"

    def create_mask(self, src):
        """
        Táº¡o mask cho padding positions.
        
        Args:
            src: [src_len, batch_size]
        
        Returns:
            mask: [batch_size, src_len] - 1 cho valid, 0 cho padding
        """
        mask = (src != self.pad_idx).permute(1, 0)
        # mask: [batch_size, src_len]
        return mask

    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Forward pass cá»§a Seq2Seq vá»›i Attention.
        
        Args:
            src: [src_len, batch_size] - CÃ¢u nguá»“n
            src_len: [batch_size] - Äá»™ dÃ i cÃ¢u nguá»“n
            trg: [trg_len, batch_size] - CÃ¢u Ä‘Ã­ch
            teacher_forcing_ratio: Tá»· lá»‡ sá»­ dá»¥ng ground truth
        
        Returns:
            outputs: [trg_len, batch_size, output_dim] - Logits cho má»—i timestep
        """
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # Tensor lÆ°u outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # Encode - giá» tráº£ vá» thÃªm encoder_outputs
        encoder_outputs, hidden, cell = self.encoder(src, src_len)
        # encoder_outputs: [src_len, batch_size, hid_dim]
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]
        
        # Táº¡o mask cho padding
        mask = self.create_mask(src)
        # mask: [batch_size, src_len]
        
        # Token Ä‘áº§u tiÃªn lÃ  <sos>
        input = trg[0, :]
        
        # Decode tá»«ng timestep
        for t in range(1, trg_len):
            # Forward decoder vá»›i attention
            output, hidden, cell, attention_weights = self.decoder(
                input, hidden, cell, encoder_outputs, mask
            )
            # output: [batch_size, output_dim]
            # attention_weights: [batch_size, src_len]
            
            # LÆ°u output
            outputs[t] = output
            
            # Teacher Forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1
        
        return outputs


print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a: Attention, EncoderAttention, DecoderAttention, Seq2SeqAttention")


# In[ ]:


# ==============================================================================
# CELL 3.2: KHá»I Táº O ATTENTION MODEL VÃ€ KIá»‚M TRA
# ==============================================================================

# =============================================================================
# KHá»I Táº O MODEL Vá»šI ATTENTION
# =============================================================================
# Hyperparameters giá»¯ nguyÃªn tá»« baseline
enc_attn = EncoderAttention(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec_attn = DecoderAttention(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

# âš ï¸ QUAN TRá»ŒNG: Äáº·t tÃªn biáº¿n lÃ  attention_model Ä‘á»ƒ phÃ¢n biá»‡t vá»›i baseline_model
attention_model = Seq2SeqAttention(enc_attn, dec_attn, device, PAD_IDX).to(device)

# Ãp dá»¥ng weight initialization
attention_model.apply(init_weights)

# Äáº¿m parameters
attention_params = sum(p.numel() for p in attention_model.parameters() if p.requires_grad)

print("=" * 60)
print("ğŸ¯ MÃ” HÃŒNH SEQ2SEQ + LUONG ATTENTION")
print("=" * 60)
print("ğŸ“Œ Äáº·c Ä‘iá»ƒm: Sá»­ dá»¥ng DYNAMIC CONTEXT VECTOR")
print("   (Attention weights thay Ä‘á»•i theo tá»«ng timestep)")
print("-" * 60)
print(f"Device:           {device}")
print(f"Input dim (EN):   {INPUT_DIM:,}")
print(f"Output dim (FR):  {OUTPUT_DIM:,}")
print(f"Embedding dim:    {ENC_EMB_DIM}")
print(f"Hidden dim:       {HID_DIM}")
print(f"Num layers:       {N_LAYERS}")
print(f"Dropout:          {ENC_DROPOUT}")
print("-" * 60)
print(f"Attention params: {attention_params:,}")
print(f"Baseline params:  {baseline_params:,}")
print(f"ThÃªm (Attention): {attention_params - baseline_params:,}")
print("=" * 60)

# =============================================================================
# KIá»‚M TRA FORWARD PASS
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ” KIá»‚M TRA FORWARD PASS (ATTENTION MODEL)")
print("=" * 60)

try:
    # Láº¥y 1 batch tá»« train_loader
    src, trg, src_len = next(iter(train_loader))
    src = src.to(device)
    trg = trg.to(device)
    
    print(f"Input (src):      {src.shape}  [seq_len, batch_size]")
    print(f"Input (src_len):  {src_len.shape}  [batch_size]")
    print(f"Target (trg):     {trg.shape}  [seq_len, batch_size]")
    
    # Test Encoder
    encoder_outputs, hidden, cell = attention_model.encoder(src, src_len)
    print(f"\nEncoder outputs:  {encoder_outputs.shape}  [src_len, batch_size, hid_dim]")
    print(f"Hidden:           {hidden.shape}  [n_layers, batch_size, hid_dim]")
    print(f"Cell:             {cell.shape}  [n_layers, batch_size, hid_dim]")
    
    # Test full forward pass
    output = attention_model(src, src_len, trg)
    print(f"\nOutput:           {output.shape}  [trg_len, batch_size, vocab_size]")
    
    # Validate output shape
    expected_shape = (trg.shape[0], trg.shape[1], OUTPUT_DIM)
    if output.shape == expected_shape:
        print("\nâœ… ATTENTION FORWARD PASS THÃ€NH CÃ”NG!")
        print("   â†’ Attention Model sáºµn sÃ ng cho Training.")
    else:
        print(f"\nâš ï¸ Shape khÃ´ng khá»›p! Expected: {expected_shape}")
    
except Exception as e:
    print(f"\nâŒ Lá»–I: {e}")
    import traceback
    traceback.print_exc()

print("=" * 60)

# âš ï¸ KHÃ”NG gÃ¡n láº¡i biáº¿n - sá»­ dá»¥ng baseline_model vÃ  attention_model riÃªng biá»‡t
# Äiá»u nÃ y Ä‘áº£m báº£o cáº£ hai mÃ´ hÃ¬nh tá»“n táº¡i Ä‘á»™c láº­p Ä‘á»ƒ so sÃ¡nh

print("\n" + "=" * 60)
print("ğŸ“Œ HAI MÃ” HÃŒNH ÄÃƒ KHá»I Táº O:")
print("   1. baseline_model  - Seq2Seq vá»›i Fixed Context Vector")
print("   2. attention_model - Seq2Seq vá»›i Luong Attention")
print("=" * 60)
print("\nâœ… PHáº¦N 3 HOÃ€N Táº¤T - Sáºµn sÃ ng cho PHáº¦N 4 (Training)!")


# # ğŸš€ PHáº¦N 4: TRAINING PROCESS
# 
# ---
# 
# ## Quy trÃ¬nh huáº¥n luyá»‡n
# 
# ```
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                         TRAINING LOOP                               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                     â”‚
# â”‚  for epoch in range(N_EPOCHS):                                      â”‚
# â”‚      â”œâ”€â”€ train_loss = train(model, train_loader, ...)              â”‚
# â”‚      â”‚       â”œâ”€â”€ Forward pass (vá»›i Teacher Forcing 0.5)            â”‚
# â”‚      â”‚       â”œâ”€â”€ TÃ­nh loss (bá» <sos>, ignore <pad>)                â”‚
# â”‚      â”‚       â”œâ”€â”€ Backward + Gradient Clipping                       â”‚
# â”‚      â”‚       â””â”€â”€ Update weights                                     â”‚
# â”‚      â”‚                                                              â”‚
# â”‚      â”œâ”€â”€ valid_loss = evaluate(model, valid_loader, ...)           â”‚
# â”‚      â”‚       â””â”€â”€ Forward pass (khÃ´ng Teacher Forcing)              â”‚
# â”‚      â”‚                                                              â”‚
# â”‚      â”œâ”€â”€ if valid_loss < best_loss:                                â”‚
# â”‚      â”‚       â””â”€â”€ Save checkpoint (best_model.pth)                  â”‚
# â”‚      â”‚                                                              â”‚
# â”‚      â””â”€â”€ if no_improvement >= PATIENCE:                            â”‚
# â”‚              â””â”€â”€ EARLY STOPPING                                     â”‚
# â”‚                                                                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# **Hyperparameters (theo Äá»’ ÃN):**
# 
# | Parameter | Value | MÃ´ táº£ |
# |-----------|-------|-------|
# | `N_EPOCHS` | 20 | Sá»‘ epoch tá»‘i Ä‘a |
# | `LEARNING_RATE` | 0.001 | Learning rate (Adam) |
# | `CLIP` | 1.0 | Gradient clipping |
# | `TEACHER_FORCING` | 0.5 | Tá»· lá»‡ Teacher Forcing |
# | `PATIENCE` | 3 | Early Stopping patience |
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 4.1: Cáº¤U HÃŒNH TRAINING VÃ€ HELPER FUNCTIONS
# ==============================================================================

import time
import math

# =============================================================================
# HYPERPARAMETERS (THEO Äá»’ ÃN)
# =============================================================================
N_EPOCHS = 20                    # Sá»‘ epoch tá»‘i Ä‘a
CLIP = 1.0                       # Gradient clipping
LEARNING_RATE = 0.001            # Learning rate
PATIENCE = 3                     # Early Stopping patience
TEACHER_FORCING_RATIO = 0.5      # Tá»· lá»‡ Teacher Forcing

# =============================================================================
# HELPER FUNCTION
# =============================================================================
def epoch_time(start_time, end_time):
    """TÃ­nh thá»i gian cháº¡y 1 epoch (phÃºt, giÃ¢y)."""
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

print("âœ… ÄÃ£ cáº¥u hÃ¬nh hyperparameters:")
print(f"   N_EPOCHS = {N_EPOCHS}")
print(f"   LEARNING_RATE = {LEARNING_RATE}")
print(f"   CLIP = {CLIP}")
print(f"   TEACHER_FORCING_RATIO = {TEACHER_FORCING_RATIO}")
print(f"   PATIENCE = {PATIENCE}")


# In[ ]:


# ==============================================================================
# CELL 4.2: HÃ€M TRAIN VÃ€ EVALUATE
# ==============================================================================

def train(model, iterator, optimizer, criterion, clip, device, teacher_forcing_ratio=0.5):
    """
    Huáº¥n luyá»‡n model trong 1 epoch.
    
    Args:
        model: MÃ´ hÃ¬nh Seq2Seq (vá»›i Attention)
        iterator: DataLoader train
        optimizer: Adam optimizer
        criterion: CrossEntropyLoss (vá»›i ignore_index=PAD_IDX)
        clip: Gradient clipping value
        device: 'cuda' hoáº·c 'cpu'
        teacher_forcing_ratio: Tá»· lá»‡ sá»­ dá»¥ng Teacher Forcing
        
    Returns:
        epoch_loss: Loss trung bÃ¬nh cá»§a epoch
    """
    model.train()
    epoch_loss = 0
    
    progress_bar = tqdm(iterator, desc="Training", leave=False)
    
    for batch in progress_bar:
        # ===== 1. UNPACK BATCH =====
        src, trg, src_len = batch
        
        # Chuyá»ƒn src, trg lÃªn device
        src = src.to(device)         # [src_len, batch_size]
        trg = trg.to(device)         # [trg_len, batch_size]
        # âš ï¸ src_len PHáº¢I náº±m trÃªn CPU cho pack_padded_sequence!
        
        # ===== 2. FORWARD PASS =====
        optimizer.zero_grad()
        
        # Forward vá»›i teacher_forcing_ratio
        output = model(src, src_len, trg, teacher_forcing_ratio)
        # output: [trg_len, batch_size, output_dim]
        
        # ===== 3. TÃNH LOSS =====
        # ğŸ“Œ LOGIC SLICING:
        # - output[0] lÃ  zeros tensor (do loop báº¯t Ä‘áº§u tá»« t=1)
        # - trg[0] lÃ  <sos> token
        # - Pháº£i bá» cáº£ hai trÆ°á»›c khi tÃ­nh loss
        output_dim = output.shape[-1]
        
        output = output[1:]   # [trg_len-1, batch_size, output_dim]
        trg = trg[1:]         # [trg_len-1, batch_size]
        
        # Reshape vá» 2D cho CrossEntropyLoss
        output = output.reshape(-1, output_dim)  # [(trg_len-1)*batch_size, output_dim]
        trg = trg.reshape(-1)                    # [(trg_len-1)*batch_size]
        
        loss = criterion(output, trg)
        
        # ===== 4. BACKWARD PASS =====
        loss.backward()
        
        # Gradient clipping Ä‘á»ƒ trÃ¡nh exploding gradient
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return epoch_loss / len(iterator)


def evaluate(model, iterator, criterion, device):
    """
    ÄÃ¡nh giÃ¡ model trÃªn táº­p validation/test.
    
    Args:
        model: MÃ´ hÃ¬nh Seq2Seq
        iterator: DataLoader val/test
        criterion: CrossEntropyLoss
        device: 'cuda' hoáº·c 'cpu'
        
    Returns:
        epoch_loss: Loss trung bÃ¬nh
    """
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(iterator, desc="Evaluating", leave=False):
            src, trg, src_len = batch
            
            src = src.to(device)
            trg = trg.to(device)
            # src_len giá»¯ nguyÃªn trÃªn CPU
            
            # Forward vá»›i teacher_forcing_ratio = 0 (khÃ´ng dÃ¹ng ground truth)
            output = model(src, src_len, trg, teacher_forcing_ratio=0)
            
            output_dim = output.shape[-1]
            
            output = output[1:]
            trg = trg[1:]
            
            output = output.reshape(-1, output_dim)
            trg = trg.reshape(-1)
            
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)


print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a: train(), evaluate()")


# ## 4.1 HUáº¤N LUYá»†N BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)
# 
# ---
# 
# **Má»¥c tiÃªu:** Huáº¥n luyá»‡n mÃ´ hÃ¬nh Baseline Ä‘á»ƒ chá»©ng minh hoáº¡t Ä‘á»™ng cá»§a context vector cá»‘ Ä‘á»‹nh.
# 
# > âš ï¸ **QUAN TRá»ŒNG:** ÄÃ¢y lÃ  bÆ°á»›c Báº®T BUá»˜C Ä‘á»ƒ Ä‘Ã¡p á»©ng tiÃªu chÃ­ 1 (3.0 Ä‘iá»ƒm).
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 4.1a: HUáº¤N LUYá»†N BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)
# ==============================================================================

from tqdm.auto import tqdm
import time
import math

print("=" * 60)
print("ğŸ—ï¸ HUáº¤N LUYá»†N BASELINE SEQ2SEQ (FIXED CONTEXT VECTOR)")
print("=" * 60)
print("ğŸ“Œ MÃ´ hÃ¬nh nÃ y sá»­ dá»¥ng context vector Cá» Äá»ŠNH")
print("   (Chá»‰ hidden state cuá»‘i cÃ¹ng cá»§a Encoder)")
print("=" * 60 + "\n")

# =============================================================================
# Cáº¤U HÃŒNH TRAINING CHO BASELINE
# =============================================================================
BASELINE_EPOCHS = 3              # Äá»§ Ä‘á»ƒ chá»©ng minh mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng
LEARNING_RATE = 0.001
CLIP = 1.0
TEACHER_FORCING_RATIO = 0.5

# Optimizer & Criterion cho Baseline
baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)
baseline_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# History tracking
baseline_history = {
    'train_loss': [],
    'valid_loss': [],
    'train_ppl': [],
    'valid_ppl': []
}

print(f"Epochs:           {BASELINE_EPOCHS}")
print(f"Learning Rate:    {LEARNING_RATE}")
print(f"Gradient Clip:    {CLIP}")
print(f"Teacher Forcing:  {TEACHER_FORCING_RATIO}")
print("=" * 60 + "\n")

# =============================================================================
# TRAINING LOOP CHO BASELINE
# =============================================================================
best_baseline_loss = float('inf')

for epoch in range(BASELINE_EPOCHS):
    start_time = time.time()
    
    # ===== TRAIN =====
    baseline_model.train()
    train_loss = 0
    
    for batch in tqdm(train_loader, desc=f"Baseline Epoch {epoch+1}/{BASELINE_EPOCHS}", leave=False):
        src, trg, src_len = batch
        src = src.to(device)
        trg = trg.to(device)
        
        baseline_optimizer.zero_grad()
        
        # Forward pass
        output = baseline_model(src, src_len, trg, TEACHER_FORCING_RATIO)
        
        # Reshape for loss
        output_dim = output.shape[-1]
        output = output[1:].reshape(-1, output_dim)
        trg = trg[1:].reshape(-1)
        
        # Calculate loss
        loss = baseline_criterion(output, trg)
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(baseline_model.parameters(), CLIP)
        baseline_optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # ===== EVALUATE =====
    baseline_model.eval()
    valid_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(valid_loader, desc="Validating", leave=False):
            src, trg, src_len = batch
            src = src.to(device)
            trg = trg.to(device)
            
            output = baseline_model(src, src_len, trg, 0)  # No teacher forcing
            
            output_dim = output.shape[-1]
            output = output[1:].reshape(-1, output_dim)
            trg = trg[1:].reshape(-1)
            
            loss = baseline_criterion(output, trg)
            valid_loss += loss.item()
    
    valid_loss /= len(valid_loader)
    
    # Calculate perplexity
    train_ppl = math.exp(train_loss)
    valid_ppl = math.exp(valid_loss)
    
    # Save history
    baseline_history['train_loss'].append(train_loss)
    baseline_history['valid_loss'].append(valid_loss)
    baseline_history['train_ppl'].append(train_ppl)
    baseline_history['valid_ppl'].append(valid_ppl)
    
    # Save best model
    if valid_loss < best_baseline_loss:
        best_baseline_loss = valid_loss
        torch.save(baseline_model.state_dict(), 'baseline_model.pth')
        save_status = "âœ… Model saved!"
    else:
        save_status = ""
    
    end_time = time.time()
    epoch_mins = int((end_time - start_time) / 60)
    epoch_secs = int((end_time - start_time) % 60)
    
    print(f"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s")
    print(f"\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}")
    print(f"\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f} {save_status}")
    print("-" * 60)

print("\n" + "=" * 60)
print("ğŸ‰ BASELINE TRAINING HOÃ€N Táº¤T!")
print("=" * 60)
print(f"Best Validation Loss: {best_baseline_loss:.3f}")
print(f"Best Validation PPL:  {math.exp(best_baseline_loss):.3f}")
print(f"Model Ä‘Ã£ lÆ°u táº¡i:     'baseline_model.pth'")
print("=" * 60)


# ## 4.2 HUáº¤N LUYá»†N ATTENTION SEQ2SEQ (MÃ” HÃŒNH CHÃNH)
# 
# ---
# 
# **Má»¥c tiÃªu:** Huáº¥n luyá»‡n mÃ´ hÃ¬nh Seq2Seq + Luong Attention vá»›i Early Stopping.
# 
# **So sÃ¡nh vá»›i Baseline:**
# - Baseline: Context vector Cá» Äá»ŠNH (chá»‰ hidden state cuá»‘i)
# - Attention: Context vector Äá»˜NG (weighted sum cá»§a táº¥t cáº£ encoder outputs)
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 4.3: KHá»I Táº O OPTIMIZER & CRITERION CHO ATTENTION MODEL
# ==============================================================================

# Optimizer: Adam cho attention_model
attention_optimizer = torch.optim.Adam(attention_model.parameters(), lr=LEARNING_RATE)

# Loss function: CrossEntropyLoss vá»›i ignore_index Ä‘á»ƒ bá» qua PAD token
attention_criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# Cáº¥u hÃ¬nh training
N_EPOCHS = 20                    # Sá»‘ epoch tá»‘i Ä‘a
PATIENCE = 3                     # Early Stopping patience

print("=" * 60)
print("ğŸ¯ Cáº¤U HÃŒNH HUáº¤N LUYá»†N ATTENTION MODEL")
print("=" * 60)
print(f"Device:              {device}")
print(f"Model:               {attention_model.__class__.__name__}")
print(f"Total Parameters:    {attention_params:,}")
print("-" * 60)
print(f"Optimizer:           Adam (lr={LEARNING_RATE})")
print(f"Loss:                CrossEntropyLoss (ignore_index={PAD_IDX})")
print(f"Epochs:              {N_EPOCHS} (max)")
print(f"Gradient Clip:       {CLIP}")
print(f"Teacher Forcing:     {TEACHER_FORCING_RATIO}")
print(f"Early Stopping:      patience={PATIENCE}")
print(f"Batch Size:          {BATCH_SIZE}")
print("=" * 60)


# In[ ]:


# ==============================================================================
# CELL 4.4: VÃ’NG Láº¶P HUáº¤N LUYá»†N ATTENTION MODEL (Vá»šI EARLY STOPPING)
# ==============================================================================

# Biáº¿n theo dÃµi
best_valid_loss = float('inf')
epochs_without_improvement = 0
attention_history = {
    'train_loss': [],
    'valid_loss': [],
    'train_ppl': [],
    'valid_ppl': []
}

print("=" * 60)
print("ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N ATTENTION MODEL")
print("=" * 60 + "\n")

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    # ===== TRAIN =====
    attention_model.train()
    train_loss = 0
    
    for batch in tqdm(train_loader, desc=f"Attention Epoch {epoch+1}/{N_EPOCHS}", leave=False):
        src, trg, src_len = batch
        src = src.to(device)
        trg = trg.to(device)
        
        attention_optimizer.zero_grad()
        
        output = attention_model(src, src_len, trg, TEACHER_FORCING_RATIO)
        
        output_dim = output.shape[-1]
        output = output[1:].reshape(-1, output_dim)
        trg = trg[1:].reshape(-1)
        
        loss = attention_criterion(output, trg)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(attention_model.parameters(), CLIP)
        attention_optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # ===== EVALUATE =====
    attention_model.eval()
    valid_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(valid_loader, desc="Validating", leave=False):
            src, trg, src_len = batch
            src = src.to(device)
            trg = trg.to(device)
            
            output = attention_model(src, src_len, trg, 0)
            
            output_dim = output.shape[-1]
            output = output[1:].reshape(-1, output_dim)
            trg = trg[1:].reshape(-1)
            
            loss = attention_criterion(output, trg)
            valid_loss += loss.item()
    
    valid_loss /= len(valid_loader)
    
    end_time = time.time()
    epoch_mins = int((end_time - start_time) / 60)
    epoch_secs = int((end_time - start_time) % 60)
    
    # ===== TÃNH PERPLEXITY =====
    train_ppl = math.exp(train_loss)
    valid_ppl = math.exp(valid_loss)
    
    # LÆ°u history
    attention_history['train_loss'].append(train_loss)
    attention_history['valid_loss'].append(valid_loss)
    attention_history['train_ppl'].append(train_ppl)
    attention_history['valid_ppl'].append(valid_ppl)
    
    # ===== CHECKPOINTING & EARLY STOPPING =====
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        epochs_without_improvement = 0
        
        # LÆ°u best model
        torch.save(attention_model.state_dict(), 'attention_best_model.pth')
        save_status = "âœ… Model saved!"
    else:
        epochs_without_improvement += 1
        save_status = f"âš ï¸ No improvement ({epochs_without_improvement}/{PATIENCE})"
    
    # ===== IN Káº¾T QUáº¢ =====
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')
    print(f'\t{save_status}')
    print("-" * 60)
    
    # ===== KIá»‚M TRA EARLY STOPPING =====
    if epochs_without_improvement >= PATIENCE:
        print("\n" + "=" * 60)
        print(f"â›” EARLY STOPPING: Val loss khÃ´ng giáº£m sau {PATIENCE} epochs")
        print("=" * 60)
        break

# =============================================================================
# Tá»”NG Káº¾T
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ‰ ATTENTION TRAINING HOÃ€N Táº¤T!")
print("=" * 60)
print(f"Epochs Ä‘Ã£ cháº¡y:       {epoch + 1}")
print(f"Best Validation Loss: {best_valid_loss:.3f}")
print(f"Best Validation PPL:  {math.exp(best_valid_loss):.3f}")
print(f"Model Ä‘Ã£ lÆ°u táº¡i:     'attention_best_model.pth'")
print("=" * 60)


# In[ ]:


# ==============================================================================
# CELL 4.5: SO SÃNH Káº¾T QUáº¢ BASELINE VS ATTENTION
# ==============================================================================

print("=" * 60)
print("ğŸ“Š SO SÃNH Káº¾T QUáº¢ TRAINING: BASELINE VS ATTENTION")
print("=" * 60)

# Load best models
baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))
attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))

# Evaluate on test set
def evaluate_model(model, iterator, criterion, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in iterator:
            src, trg, src_len = batch
            src = src.to(device)
            trg = trg.to(device)
            output = model(src, src_len, trg, 0)
            output_dim = output.shape[-1]
            output = output[1:].reshape(-1, output_dim)
            trg = trg[1:].reshape(-1)
            loss = criterion(output, trg)
            total_loss += loss.item()
    return total_loss / len(iterator)

# Evaluate both models
baseline_test_loss = evaluate_model(baseline_model, test_loader, baseline_criterion, device)
attention_test_loss = evaluate_model(attention_model, test_loader, attention_criterion, device)

print("\n" + "-" * 60)
print("ğŸ“Œ BASELINE SEQ2SEQ (Fixed Context Vector):")
print(f"   Test Loss: {baseline_test_loss:.3f} | Test PPL: {math.exp(baseline_test_loss):7.3f}")
print("\n" + "-" * 60)
print("ğŸ¯ ATTENTION SEQ2SEQ (Dynamic Context Vector):")
print(f"   Test Loss: {attention_test_loss:.3f} | Test PPL: {math.exp(attention_test_loss):7.3f}")
print("\n" + "-" * 60)

# Improvement
improvement = baseline_test_loss - attention_test_loss
ppl_improvement = math.exp(baseline_test_loss) - math.exp(attention_test_loss)
print(f"ğŸ“ˆ Cáº£i thiá»‡n khi dÃ¹ng Attention:")
print(f"   Loss giáº£m:       {improvement:.3f}")
print(f"   Perplexity giáº£m: {ppl_improvement:.3f}")
print("=" * 60)
print("\nâœ… PHáº¦N 4 HOÃ€N Táº¤T - Sáºµn sÃ ng cho PHáº¦N 5 (Inference & Evaluation)!")


# # ğŸ“Š PHáº¦N 5: INFERENCE & BLEU EVALUATION
# 
# ---
# 
# ## Quy trÃ¬nh Inference
# 
# ```
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                        GREEDY DECODING                              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚                                                                     â”‚
# â”‚  Input (EN): "A man walks"                                          â”‚
# â”‚       â†“                                                             â”‚
# â”‚  Tokenize + Numericalize                                            â”‚
# â”‚       â†“                                                             â”‚
# â”‚  Encoder â†’ (encoder_outputs, hidden, cell)                          â”‚
# â”‚       â†“                                                             â”‚
# â”‚  Loop until <eos> or MAX_LEN:                                       â”‚
# â”‚       â”œâ”€â”€ Decoder(input, hidden, cell, encoder_outputs, mask)       â”‚
# â”‚       â”œâ”€â”€ argmax(output) â†’ predicted token                          â”‚
# â”‚       â””â”€â”€ Append to result                                          â”‚
# â”‚       â†“                                                             â”‚
# â”‚  Output (FR): "Un homme marche"                                     â”‚
# â”‚                                                                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# **BLEU Score Evaluation:**
# - Sá»­ dá»¥ng `nltk.translate.bleu_score`
# - Smoothing function Ä‘á»ƒ xá»­ lÃ½ n-gram = 0
# - ÄÃ¡nh giÃ¡ trÃªn toÃ n bá»™ táº­p Test
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 5.1: HÃ€M TRANSLATE (CHO Cáº¢ BASELINE VÃ€ ATTENTION)
# ==============================================================================

def translate_attention(sentence: str) -> str:
    """
    Dá»‹ch má»™t cÃ¢u tiáº¿ng Anh sang tiáº¿ng PhÃ¡p báº±ng Attention Model.
    Sá»­ dá»¥ng Greedy Decoding.
    """
    MAX_LEN = 50
    attention_model.eval()
    
    # Tokenize
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    
    # Numericalize
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)
    
    # Encoder forward
    with torch.no_grad():
        encoder_outputs, hidden, cell = attention_model.encoder(src_tensor, src_len)
    
    # Mask
    mask = (src_tensor != PAD_IDX).permute(1, 0)
    
    # Greedy decoding
    trg_indexes = [SOS_IDX]
    
    for _ in range(MAX_LEN):
        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
        
        with torch.no_grad():
            output, hidden, cell, attention = attention_model.decoder(
                trg_tensor, hidden, cell, encoder_outputs, mask
            )
        
        pred_token = output.argmax(1).item()
        trg_indexes.append(pred_token)
        
        if pred_token == EOS_IDX:
            break
    
    # Convert to words
    trg_tokens = [vocab_fr.get_itos()[i] for i in trg_indexes]
    
    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]
    
    return ' '.join(trg_tokens)


def translate_baseline(sentence: str) -> str:
    """
    Dá»‹ch má»™t cÃ¢u tiáº¿ng Anh sang tiáº¿ng PhÃ¡p báº±ng Baseline Model.
    Sá»­ dá»¥ng Greedy Decoding vá»›i FIXED context vector.
    """
    MAX_LEN = 50
    baseline_model.eval()
    
    # Tokenize
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    
    # Numericalize
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)
    
    # Encoder forward (baseline chá»‰ tráº£ vá» hidden, cell)
    with torch.no_grad():
        hidden, cell = baseline_model.encoder(src_tensor, src_len)
    
    # Greedy decoding
    trg_indexes = [SOS_IDX]
    
    for _ in range(MAX_LEN):
        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
        
        with torch.no_grad():
            output, hidden, cell = baseline_model.decoder(
                trg_tensor, hidden, cell
            )
        
        pred_token = output.argmax(1).item()
        trg_indexes.append(pred_token)
        
        if pred_token == EOS_IDX:
            break
    
    # Convert to words
    trg_tokens = [vocab_fr.get_itos()[i] for i in trg_indexes]
    
    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]
    
    return ' '.join(trg_tokens)


# Alias cho backward compatibility
translate = translate_attention

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m translate_attention() vÃ  translate_baseline()")


# In[ ]:


# ==============================================================================
# CELL 5.2: TÃNH BLEU SCORE (NLTK)
# ==============================================================================

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def calculate_bleu_score(test_src, test_trg, num_samples=None):
    """
    TÃ­nh Ä‘iá»ƒm BLEU trung bÃ¬nh trÃªn táº­p Test.
    
    Args:
        test_src: List cÃ¢u nguá»“n (tiáº¿ng Anh)
        test_trg: List cÃ¢u Ä‘Ã­ch (tiáº¿ng PhÃ¡p)
        num_samples: Sá»‘ máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ (None = toÃ n bá»™)
        
    Returns:
        bleu_avg: Äiá»ƒm BLEU trung bÃ¬nh (0-1)
    """
    # Smoothing Ä‘á»ƒ xá»­ lÃ½ n-gram = 0
    smooth = SmoothingFunction().method1
    
    total_bleu = 0
    count = 0
    
    # Giá»›i háº¡n sá»‘ máº«u náº¿u cáº§n
    if num_samples:
        indices = random.sample(range(len(test_src)), min(num_samples, len(test_src)))
    else:
        indices = range(len(test_src))
    
    print("ğŸ“Š Äang tÃ­nh BLEU Score...")
    
    for idx in tqdm(indices, desc="Calculating BLEU"):
        src_sentence = test_src[idx]
        trg_sentence = test_trg[idx]
        
        # Dá»‹ch cÃ¢u
        pred_sentence = translate(src_sentence)
        
        # Tokenize
        pred_tokens = pred_sentence.split()
        ref_tokens = tokenizer_fr(trg_sentence.lower())
        
        # NLTK sentence_bleu format
        reference = [ref_tokens]  # List of list
        hypothesis = pred_tokens
        
        try:
            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth)
            total_bleu += bleu
            count += 1
        except:
            continue
    
    bleu_avg = total_bleu / count if count > 0 else 0
    return bleu_avg


def demo_translation(test_src, test_trg, num_examples=5):
    """
    Demo dá»‹ch má»™t sá»‘ cÃ¢u ngáº«u nhiÃªn tá»« táº­p Test.
    """
    print("\n" + "=" * 70)
    print("ğŸ” DEMO Dá»ŠCH MáºªU")
    print("=" * 70)
    
    # Chá»n ngáº«u nhiÃªn
    indices = random.sample(range(len(test_src)), num_examples)
    
    for i, idx in enumerate(indices, 1):
        src = test_src[idx]
        trg = test_trg[idx]
        pred = translate(src)
        
        print(f"\n--- VÃ­ dá»¥ {i} ---")
        print(f"ğŸ“¥ Source (EN):     {src}")
        print(f"ğŸ“Œ Reference (FR):  {trg}")
        print(f"ğŸ¤– Predicted (FR):  {pred}")
    
    print("\n" + "=" * 70)


print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a: calculate_bleu_score(), demo_translation()")


# In[ ]:


# ==============================================================================
# CELL 5.3: DEMO SO SÃNH BASELINE VS ATTENTION
# ==============================================================================

print("=" * 70)
print("ğŸ“Š DEMO SO SÃNH: BASELINE VS ATTENTION")
print("=" * 70)

# Load best models
baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))
attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))
baseline_model.eval()
attention_model.eval()
print("âœ… ÄÃ£ load cáº£ hai models\n")

# Demo 5 cÃ¢u ngáº«u nhiÃªn
indices = random.sample(range(len(test_en)), 5)

for i, idx in enumerate(indices, 1):
    src = test_en[idx]
    trg = test_fr[idx]
    pred_baseline = translate_baseline(src)
    pred_attention = translate_attention(src)
    
    print(f"\n--- VÃ­ dá»¥ {i} ---")
    print(f"ğŸ“¥ Source (EN):     {src}")
    print(f"ğŸ“Œ Reference (FR):  {trg}")
    print(f"ğŸ—ï¸ Baseline (FR):   {pred_baseline}")
    print(f"ğŸ¯ Attention (FR):  {pred_attention}")

print("\n" + "=" * 70)


# In[ ]:


# ==============================================================================
# CELL 5.4: ÄÃNH GIÃ BLEU SCORE - BASELINE VS ATTENTION
# ==============================================================================

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def calculate_bleu(model_translate_fn, test_src, test_trg, num_samples=None):
    """
    TÃ­nh Ä‘iá»ƒm BLEU trung bÃ¬nh cho má»™t hÃ m translate.
    """
    smooth = SmoothingFunction().method1
    total_bleu = 0
    count = 0
    
    if num_samples:
        indices = random.sample(range(len(test_src)), min(num_samples, len(test_src)))
    else:
        indices = range(len(test_src))
    
    for idx in tqdm(indices, desc="Calculating BLEU"):
        src_sentence = test_src[idx]
        trg_sentence = test_trg[idx]
        
        pred_sentence = model_translate_fn(src_sentence)
        
        pred_tokens = pred_sentence.split()
        ref_tokens = tokenizer_fr(trg_sentence.lower())
        
        reference = [ref_tokens]
        hypothesis = pred_tokens
        
        try:
            bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth)
            total_bleu += bleu
            count += 1
        except:
            continue
    
    return total_bleu / count if count > 0 else 0


print("=" * 70)
print("ğŸ“Š ÄÃNH GIÃ BLEU SCORE: BASELINE VS ATTENTION")
print("=" * 70)

# TÃ­nh BLEU cho Baseline
print("\nğŸ—ï¸ TÃ­nh BLEU cho BASELINE...")
baseline_bleu = calculate_bleu(translate_baseline, test_en, test_fr, num_samples=None)

# TÃ­nh BLEU cho Attention
print("\nğŸ¯ TÃ­nh BLEU cho ATTENTION...")
attention_bleu = calculate_bleu(translate_attention, test_en, test_fr, num_samples=None)

print("\n" + "=" * 70)
print("ğŸ“Š Káº¾T QUáº¢ BLEU SCORE")
print("=" * 70)
print(f"ğŸ—ï¸ BASELINE (Fixed Context):   {baseline_bleu * 100:.2f}%")
print(f"ğŸ¯ ATTENTION (Dynamic Context): {attention_bleu * 100:.2f}%")
print("-" * 70)
print(f"ğŸ“ˆ Cáº£i thiá»‡n vá»›i Attention:     {(attention_bleu - baseline_bleu) * 100:.2f}%")
print("=" * 70)

# Báº£ng Ä‘Ã¡nh giÃ¡
print("""
ğŸ“Š HÆ¯á»šNG DáºªN ÄÃNH GIÃ BLEU SCORE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BLEU Score      â”‚ ÄÃ¡nh giÃ¡                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ < 10%           â”‚ KÃ©m - Gáº§n nhÆ° vÃ´ nghÄ©a           â”‚
â”‚ 10% - 19%       â”‚ Yáº¿u - KhÃ³ hiá»ƒu                   â”‚
â”‚ 20% - 29%       â”‚ Trung bÃ¬nh - Hiá»ƒu Ã½ chÃ­nh        â”‚
â”‚ 30% - 40%       â”‚ KhÃ¡ - Cháº¥t lÆ°á»£ng cháº¥p nháº­n Ä‘Æ°á»£c  â”‚
â”‚ 40% - 50%       â”‚ Tá»‘t - Cháº¥t lÆ°á»£ng cao             â”‚
â”‚ > 50%           â”‚ Ráº¥t tá»‘t - Gáº§n vá»›i ngÆ°á»i dá»‹ch     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ Káº¿t luáº­n: 
- Baseline Seq2Seq vá»›i Fixed Context Vector hoáº¡t Ä‘á»™ng, nhÆ°ng háº¡n cháº¿.
- Attention Seq2Seq cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ nhá» Dynamic Context Vector.
""")


# In[ ]:


# ==============================================================================
# CELL 5.5: Dá»ŠCH CÃ‚U TÃ™Y Ã
# ==============================================================================

print("=" * 70)
print("ğŸ–Šï¸ Dá»ŠCH CÃ‚U TÃ™Y Ã")
print("=" * 70)

test_sentences = [
    "I love machine learning.",
    "The weather is beautiful today.",
    "A man is walking with his dog.",
    "Two children are playing in the park.",
    "A woman is reading a book.",
]

for sentence in test_sentences:
    result = translate(sentence)
    print(f"\nğŸ“¥ EN: {sentence}")
    print(f"ğŸ‡«ğŸ‡· FR: {result}")

print("\n" + "=" * 70)
print("\nâœ… PHáº¦N 5 HOÃ€N Táº¤T - Sáºµn sÃ ng cho PHáº¦N 6 (Analysis)!")


# In[ ]:


# ==============================================================================
# CELL 4.6: Váº¼ BIá»‚U Äá»’ TRAINING HISTORY (Optional)
# ==============================================================================

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot Loss
axes[0].plot(training_history['train_loss'], label='Train Loss', marker='o')
axes[0].plot(training_history['valid_loss'], label='Valid Loss', marker='s')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training & Validation Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot Perplexity
axes[1].plot(training_history['train_ppl'], label='Train PPL', marker='o')
axes[1].plot(training_history['valid_ppl'], label='Valid PPL', marker='s')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Perplexity')
axes[1].set_title('Training & Validation Perplexity')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150)
plt.show()

print("âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ táº¡i 'training_history.png'")


# # ğŸ“Š PHáº¦N 6: CÃC HÃ€M Bá»” SUNG CHO BÃO CÃO
# 
# ---
# 
# ## CÃ¡c cÃ´ng cá»¥ phÃ¢n tÃ­ch nÃ¢ng cao
# 
# **Bao gá»“m:**
# 1. `plot_history()` - Váº½ biá»ƒu Ä‘á»“ Loss vá»›i highlight epoch tá»‘t nháº¥t
# 2. `analyze_model_performance()` - PhÃ¢n tÃ­ch lá»—i vá»›i BLEU score
# 3. `translate_beam_search()` - Beam Search decoding (Ä‘iá»ƒm cá»™ng)
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 6.1: HÃ€M Váº¼ BIá»‚U Äá»’ LOSS
# ==============================================================================

import matplotlib.pyplot as plt

def plot_history(history, title="Training History", filename="training_history.png"):
    """
    Váº½ biá»ƒu Ä‘á»“ Train Loss vÃ  Valid Loss tá»« history.
    
    Args:
        history: Dict chá»©a 'train_loss' vÃ  'valid_loss' (list)
        title: TiÃªu Ä‘á» biá»ƒu Ä‘á»“
        filename: TÃªn file PNG Ä‘á»ƒ lÆ°u
    
    Returns:
        None (lÆ°u file PNG)
    """
    plt.figure(figsize=(10, 6))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)
    plt.plot(epochs, history['valid_loss'], 'r-s', label='Valid Loss', linewidth=2, markersize=6)
    
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(epochs)
    
    # Highlight best epoch
    best_epoch = history['valid_loss'].index(min(history['valid_loss'])) + 1
    best_loss = min(history['valid_loss'])
    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best: Epoch {best_epoch}')
    plt.scatter([best_epoch], [best_loss], color='green', s=100, zorder=5, marker='*')
    
    plt.tight_layout()
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {filename}")
    print(f"   Best Epoch: {best_epoch} | Best Valid Loss: {best_loss:.4f}")

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m plot_history()")


# In[ ]:


# ==============================================================================
# CELL 6.2: HÃ€M PHÃ‚N TÃCH HIá»†U SUáº¤T MÃ” HÃŒNH
# ==============================================================================

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def analyze_model_performance(translate_fn, src_sentences, trg_sentences, num_examples=5):
    """
    PhÃ¢n tÃ­ch hiá»‡u suáº¥t mÃ´ hÃ¬nh: tÃ¬m cÃ¢u BLEU cao nháº¥t vÃ  tháº¥p nháº¥t.
    
    Args:
        translate_fn: HÃ m dá»‹ch (translate_attention hoáº·c translate_baseline)
        src_sentences: List cÃ¢u nguá»“n (tiáº¿ng Anh)
        trg_sentences: List cÃ¢u Ä‘Ã­ch ground truth (tiáº¿ng PhÃ¡p)
        num_examples: Sá»‘ cÃ¢u hiá»ƒn thá»‹ cho má»—i nhÃ³m (cao/tháº¥p)
    
    Returns:
        Dict chá»©a 'best' vÃ  'worst' examples
    """
    smooth = SmoothingFunction().method1
    results = []
    
    print("=" * 80)
    print("ğŸ“Š PHÃ‚N TÃCH HIá»†U SUáº¤T MÃ” HÃŒNH")
    print("=" * 80)
    print(f"Äang Ä‘Ã¡nh giÃ¡ {len(src_sentences)} cÃ¢u...")
    
    for idx in tqdm(range(len(src_sentences)), desc="Analyzing"):
        src = src_sentences[idx]
        trg = trg_sentences[idx]
        pred = translate_fn(src)
        
        # TÃ­nh BLEU
        pred_tokens = pred.split()
        ref_tokens = tokenizer_fr(trg.lower())
        
        try:
            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
        except:
            bleu = 0.0
        
        results.append({
            'idx': idx,
            'src': src,
            'trg': trg,
            'pred': pred,
            'bleu': bleu
        })
    
    # Sáº¯p xáº¿p theo BLEU
    results_sorted = sorted(results, key=lambda x: x['bleu'], reverse=True)
    
    best_examples = results_sorted[:num_examples]
    worst_examples = results_sorted[-num_examples:]
    
    # TÃ­nh thá»‘ng kÃª
    bleu_scores = [r['bleu'] for r in results]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)
    
    print("\n" + "-" * 80)
    print(f"ğŸ“ˆ THá»NG KÃŠ Tá»”NG QUAN")
    print("-" * 80)
    print(f"   Tá»•ng sá»‘ cÃ¢u:     {len(results)}")
    print(f"   BLEU trung bÃ¬nh: {avg_bleu * 100:.2f}%")
    print(f"   BLEU cao nháº¥t:   {max(bleu_scores) * 100:.2f}%")
    print(f"   BLEU tháº¥p nháº¥t:  {min(bleu_scores) * 100:.2f}%")
    
    # In cÃ¢u BLEU cao nháº¥t
    print("\n" + "=" * 80)
    print(f"ğŸ† TOP {num_examples} CÃ‚U BLEU CAO NHáº¤T")
    print("=" * 80)
    
    for i, ex in enumerate(best_examples, 1):
        print(f"\n--- Rank {i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")
    
    # In cÃ¢u BLEU tháº¥p nháº¥t
    print("\n" + "=" * 80)
    print(f"âš ï¸ TOP {num_examples} CÃ‚U BLEU THáº¤P NHáº¤T")
    print("=" * 80)
    
    for i, ex in enumerate(worst_examples, 1):
        print(f"\n--- Rank {len(results) - num_examples + i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")
    
    print("\n" + "=" * 80)
    
    return {
        'avg_bleu': avg_bleu,
        'best': best_examples,
        'worst': worst_examples,
        'all_results': results
    }

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m analyze_model_performance()")


# In[ ]:


# ==============================================================================
# CELL 6.3: HÃ€M BEAM SEARCH DECODING (ÄIá»‚M Cá»˜NG)
# ==============================================================================

def translate_beam_search(sentence, model, beam_size=3, max_len=50):
    """
    Dá»‹ch cÃ¢u sá»­ dá»¥ng Beam Search Decoding (cho Attention Model).
    
    Args:
        sentence: CÃ¢u tiáº¿ng Anh cáº§n dá»‹ch
        model: Attention model (Seq2SeqAttention)
        beam_size: Sá»‘ beam giá»¯ láº¡i má»—i bÆ°á»›c
        max_len: Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u dá»‹ch
    
    Returns:
        str: CÃ¢u tiáº¿ng PhÃ¡p Ä‘Æ°á»£c dá»‹ch
    """
    model.eval()
    
    # Tokenize vÃ  numericalize
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)
    
    # Encode
    with torch.no_grad():
        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)
    
    # Mask
    mask = (src_tensor != PAD_IDX).permute(1, 0)
    
    # Beam Search initialization
    beams = [(0.0, [SOS_IDX], hidden, cell)]
    completed = []
    
    for _ in range(max_len):
        new_beams = []
        
        for score, tokens, h, c in beams:
            if tokens[-1] == EOS_IDX:
                completed.append((score, tokens))
                continue
            
            trg_tensor = torch.LongTensor([tokens[-1]]).to(device)
            
            with torch.no_grad():
                output, new_h, new_c, _ = model.decoder(trg_tensor, h, c, encoder_outputs, mask)
            
            # Log probabilities
            log_probs = F.log_softmax(output, dim=1)
            
            # Top-k candidates
            topk_log_probs, topk_indices = log_probs.topk(beam_size)
            
            for i in range(beam_size):
                new_score = score + topk_log_probs[0, i].item()
                new_token = topk_indices[0, i].item()
                new_tokens = tokens + [new_token]
                new_beams.append((new_score, new_tokens, new_h, new_c))
        
        # Keep top beams
        new_beams.sort(key=lambda x: x[0], reverse=True)
        beams = new_beams[:beam_size]
        
        if len(beams) == 0:
            break
    
    # Add remaining beams to completed
    for score, tokens, h, c in beams:
        completed.append((score, tokens))
    
    # Chá»n beam cÃ³ score cao nháº¥t (normalize by length)
    if completed:
        completed.sort(key=lambda x: x[0] / len(x[1]), reverse=True)
        best_tokens = completed[0][1]
    else:
        best_tokens = [SOS_IDX]
    
    # Convert to words
    trg_tokens = [vocab_fr.get_itos()[i] for i in best_tokens]
    
    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]
    
    return ' '.join(trg_tokens)

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m translate_beam_search()")


# In[ ]:


# ==============================================================================
# CELL 6.4: DEMO Sá»¬ Dá»¤NG CÃC HÃ€M Má»šI
# ==============================================================================

print("=" * 80)
print("ğŸš€ DEMO CÃC HÃ€M Bá»” SUNG")
print("=" * 80)

# 1. Váº½ biá»ƒu Ä‘á»“ Loss
print("\nğŸ“Š 1. Váº¼ BIá»‚U Äá»’ LOSS")
print("-" * 40)

if baseline_history['train_loss']:
    plot_history(baseline_history, "Baseline Seq2Seq", "baseline_loss.png")

if attention_history['train_loss']:
    plot_history(attention_history, "Attention Seq2Seq", "attention_loss.png")

# 2. PhÃ¢n tÃ­ch lá»—i (trÃªn subset nhá» Ä‘á»ƒ demo)
print("\nğŸ“Š 2. PHÃ‚N TÃCH Lá»–I (DEMO - 100 CÃ‚U)")
print("-" * 40)

# Load best models
baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))
attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))

print("\nğŸ¯ PhÃ¢n tÃ­ch ATTENTION MODEL:")
attention_analysis = analyze_model_performance(
    translate_attention, 
    test_en[:100],  # Chá»‰ 100 cÃ¢u Ä‘á»ƒ demo
    test_fr[:100], 
    num_examples=3
)

# 3. Demo Beam Search
print("\nğŸ“Š 3. DEMO BEAM SEARCH")
print("-" * 40)

demo_sentences = [
    "A man is walking with his dog.",
    "Two children are playing in the park."
]

print("\nğŸ” So sÃ¡nh Greedy vs Beam Search:")
for sent in demo_sentences:
    greedy = translate_attention(sent)
    beam = translate_beam_search(sent, attention_model, beam_size=3)
    
    print(f"\nğŸ“¥ EN: {sent}")
    print(f"   Greedy:      {greedy}")
    print(f"   Beam (k=3):  {beam}")

print("\n" + "=" * 80)
print("âœ… HOÃ€N Táº¤T Táº¤T Cáº¢ DEMO!")
print("=" * 80)


# # ğŸ“Š PHáº¦N 6: CÃC HÃ€M Bá»” SUNG CHO BÃO CÃO
# 
# ---
# 
# ## CÃ¡c cÃ´ng cá»¥ phÃ¢n tÃ­ch nÃ¢ng cao
# 
# **Bao gá»“m:**
# 1. `plot_history()` - Váº½ biá»ƒu Ä‘á»“ Loss vá»›i highlight epoch tá»‘t nháº¥t
# 2. `analyze_model_performance()` - PhÃ¢n tÃ­ch lá»—i vá»›i BLEU score
# 3. `translate_beam_search()` - Beam Search decoding (Ä‘iá»ƒm cá»™ng)
# 
# ---

# In[ ]:


# ==============================================================================
# CELL 6.1: HÃ€M Váº¼ BIá»‚U Äá»’ LOSS
# ==============================================================================

import matplotlib.pyplot as plt

def plot_history(history, title="Training History", filename="training_history.png"):
    """
    Váº½ biá»ƒu Ä‘á»“ Train Loss vÃ  Valid Loss tá»« history.
    
    Args:
        history: Dict chá»©a 'train_loss' vÃ  'valid_loss' (list)
        title: TiÃªu Ä‘á» biá»ƒu Ä‘á»“
        filename: TÃªn file PNG Ä‘á»ƒ lÆ°u
    
    Returns:
        None (lÆ°u file PNG)
    """
    plt.figure(figsize=(10, 6))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)
    plt.plot(epochs, history['valid_loss'], 'r-s', label='Valid Loss', linewidth=2, markersize=6)
    
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(epochs)
    
    # Highlight best epoch
    best_epoch = history['valid_loss'].index(min(history['valid_loss'])) + 1
    best_loss = min(history['valid_loss'])
    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best: Epoch {best_epoch}')
    plt.scatter([best_epoch], [best_loss], color='green', s=100, zorder=5, marker='*')
    
    plt.tight_layout()
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {filename}")
    print(f"   Best Epoch: {best_epoch} | Best Valid Loss: {best_loss:.4f}")

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m plot_history()")


# In[ ]:


# ==============================================================================
# CELL 6.2: HÃ€M PHÃ‚N TÃCH HIá»†U SUáº¤T MÃ” HÃŒNH
# ==============================================================================

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def analyze_model_performance(translate_fn, src_sentences, trg_sentences, num_examples=5):
    """
    PhÃ¢n tÃ­ch hiá»‡u suáº¥t mÃ´ hÃ¬nh: tÃ¬m cÃ¢u BLEU cao nháº¥t vÃ  tháº¥p nháº¥t.
    
    Args:
        translate_fn: HÃ m dá»‹ch (translate_attention hoáº·c translate_baseline)
        src_sentences: List cÃ¢u nguá»“n (tiáº¿ng Anh)
        trg_sentences: List cÃ¢u Ä‘Ã­ch ground truth (tiáº¿ng PhÃ¡p)
        num_examples: Sá»‘ cÃ¢u hiá»ƒn thá»‹ cho má»—i nhÃ³m (cao/tháº¥p)
    
    Returns:
        Dict chá»©a 'best' vÃ  'worst' examples
    """
    smooth = SmoothingFunction().method1
    results = []
    
    print("=" * 80)
    print("ğŸ“Š PHÃ‚N TÃCH HIá»†U SUáº¤T MÃ” HÃŒNH")
    print("=" * 80)
    print(f"Äang Ä‘Ã¡nh giÃ¡ {len(src_sentences)} cÃ¢u...")
    
    for idx in tqdm(range(len(src_sentences)), desc="Analyzing"):
        src = src_sentences[idx]
        trg = trg_sentences[idx]
        pred = translate_fn(src)
        
        # TÃ­nh BLEU
        pred_tokens = pred.split()
        ref_tokens = tokenizer_fr(trg.lower())
        
        try:
            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth)
        except:
            bleu = 0.0
        
        results.append({
            'idx': idx,
            'src': src,
            'trg': trg,
            'pred': pred,
            'bleu': bleu
        })
    
    # Sáº¯p xáº¿p theo BLEU
    results_sorted = sorted(results, key=lambda x: x['bleu'], reverse=True)
    
    best_examples = results_sorted[:num_examples]
    worst_examples = results_sorted[-num_examples:]
    
    # TÃ­nh thá»‘ng kÃª
    bleu_scores = [r['bleu'] for r in results]
    avg_bleu = sum(bleu_scores) / len(bleu_scores)
    
    print("\n" + "-" * 80)
    print(f"ğŸ“ˆ THá»NG KÃŠ Tá»”NG QUAN")
    print("-" * 80)
    print(f"   Tá»•ng sá»‘ cÃ¢u:     {len(results)}")
    print(f"   BLEU trung bÃ¬nh: {avg_bleu * 100:.2f}%")
    print(f"   BLEU cao nháº¥t:   {max(bleu_scores) * 100:.2f}%")
    print(f"   BLEU tháº¥p nháº¥t:  {min(bleu_scores) * 100:.2f}%")
    
    # In cÃ¢u BLEU cao nháº¥t
    print("\n" + "=" * 80)
    print(f"ğŸ† TOP {num_examples} CÃ‚U BLEU CAO NHáº¤T")
    print("=" * 80)
    
    for i, ex in enumerate(best_examples, 1):
        print(f"\n--- Rank {i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")
    
    # In cÃ¢u BLEU tháº¥p nháº¥t
    print("\n" + "=" * 80)
    print(f"âš ï¸ TOP {num_examples} CÃ‚U BLEU THáº¤P NHáº¤T")
    print("=" * 80)
    
    for i, ex in enumerate(worst_examples, 1):
        print(f"\n--- Rank {len(results) - num_examples + i} | BLEU: {ex['bleu'] * 100:.2f}% ---")
        print(f"   EN (Source):    {ex['src']}")
        print(f"   FR (Reference): {ex['trg']}")
        print(f"   FR (Predicted): {ex['pred']}")
    
    print("\n" + "=" * 80)
    
    return {
        'avg_bleu': avg_bleu,
        'best': best_examples,
        'worst': worst_examples,
        'all_results': results
    }

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m analyze_model_performance()")


# In[ ]:


# ==============================================================================
# CELL 6.3: HÃ€M BEAM SEARCH DECODING (ÄIá»‚M Cá»˜NG)
# ==============================================================================

def translate_beam_search(sentence, model, beam_size=3, max_len=50):
    """
    Dá»‹ch cÃ¢u sá»­ dá»¥ng Beam Search Decoding (cho Attention Model).
    
    Args:
        sentence: CÃ¢u tiáº¿ng Anh cáº§n dá»‹ch
        model: Attention model (Seq2SeqAttention)
        beam_size: Sá»‘ beam giá»¯ láº¡i má»—i bÆ°á»›c
        max_len: Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u dá»‹ch
    
    Returns:
        str: CÃ¢u tiáº¿ng PhÃ¡p Ä‘Æ°á»£c dá»‹ch
    """
    model.eval()
    
    # Tokenize vÃ  numericalize
    tokens = tokenizer_en(sentence.lower())
    tokens = ['<sos>'] + tokens + ['<eos>']
    src_indexes = [vocab_en[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.tensor([len(src_indexes)], dtype=torch.long)
    
    # Encode
    with torch.no_grad():
        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)
    
    # Mask
    mask = (src_tensor != PAD_IDX).permute(1, 0)
    
    # Beam Search initialization
    beams = [(0.0, [SOS_IDX], hidden, cell)]
    completed = []
    
    for _ in range(max_len):
        new_beams = []
        
        for score, tokens, h, c in beams:
            if tokens[-1] == EOS_IDX:
                completed.append((score, tokens))
                continue
            
            trg_tensor = torch.LongTensor([tokens[-1]]).to(device)
            
            with torch.no_grad():
                output, new_h, new_c, _ = model.decoder(trg_tensor, h, c, encoder_outputs, mask)
            
            # Log probabilities
            log_probs = F.log_softmax(output, dim=1)
            
            # Top-k candidates
            topk_log_probs, topk_indices = log_probs.topk(beam_size)
            
            for i in range(beam_size):
                new_score = score + topk_log_probs[0, i].item()
                new_token = topk_indices[0, i].item()
                new_tokens = tokens + [new_token]
                new_beams.append((new_score, new_tokens, new_h, new_c))
        
        # Keep top beams
        new_beams.sort(key=lambda x: x[0], reverse=True)
        beams = new_beams[:beam_size]
        
        if len(beams) == 0:
            break
    
    # Add remaining beams to completed
    for score, tokens, h, c in beams:
        completed.append((score, tokens))
    
    # Chá»n beam cÃ³ score cao nháº¥t (normalize by length)
    if completed:
        completed.sort(key=lambda x: x[0] / len(x[1]), reverse=True)
        best_tokens = completed[0][1]
    else:
        best_tokens = [SOS_IDX]
    
    # Convert to words
    trg_tokens = [vocab_fr.get_itos()[i] for i in best_tokens]
    
    if trg_tokens[0] == '<sos>':
        trg_tokens = trg_tokens[1:]
    if '<eos>' in trg_tokens:
        trg_tokens = trg_tokens[:trg_tokens.index('<eos>')]
    
    return ' '.join(trg_tokens)

print("âœ… ÄÃ£ Ä‘á»‹nh nghÄ©a hÃ m translate_beam_search()")
# In[ ]:


# ==============================================================================
# CELL 6.4: DEMO Sá»¬ Dá»¤NG CÃC HÃ€M Má»šI
# ==============================================================================

print("=" * 80)
print("ğŸš€ DEMO CÃC HÃ€M Bá»” SUNG")
print("=" * 80)

# 1. Váº½ biá»ƒu Ä‘á»“ Loss
print("\nğŸ“Š 1. Váº¼ BIá»‚U Äá»’ LOSS")
print("-" * 40)

if baseline_history['train_loss']:
    plot_history(baseline_history, "Baseline Seq2Seq", "baseline_loss.png")

if attention_history['train_loss']:
    plot_history(attention_history, "Attention Seq2Seq", "attention_loss.png")

# 2. PhÃ¢n tÃ­ch lá»—i (trÃªn subset nhá» Ä‘á»ƒ demo)
print("\nğŸ“Š 2. PHÃ‚N TÃCH Lá»–I (DEMO - 100 CÃ‚U)")
print("-" * 40)

# Load best models
baseline_model.load_state_dict(torch.load('baseline_model.pth', map_location=device, weights_only=True))
attention_model.load_state_dict(torch.load('attention_best_model.pth', map_location=device, weights_only=True))

print("\nğŸ¯ PhÃ¢n tÃ­ch ATTENTION MODEL:")
attention_analysis = analyze_model_performance(
    translate_attention, 
    test_en[:100],  # Chá»‰ 100 cÃ¢u Ä‘á»ƒ demo
    test_fr[:100], 
    num_examples=3
)

# 3. Demo Beam Search
print("\nğŸ“Š 3. DEMO BEAM SEARCH")
print("-" * 40)

demo_sentences = [
    "A man is walking with his dog.",
    "Two children are playing in the park."
]

print("\nğŸ” So sÃ¡nh Greedy vs Beam Search:")
for sent in demo_sentences:
    greedy = translate_attention(sent)
    beam = translate_beam_search(sent, attention_model, beam_size=3)
    
    print(f"\nğŸ“¥ EN: {sent}")
    print(f"   Greedy:      {greedy}")
    print(f"   Beam (k=3):  {beam}")

print("\n" + "=" * 80)
print("âœ… HOÃ€N Táº¤T Táº¤T Cáº¢ DEMO!")
print("=" * 80)

