"""
==============================================================================
PH·∫¶N 3: TRAINING PROCESS (C√≥ Early Stopping)
==============================================================================
Tu√¢n th·ªß y√™u c·∫ßu ƒë·ªì √°n:
- Early Stopping sau 3 epoch kh√¥ng c·∫£i thi·ªán
- Teacher Forcing ratio = 0.5
- Gradient Clipping
- Checkpoint best model
==============================================================================
"""

import time
import math
import torch
import torch.nn as nn
from tqdm import tqdm

# ==============================================================================
# 1. C·∫§U H√åNH HYPERPARAMETERS
# ==============================================================================
N_EPOCHS = 20           # S·ªë epoch t·ªëi ƒëa
CLIP = 1.0              # Gradient clipping
LEARNING_RATE = 0.001   # Learning rate
PATIENCE = 3            # Early Stopping: d·ª´ng sau N epoch kh√¥ng c·∫£i thi·ªán
TEACHER_FORCING_RATIO = 0.5  # T·ª∑ l·ªá Teacher Forcing

# ==============================================================================
# 2. H√ÄM EPOCH_TIME (Helper)
# ==============================================================================
def epoch_time(start_time, end_time):
    """T√≠nh th·ªùi gian ch·∫°y 1 epoch (ph√∫t, gi√¢y)."""
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


# ==============================================================================
# 3. H√ÄM TRAIN (1 Epoch)
# ==============================================================================
def train(model, iterator, optimizer, criterion, clip, device, teacher_forcing_ratio=0.5):
    """
    Hu·∫•n luy·ªán model trong 1 epoch.
    
    Args:
        model: M√¥ h√¨nh Seq2Seq
        iterator: DataLoader train
        optimizer: Adam optimizer
        criterion: CrossEntropyLoss
        clip: Gradient clipping value
        device: 'cuda' ho·∫∑c 'cpu'
        teacher_forcing_ratio: T·ª∑ l·ªá s·ª≠ d·ª•ng Teacher Forcing (0.5)
        
    Returns:
        epoch_loss: Loss trung b√¨nh c·ªßa epoch
    """
    model.train()
    epoch_loss = 0
    
    progress_bar = tqdm(iterator, desc="Training", leave=False)
    
    for batch in progress_bar:
        # ===== 1. UNPACK BATCH =====
        # collate_fn tr·∫£ v·ªÅ: (src, trg, src_len)
        src, trg, src_len = batch
        
        # Chuy·ªÉn src, trg l√™n device
        src = src.to(device)         # [src_len, batch_size]
        trg = trg.to(device)         # [trg_len, batch_size]
        # ‚ö†Ô∏è src_len PH·∫¢I n·∫±m tr√™n CPU cho pack_padded_sequence
        # KH√îNG g·ªçi src_len.to(device)!
        
        # ===== 2. FORWARD PASS =====
        optimizer.zero_grad()
        
        # Forward v·ªõi teacher_forcing_ratio
        # output shape: [trg_len, batch_size, output_dim]
        output = model(src, src_len, trg, teacher_forcing_ratio)
        
        # ===== 3. T√çNH LOSS =====
        """
        üìå LOGIC SLICING (QUAN TR·ªåNG):
        - output[0] l√† zeros tensor (do loop b·∫Øt ƒë·∫ßu t·ª´ t=1)
        - trg[0] l√† <sos> token
        - Ph·∫£i b·ªè c·∫£ hai tr∆∞·ªõc khi t√≠nh loss
        
        Sau khi slice:
        - output: [trg_len-1, batch_size, output_dim]
        - trg:    [trg_len-1, batch_size]
        """
        output_dim = output.shape[-1]
        
        # B·ªè timestep ƒë·∫ßu ti√™n
        output = output[1:]   # [trg_len-1, batch_size, output_dim]
        trg = trg[1:]         # [trg_len-1, batch_size]
        
        # Reshape v·ªÅ 2D cho CrossEntropyLoss
        output = output.reshape(-1, output_dim)  # [(trg_len-1)*batch_size, output_dim]
        trg = trg.reshape(-1)                    # [(trg_len-1)*batch_size]
        
        # T√≠nh loss
        loss = criterion(output, trg)
        
        # ===== 4. BACKWARD PASS =====
        loss.backward()
        
        # Clip gradient ƒë·ªÉ tr√°nh exploding gradient
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        # Update weights
        optimizer.step()
        
        epoch_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return epoch_loss / len(iterator)


# ==============================================================================
# 4. H√ÄM EVALUATE
# ==============================================================================
def evaluate(model, iterator, criterion, device):
    """
    ƒê√°nh gi√° model tr√™n t·∫≠p validation/test.
    
    Args:
        model: M√¥ h√¨nh Seq2Seq
        iterator: DataLoader val/test
        criterion: CrossEntropyLoss
        device: 'cuda' ho·∫∑c 'cpu'
        
    Returns:
        epoch_loss: Loss trung b√¨nh
    """
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(iterator, desc="Evaluating", leave=False):
            # Unpack batch
            src, trg, src_len = batch
            
            src = src.to(device)
            trg = trg.to(device)
            # src_len gi·ªØ nguy√™n tr√™n CPU
            
            # Forward v·ªõi teacher_forcing_ratio = 0 (kh√¥ng d√πng ground truth)
            output = model(src, src_len, trg, teacher_forcing_ratio=0)
            
            # T√≠nh loss (y h·ªát h√†m train)
            output_dim = output.shape[-1]
            
            output = output[1:]
            trg = trg[1:]
            
            output = output.reshape(-1, output_dim)
            trg = trg.reshape(-1)
            
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)


# ==============================================================================
# 5. KH·ªûI T·∫†O OPTIMIZER & CRITERION
# ==============================================================================

# Optimizer: Adam v·ªõi lr = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Loss function: CrossEntropyLoss v·ªõi ignore_index ƒë·ªÉ b·ªè qua PAD token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# √Åp d·ª•ng weight initialization
model.apply(init_weights)

# ƒê·∫øm s·ªë tham s·ªë
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print("=" * 60)
print(" C·∫§U H√åNH HU·∫§N LUY·ªÜN")
print("=" * 60)
print(f"Device:              {device}")
print(f"Total Parameters:    {total_params:,}")
print(f"Epochs:              {N_EPOCHS}")
print(f"Learning Rate:       {LEARNING_RATE}")
print(f"Gradient Clip:       {CLIP}")
print(f"Teacher Forcing:     {TEACHER_FORCING_RATIO}")
print(f"Early Stopping:      Patience = {PATIENCE}")
print(f"Batch Size:          {BATCH_SIZE}")
print("=" * 60)


# ==============================================================================
# 6. V√íNG L·∫∂P HU·∫§N LUY·ªÜN CH√çNH (V·ªöI EARLY STOPPING)
# ==============================================================================

# Bi·∫øn theo d√µi
best_valid_loss = float('inf')
epochs_without_improvement = 0
training_history = {
    'train_loss': [],
    'valid_loss': [],
    'train_ppl': [],
    'valid_ppl': []
}

print("\n" + "=" * 60)
print(" B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN")
print("=" * 60 + "\n")

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    # ===== TRAIN =====
    train_loss = train(
        model, train_loader, optimizer, criterion, 
        CLIP, device, TEACHER_FORCING_RATIO
    )
    
    # ===== EVALUATE =====
    valid_loss = evaluate(model, valid_loader, criterion, device)
    
    end_time = time.time()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    # ===== T√çNH PERPLEXITY =====
    train_ppl = math.exp(train_loss)
    valid_ppl = math.exp(valid_loss)
    
    # L∆∞u history
    training_history['train_loss'].append(train_loss)
    training_history['valid_loss'].append(valid_loss)
    training_history['train_ppl'].append(train_ppl)
    training_history['valid_ppl'].append(valid_ppl)
    
    # ===== CHECKPOINTING & EARLY STOPPING =====
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        epochs_without_improvement = 0
        
        # L∆∞u best model
        torch.save(model.state_dict(), 'best_model.pth')
        save_status = "‚úÖ Model saved!"
    else:
        epochs_without_improvement += 1
        save_status = f"‚ö†Ô∏è No improvement ({epochs_without_improvement}/{PATIENCE})"
    
    # ===== IN K·∫æT QU·∫¢ =====
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')
    print(f'\t{save_status}')
    print("-" * 60)
    
    # ===== KI·ªÇM TRA EARLY STOPPING =====
    if epochs_without_improvement >= PATIENCE:
        print("\n" + "=" * 60)
        print(f"‚õî EARLY STOPPING: Val loss kh√¥ng gi·∫£m sau {PATIENCE} epochs")
        print("=" * 60)
        break


# ==============================================================================
# 7. T·ªîNG K·∫æT HU·∫§N LUY·ªÜN
# ==============================================================================
print("\n" + "=" * 60)
print(" HU·∫§N LUY·ªÜN HO√ÄN T·∫§T!")
print("=" * 60)
print(f"Epochs ƒë√£ ch·∫°y:      {epoch + 1}")
print(f"Best Validation Loss: {best_valid_loss:.3f}")
print(f"Best Validation PPL:  {math.exp(best_valid_loss):.3f}")
print(f"Model ƒë√£ l∆∞u t·∫°i:     'best_model.pth'")
print("=" * 60)


# ==============================================================================
# 8. ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST
# ==============================================================================

# Load best model
model.load_state_dict(torch.load('best_model.pth', map_location=device))

# ƒê√°nh gi√° tr√™n test
test_loss = evaluate(model, test_loader, criterion, device)

print("\n" + "=" * 60)
print(" K·∫æT QU·∫¢ TR√äN T·∫¨P TEST")
print("=" * 60)
print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')
print("=" * 60)


# ==============================================================================
# 9. V·∫º BI·ªÇU ƒê·ªí TRAINING HISTORY (Optional)
# ==============================================================================
"""
# Uncomment ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot Loss
axes[0].plot(training_history['train_loss'], label='Train Loss', marker='o')
axes[0].plot(training_history['valid_loss'], label='Valid Loss', marker='s')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training & Validation Loss')
axes[0].legend()
axes[0].grid(True)

# Plot Perplexity
axes[1].plot(training_history['train_ppl'], label='Train PPL', marker='o')
axes[1].plot(training_history['valid_ppl'], label='Valid PPL', marker='s')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Perplexity')
axes[1].set_title('Training & Validation Perplexity')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150)
plt.show()

print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·∫°i 'training_history.png'")
"""
